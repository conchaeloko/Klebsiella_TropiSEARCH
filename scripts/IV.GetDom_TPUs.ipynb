{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "\n",
    "from transformers import AutoTokenizer, EsmForProteinFolding\n",
    "from transformers.models.esm.openfold_utils.protein import to_pdb, Protein as OFProtein\n",
    "from transformers.models.esm.openfold_utils.feats import atom14_to_atom37\n",
    "\n",
    "os.environ['TPU_NUM_DEVICES'] = '4'\n",
    "os.environ['LD_PRELOAD'] = ''\n",
    "os.environ['XLA_USE_BF16'] = '1'\n",
    "\n",
    "def tokenized_sequences(sequences, fasta_or_csv) :\n",
    "    \"\"\"\n",
    "    The function takes as an input a either a multifasta file or a dataframe with two columns.\n",
    "    If the input is a dataframe, the shape would consist of two columns with :\n",
    "    - 'id', which corresponds to the protein name\n",
    "    - 'sequence', which corresponds to the aa sequence\n",
    "    The function returns a list of tuples (a,b) with a as the id and b as the tokenized inputs\n",
    "    \"\"\"\n",
    "    if fasta_or_csv == \"csv\" :\n",
    "        dico_seq = {}\n",
    "        for i, row in sequences.iterrows():\n",
    "            dico_seq[row[\"id\"]] =  row[\"sequence\"]\n",
    "    elif fasta_or_csv == \"fasta\" :\n",
    "        from Bio import SeqIO\n",
    "        dico_seq = {record.id : str(record.seq) for record in SeqIO.parse(sequences, \"fasta\")}\n",
    "    tokenized_sequences = []\n",
    "    for idd in dico_seq :\n",
    "        tokenized_input = tokenizer(dico_seq[idd], return_tensors=\"pt\", add_special_tokens=False)['input_ids']\n",
    "        a = (idd , tokenized_input)\n",
    "        tokenized_sequences.append(a)\n",
    "    return tokenized_sequences\n",
    "\n",
    "\n",
    "# Move the device setup and model instantiation inside the prediction function\n",
    "def esmfold_prediction(index, flags, tokenized_sequences, path_out):\n",
    "    device = xm.xla_device()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/esmfold_v1\")\n",
    "    model = EsmForProteinFolding.from_pretrained(\"facebook/esmfold_v1\")\n",
    "    model = model.to(dtype=torch.bfloat16)\n",
    "    model = model.to(device)\n",
    "    model.trunk.set_chunk_size(64)\n",
    "    for protein in tokenized_sequences:\n",
    "        pdb_files = []\n",
    "        with torch.no_grad():\n",
    "            prot_to_pred = protein[1].to(device)\n",
    "            output = model(prot_to_pred)\n",
    "        print(\"Is it ?\")\n",
    "        with open(f\"{path_out}/{protein[0]}.esmfold_out\", \"w\") as outfile:\n",
    "            outfile.write(pdb_txt[0])\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "path_data = \"/home/robbyconchaeloko/DpoK-serotypeTropism/data\"\n",
    "df = pd.read_csv(f\"{path_data}/Results_III_sequences.v3.csv\" , sep = \"\\t\", names = [\"id\",\"sequence\"])\n",
    "eg_tokenized = tokenized_sequences(df , \"csv\")\n",
    "\n",
    "def main():\n",
    "    # Set the number of TPU cores (usually 8 for a single Cloud TPU v3-8)\n",
    "    num_tpu_cores = 3\n",
    "    # Replace the following with the output of the tokenized_sequences function\n",
    "    tokenized_sequences = [...]\n",
    "    # Set the output path\n",
    "    path_out = \"/home/robbyconchaeloko/output\"\n",
    "    xmp.spawn(esmfold_prediction, args=({}, eg_tokenized, path_out), nprocs=num_tpu_cores, start_method=\"fork\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
