{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report , roc_auc_score, matthews_corrcoef , confusion_matrix\n",
    "from scipy.stats import fisher_exact\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm \n",
    "from multiprocessing.pool import ThreadPool \n",
    "\n",
    "path_work = f\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023\"\n",
    "path_task = f\"{path_work}/Rafa_task\"\n",
    "path_embeddings = f\"{path_task}/embeddings\"\n",
    "path_3D = f\"{path_task}/3Dsequence_similarity\"\n",
    "path_aa_seq = f\"{path_task}/sequence_similarity\"\n",
    "\n",
    "path_methods = [path_embeddings , path_3D , path_aa_seq]\n",
    "\n",
    "path_dfs = []\n",
    "for method in path_methods :\n",
    "    for file in os.listdir(method) :\n",
    "        if file[-3:] == \"csv\" :\n",
    "            f_path = f\"{method}/{file}\"\n",
    "            path_dfs.append(f_path)\n",
    "            \n",
    "\n",
    "index_dico = {\"cdhit\" : \"seq_similarity\" , \"foldseek\" : \"3D_similarity\" , \"embeddings\" : \"embeddings_similarity\"}\n",
    "dico_KL_type = json.load(open(f\"{path_task}/dico_prophage_ID.KLtypes.json\"))\n",
    "KL_types = set(values[0] for values in dico_KL_type.values())\n",
    "\n",
    "\n",
    "def make_df_type(path_df , label) :\n",
    "    df = pd.read_csv(f\"{path_df}\" , sep = \"\\t\" , header = 0 , index_col = 0)\n",
    "    method = index_dico[path_df.split(\"/\")[-1].split(\"_\")[1].split(\".\")[1]]\n",
    "    threshold = path_df.split(\"/\")[-1].split(\"_\")[-1].split(\".csv\")[0]\n",
    "    dico_KL_prophage_id = json.load(open(f\"{path_task}/dico_prophage_ID.KLtypes.json\"))\n",
    "    df[\"Labels\"] = df.index.to_series().map(lambda x : 1 if label in dico_KL_type.get(x, []) else 0)\n",
    "    support = 0 \n",
    "    X = df.drop([\"Labels\"] , axis = 1)\n",
    "    for prophage , kltypes in dico_KL_type.items() :\n",
    "        if prophage in (X.index.tolist()) :\n",
    "            if X.loc[[prophage]].sum().sum() > 0 and label in kltypes:\n",
    "                support+=1\n",
    "    return df , method , threshold , support\n",
    "\n",
    "\n",
    "def fit_rf_model_random_search(df):\n",
    "    X = df.drop([\"Labels\"] , axis = 1)\n",
    "    n_Dpo_clusters = len(X.columns)\n",
    "    n_iter=1\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, df[\"Labels\"], test_size=0.2, random_state=42)\n",
    "    param_grid = {\n",
    "            'bootstrap': [True, False],\n",
    "            'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "            'max_features': ['auto', 'sqrt'],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'n_estimators': [200, 400, 600, 800]\n",
    "        }\n",
    "    rf = RandomForestClassifier()\n",
    "    rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, \n",
    "                                   n_iter=n_iter, cv=4, verbose=2, random_state=42, n_jobs=-1)\n",
    "    rf_random.fit(X_train, y_train)\n",
    "    # Print the best parameters from the Randomized Search\n",
    "    predictions = rf_random.predict(X_test)\n",
    "    report = classification_report(y_test, predictions, output_dict=True)\n",
    "    # Getting the TPR and TNR : \n",
    "    try : \n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "        tpr = tp / (tp + fn)\n",
    "        tnr = tn / (tn + fp)\n",
    "        mcc = matthews_corrcoef(y_test, predictions)\n",
    "        return tpr , tnr , mcc  , report['1']['f1-score'] , n_Dpo_clusters \n",
    "    except ValueError :\n",
    "        return None , None , None  , None , n_Dpo_clusters \n",
    "\n",
    "    \n",
    "def write_line(method , label , support , threshold , n_clusters , tpr , tnr , f1_score , mcc) :\n",
    "    with open(f\"{path_task}/{method}_RF_FullReport.corrected.tsv\", \"a+\") as outfile :\n",
    "        prefix = f\"{label}\\t{support}\\t{threshold}\\t{n_clusters}\"\n",
    "        if f1_score != None :\n",
    "            metrics = {\"TPR\" : tpr , \"TNR\" : tnr , \"F1_score\" : f1_score , \"MCC\" : mcc}\n",
    "            for metric in metrics :\n",
    "                outfile.write(f\"{prefix}\\t{metric}\\t{metrics[metric]}\\n\")\n",
    "        else :\n",
    "            outfile.write(f\"{prefix}\\tNo prediction in that instance\\n\")\n",
    "        \n",
    "\n",
    "def final_function(path_df) :\n",
    "    for label in KL_types :\n",
    "        df , method , threshold , support = make_df_type(path_df , label)\n",
    "        tpr , tnr , mcc , f1_score , n_Dpo_clusters = fit_rf_model_random_search(df)\n",
    "        write_line(method , label , support , threshold , n_Dpo_clusters , tpr , tnr , f1_score , mcc)\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    with ThreadPool(20) as p:\n",
    "        p.map(final_function, path_dfs)\n",
    "        \n",
    "        \n",
    "df , method , threshold = make_df_type(path_dfs[5] , \"KL64\")\n",
    "fit_rf_model_random_search(df)\n",
    "write_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_rf_model_random_search(label):\n",
    "    for path_df in paths_df : \n",
    "        try : \n",
    "            file = path_df.split(\"/\")[-1]\n",
    "            df = pd.read_csv(f\"{path_df}\", sep = \"\\t\", index_col = 0)\n",
    "            df[\"Labels\"] = df.index.to_series().map(lambda x : 1 if label in dico_KL_type.get(x, []) else 0)\n",
    "            \n",
    "            \n",
    "            X = df.drop([\"Labels\"] , axis = 1)\n",
    "            n_iter=10\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, df[\"Labels\"], test_size=0.2, random_state=42)\n",
    "            param_grid = {\n",
    "                    'bootstrap': [True, False],\n",
    "                    'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "                    'max_features': ['auto', 'sqrt'],\n",
    "                    'min_samples_leaf': [1, 2, 4],\n",
    "                    'min_samples_split': [2, 5, 10],\n",
    "                    'n_estimators': [200, 400, 600, 800]\n",
    "                }\n",
    "            rf = RandomForestClassifier()\n",
    "            rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, \n",
    "                                           n_iter=n_iter, cv=4, verbose=2, random_state=42, n_jobs=-1)\n",
    "            rf_random.fit(X_train, y_train)\n",
    "            # Print the best parameters from the Randomized Search\n",
    "            predictions = rf_random.predict(X_test)\n",
    "            report = classification_report(y_test, predictions, output_dict=True)\n",
    "            with open(f\"{path_project}/{dico_names[file]}.KL_report.tsv\", \"a+\") as outfile : \n",
    "                outfile.write(f\"{label}\\t{report['1']['precision']}\\t{report['1']['recall']}\\t{report['1']['f1-score']}\\t{report['1']['support']}\\n\")\n",
    "        except Exception as e :\n",
    "            with open(f\"{path_project}/{dico_names[file]}.KL_report.tsv\", \"a+\") as outfile : \n",
    "                outfile.write(f\"{label}\\tFailed\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "support = 0 \n",
    "df.drop([\"Labels\"], axis = 1)\n",
    "for prophage , kltypes in dico_KL_type.items() :\n",
    "    if prophage in (df.index.tolist()) :\n",
    "        if df.loc[[prophage]].sum().sum() > 0 and \"KL64\" in kltypes:\n",
    "            support+=1 \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute class weights\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "# convert class weights to a tensor\n",
    "weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "# move to GPU if available\n",
    "weights = weights.to(device)\n",
    "# use in your loss function\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
