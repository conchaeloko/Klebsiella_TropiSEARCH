{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f82710",
   "metadata": {},
   "source": [
    "# Build the depolymerase database \n",
    "***\n",
    "\n",
    "## The cookbook for building the depolymerase database :\n",
    "### A. Get the E.C associated with the depolymerase activity \n",
    "### B. Collect the IPR entries associated with the E.C of interest\n",
    "### C. Scan the interproscan descriptions ; keep those that are relevant in our case\n",
    "### D. Download the protein sequences, make a filtered hmm profile\n",
    "### E. Make the DB\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2eacd4",
   "metadata": {},
   "source": [
    "> A. <b> Get the relevant E.C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3c1191",
   "metadata": {},
   "source": [
    "Two classes of enzymes are associated with a depolymerase activity : the lyase and the hydrolase\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "<b>\n",
    "Lyases \n",
    " <b/>\n",
    "</div>\n",
    "EC 4. Lyases ; 4.2 Carbon-oxygen lyases ; 4.2.2 Acting on polysaccharides\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "==> All the groups with 4.2.2\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">   \n",
    "<b>\n",
    "Hydrolases \n",
    "<b/>    \n",
    "</div>\n",
    "EC 3. Hydrolases ; 3.2 Glycosylases ; 3.2.1 Glycosidases, i.e. enzymes hydrolyzing O- and S-glycosyl compounds \n",
    "<div class=\"alert alert-block alert-success\">\n",
    "==> All the groups with 3.2.1\n",
    "</div>\n",
    " \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e907c75e",
   "metadata": {},
   "source": [
    ">B. <b>Collect the IPE entries associated with the E.C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c058c8ba",
   "metadata": {},
   "source": [
    "1. Fetch the file with the informations associated with each IPR entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10749378",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PPT_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e2a2c4e0827b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPPT_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpath_PFAM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/robbyce/Documents/bioinformatics/Depolymerase_DB\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PPT_functions'"
     ]
    }
   ],
   "source": [
    "import xmltodict\n",
    "import pprint\n",
    "import json\n",
    "#from PPT_functions import *\n",
    "\n",
    "path_PFAM = \"/home/robbyce/Documents/bioinformatics/Depolymerase_DB\"\n",
    "path_PFAM = \"/home/conchae/databases/depolymerase_building\"\n",
    "# Getting the file with the description of each IPR entries\n",
    "# ! wget https://ftp.ebi.ac.uk/pub/databases/interpro/current_release/interpro.xml.gz\n",
    "xml_interpro = xmltodict.parse(open(f\"{path_PFAM}/interpro.xml\").read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d7bd37",
   "metadata": {},
   "source": [
    "2. Generate an object with the IPR ACC and descriptions of the IPR entries associated with the relevant E.C ==> candidates IPR : n = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40e9461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m=0\n",
    "Ec_numbers = [\"4.2.2\",\"3.2.1\"]\n",
    "\n",
    "IPR_scan = {}\n",
    "IPR_s = set ()\n",
    "clean_IPR = {}\n",
    "\n",
    "elimininated = {'Active_site' : [],\n",
    " 'Binding_site': [],\n",
    " 'Conserved_site': [],\n",
    " 'PTM': []}\n",
    "\n",
    "lyases = set()\n",
    "hydrolases =set()\n",
    "# Get a list of correct IPR : \n",
    "for index_i, entry in tqdm(enumerate(xml_interpro['interprodb'][\"interpro\"])) :\n",
    "    if \"external_doc_list\" in xml_interpro['interprodb'][\"interpro\"][index_i] :\n",
    "        output = xml_interpro['interprodb'][\"interpro\"][index_i]\n",
    "        if output[\"@type\"] not in elimininated :\n",
    "            if \"db_xref\" in xml_interpro['interprodb'][\"interpro\"][index_i][\"external_doc_list\"] :\n",
    "                for index_db ,db in enumerate(xml_interpro['interprodb'][\"interpro\"][index_i][\"external_doc_list\"][\"db_xref\"]) :\n",
    "                    try :\n",
    "                        if db[\"@db\"]=='EC' :\n",
    "                            try :\n",
    "                                if db[\"@dbkey\"][0:5] in Ec_numbers :\n",
    "                                    IPR_s.add(entry[\"@id\"])\n",
    "                                    if db[\"@dbkey\"][0:5] == \"4.2.2\" :\n",
    "                                        lyases.add(entry[\"@id\"])\n",
    "                                    else : \n",
    "                                        hydrolases.add(entry[\"@id\"])\n",
    "                            except Exception as e :\n",
    "                                pass\n",
    "                    except Exception as e :\n",
    "                        if xml_interpro['interprodb'][\"interpro\"][index_i][\"external_doc_list\"][\"db_xref\"][\"@db\"] == \"EC\":\n",
    "                            if xml_interpro['interprodb'][\"interpro\"][index_i][\"external_doc_list\"][\"db_xref\"][\"@dbkey\"][0:5] in Ec_numbers:\n",
    "                                IPR_s.add(entry[\"@id\"])\n",
    "                                if xml_interpro['interprodb'][\"interpro\"][index_i][\"external_doc_list\"][\"db_xref\"][\"@dbkey\"][0:5] == \"4.2.2\" :\n",
    "                                    lyases.add(entry[\"@id\"])\n",
    "                                else : \n",
    "                                    hydrolases.add(entry[\"@id\"])\n",
    "\n",
    "for IPR in IPR_s :\n",
    "    output = get_full_entry(IPR , False)\n",
    "    if output[\"@type\"] not in elimininated :\n",
    "        short_name = output[\"@short_name\"]\n",
    "        if \"p\" not in output[\"abstract\"] :\n",
    "            full_text = output[\"abstract\"][\"#text\"]\n",
    "        elif isinstance(output[\"abstract\"][\"p\"] , str) :\n",
    "            full_text = output[\"abstract\"][\"p\"]\n",
    "            pass\n",
    "        elif isinstance(output[\"abstract\"][\"p\"] , dict) :\n",
    "            full_text = output[\"abstract\"][\"p\"][\"#text\"]\n",
    "            pass\n",
    "        elif isinstance(output[\"abstract\"][\"p\"] , list) :\n",
    "            full_text = str()\n",
    "            if len(output[\"abstract\"][\"p\"]) == 1 :\n",
    "                full_text = output[\"abstract\"][\"p\"][0]\n",
    "            else :\n",
    "                if all_strings(output[\"abstract\"][\"p\"]) == True :\n",
    "                    full_text = \"\".join(output[\"abstract\"][\"p\"])\n",
    "                    # *** Until here, it's fine ***\n",
    "                else :\n",
    "                    for index_p, ele_p in enumerate(output[\"abstract\"][\"p\"]) :\n",
    "                        try :\n",
    "                            if isinstance (ele_p , str) :\n",
    "                                full_text = full_text + ele_p\n",
    "                            elif isinstance (ele_p , dict) and \"#text\" in ele_p:\n",
    "                                full_text = full_text + ele_p[\"#text\"]\n",
    "                            else :\n",
    "                                continue\n",
    "                        except Exception as e :\n",
    "                            print(e , IPR,output[\"abstract\"][\"p\"])\n",
    "            pass\n",
    "        # Reformatting the description :\n",
    "        bad_char = [\"[\", \"]\",\"\\n\", \" , \", \" , .\", \" , , .\"]\n",
    "        clean_description = str()\n",
    "        very_clean_description = str()\n",
    "        for char in bad_char :\n",
    "            full_text= full_text.replace(char, \"\")\n",
    "        #for char in full_text :\n",
    "            #if char in bad_char :\n",
    "                #continue\n",
    "            #else :\n",
    "                #clean_description = clean_description + char\n",
    "        clean_description = \" \".join(full_text.split())\n",
    "        #for char in clean_description :\n",
    "            #if char in bad_char :\n",
    "                #very_clean_description = very_clean_description + \".\"\n",
    "                #continue\n",
    "            #else :\n",
    "                #very_clean_description = very_clean_description + char\n",
    "        # Creating the dictionary \n",
    "        a = {\"short_name\" : short_name, \"description\" : clean_description}\n",
    "        clean_IPR[IPR] = a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0226cab4",
   "metadata": {},
   "source": [
    "***\n",
    "> C. <b>Scan the description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbd07ff",
   "metadata": {},
   "source": [
    "1. The relevant functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eee1464",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_full_entry(IPR , pprint_o_nah) :\n",
    "    import pprint\n",
    "    pp = pprint.PrettyPrinter(width = 150)\n",
    "    for index_i, entry in enumerate(xml_interpro['interprodb'][\"interpro\"]) :\n",
    "        if entry[\"@id\"] == IPR :\n",
    "            output = xml_interpro['interprodb'][\"interpro\"][index_i]\n",
    "    if pprint_o_nah == True :\n",
    "        return pp.pprint(output)\n",
    "    else :\n",
    "        return output\n",
    "    \n",
    "def all_strings(list_p) :\n",
    "    for index_list, element in enumerate(list_p) :\n",
    "        if isinstance(element , str) == False :\n",
    "            return False\n",
    "            break\n",
    "        else :\n",
    "            continue\n",
    "    else :\n",
    "        return True\n",
    "    \n",
    "    \n",
    "def any_string(list_p) :\n",
    "    for index_list, element in enumerate(list_p) :\n",
    "        if isinstance(element , str) == True :\n",
    "            return True\n",
    "            break\n",
    "    else :\n",
    "        return False\n",
    "    \n",
    "def return_string(list_p) :\n",
    "    string = str()\n",
    "    for index_list, element in enumerate(list_p) :\n",
    "        if isinstance(element , str) == True :\n",
    "            string = string + element\n",
    "        else :\n",
    "            continue\n",
    "            \n",
    "    \n",
    "def go_terms(IPR, search_list) :\n",
    "    for index_i, entry in enumerate(xml_interpro['interprodb'][\"interpro\"]) :\n",
    "        if entry[\"@id\"] == IPR :\n",
    "            if \"class_list\" in entry.keys() :\n",
    "                descriptions = []\n",
    "                if isinstance(entry[\"class_list\"][\"classification\"] , dict ) :\n",
    "                    descriptions.append(go_term[\"description\"]) \n",
    "                elif isinstance(entry[\"class_list\"][\"classification\"] , list ) :\n",
    "                    for i_go, go_term in enumerate(entry[\"class_list\"][\"classification\"]) :\n",
    "                        descriptions.append(go_term[\"description\"]) \n",
    "                for i_des, go_description in enumerate(descriptions):\n",
    "                    if go_description in search_list :\n",
    "                        if IPR not in IPR_depo :\n",
    "                            get_full_entry(IPR, True)\n",
    "                            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c70b88",
   "metadata": {},
   "source": [
    "2. Some code to facilitate the classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9c1a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "depo_terms = [\"hyluronidase\", \"pectin\", \"pectate\",\"sialidase\", \"levanase\", \"xylosidase\",\"dextranase\",\"rhamnosidase\",\"alginate\"]\n",
    "\n",
    "for id__ in list(clean_IPR.keys())[600:] :\n",
    "    if id__ in lyases :\n",
    "        #print(id__)\n",
    "        #pp.pprint(clean_IPR[id__])\n",
    "        #print(\"\\n\")\n",
    "        pass\n",
    "    elif good_term(clean_IPR[id__][\"description\"].lower()) == True :\n",
    "        #print(id__)\n",
    "        #pp.pprint(clean_IPR[id__])\n",
    "        #print(\"\\n\")\n",
    "        pass\n",
    "    elif clean_IPR[id__][\"description\"].lower().count(\"cbm\") > 0 :\n",
    "        #print(id__)\n",
    "        #pp.pprint(clean_IPR[id__])\n",
    "        #print(\"\\n\")\n",
    "        pass\n",
    "    elif clean_IPR[id__][\"description\"].lower().count(\"peptidoglycan\") > 0 :\n",
    "        #print(id__)\n",
    "        #pp.pprint(clean_IPR[id__])\n",
    "        #print(\"\\n\")\n",
    "        pass\n",
    "    elif clean_IPR[id__][\"description\"].lower().count(\"peptidase\") > 0 :\n",
    "        #print(id__)\n",
    "        #pp.pprint(clean_IPR[id__])\n",
    "        #print(\"\\n\")\n",
    "        pass\n",
    "    elif clean_IPR[id__][\"description\"].lower().count(\"dna\") > 0 :\n",
    "        #print(id__)\n",
    "        #pp.pprint(clean_IPR[id__])\n",
    "        #print(\"\\n\")\n",
    "        pass\n",
    "    elif clean_IPR[id__][\"description\"].lower().count(\"polysaccharide\") > 0 :\n",
    "        #print(id__)\n",
    "        #pp.pprint(clean_IPR[id__])\n",
    "        #print(\"\\n\")\n",
    "        pass\n",
    "    elif clean_IPR[id__][\"description\"].lower().count(\"o-glycosyl hydrolases\") > 0 :\n",
    "        #print(id__)\n",
    "        #pp.pprint(clean_IPR[id__])\n",
    "        #print(\"\\n\")\n",
    "        pass\n",
    "    else :\n",
    "        print(id__)\n",
    "        pp.pprint(clean_IPR[id__])\n",
    "        print(\"\\n\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f599e08",
   "metadata": {},
   "source": [
    "3. After reading the descriptions of the IPR, we kept this final list of entries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcc8e0cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IPR_depolymerase = ['IPR029411', 'IPR002152', 'IPR012480', 'IPR039513', 'IPR002925', 'IPR041624', 'IPR024200', 'IPR029456', 'IPR015220', 'IPR041111', 'IPR036962',\n",
    " 'IPR016285', 'IPR005323', 'IPR016007', 'IPR044505', 'IPR015178', 'IPR011840', 'IPR024561', 'IPR006048', 'IPR009470', 'IPR004197', 'IPR012878',\n",
    " 'IPR015165', 'IPR024732', 'IPR023309', 'IPR040784', 'IPR014551', 'IPR032312', 'IPR000490', 'IPR006103', 'IPR001139', 'IPR031330', 'IPR000400',\n",
    " 'IPR044846', 'IPR011040', 'IPR011050', 'IPR024428', 'IPR000743', 'IPR002241', 'IPR005192', 'IPR011837', 'IPR024430', 'IPR000852', 'IPR006710',\n",
    " 'IPR011839', 'IPR010905', 'IPR013775', 'IPR011838', 'IPR026071', 'IPR032792', 'IPR043534', 'IPR012939', 'IPR037110', 'IPR003469', 'IPR017736',\n",
    " 'IPR015289', 'IPR016283', 'IPR016282', 'IPR039743', 'IPR005197', 'IPR016289', 'IPR018155', 'IPR035669', 'IPR023933', 'IPR036026', 'IPR023296',\n",
    " 'IPR016840', 'IPR013777', 'IPR000165', 'IPR010720', 'IPR001554', 'IPR001661', 'IPR002252', 'IPR027291', 'IPR006215', 'IPR005604', 'IPR019563',\n",
    " 'IPR022844', 'IPR011395', 'IPR011496', 'IPR015883', 'IPR005198', 'IPR017069', 'IPR022859', 'IPR025975', 'IPR018082', 'IPR012334', 'IPR000322',\n",
    " 'IPR001860', 'IPR000805', 'IPR036434', 'IPR038964', 'IPR006046', 'IPR024745', 'IPR013319', 'IPR000334', 'IPR031335', 'IPR003476', 'IPR001382',\n",
    " 'IPR037019', 'IPR005193', 'IPR037398', 'IPR013785', 'IPR010702', 'IPR026856', 'IPR023295', 'IPR039174', 'IPR034641', 'IPR036278', 'IPR045032',\n",
    " 'IPR016590', 'IPR002022', 'IPR012970', 'IPR038970', 'IPR044112', 'IPR000514', 'IPR001724', 'IPR001362', 'IPR025092', 'IPR006775', 'IPR036881',\n",
    " 'IPR033654', 'IPR039279', 'IPR006047', 'IPR006101', 'IPR016288', 'IPR001137', 'IPR004185', 'IPR004888', 'IPR001547', 'IPR005199', 'IPR011583',\n",
    " 'IPR000933', 'IPR011683', 'IPR001722', 'IPR005201', 'IPR001439', 'IPR002037', 'IPR011100', 'IPR027260', 'IPR045857', 'IPR009860', 'IPR016455',\n",
    " 'IPR014895', 'IPR027946', 'IPR008929', 'IPR000125', 'IPR024746', 'IPR001329', 'IPR000726', 'IPR000974', 'IPR011330', 'IPR006633', 'IPR008979',\n",
    " 'IPR023346', 'IPR000974', 'IPR019282', 'IPR000922', 'IPR041351', 'IPR006421', 'IPR003790', 'IPR001088', 'IPR010713', 'IPR023309', 'IPR011839',\n",
    " 'IPR000125', 'IPR035394', 'IPR032091', 'IPR015177', 'IPR039448', 'IPR015331', 'IPR011613', 'IPR015179', 'IPR014635', 'IPR001371', 'IPR016282',\n",
    " 'IPR016714', 'IPR006065', 'IPR001223', 'IPR025706', 'IPR041542', 'IPR033452', 'IPR014718', 'IPR035396', 'IPR044914', 'IPR006425', 'IPR021016',\n",
    " 'IPR008811', 'IPR001286', 'IPR005200', 'IPR024733', 'IPR000757', 'IPR032790', 'IPR008397', 'IPR004898', 'IPR006626', 'IPR003159', 'IPR008902',\n",
    " 'IPR007724', 'IPR046372', 'IPR001000', 'IPR016287', 'IPR013776', 'IPR002594', 'IPR023720', 'IPR000556', 'IPR016286', 'IPR016828', 'IPR029070',\n",
    " 'IPR006584', 'IPR039514', 'IPR026283', 'IPR032979', 'IPR009939', 'IPR039473', 'IPR007781', 'IPR000677', 'IPR038901', 'IPR008291', 'IPR040527',\n",
    " 'IPR036439']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75892ec",
   "metadata": {},
   "source": [
    "***\n",
    "> D.<b> Download the protein sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b142a6c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(IPR_depolymerase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c685dc",
   "metadata": {},
   "source": [
    "1. Download the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f6b185",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# standard library modules\n",
    "import sys, errno, re, json, ssl\n",
    "from urllib import request\n",
    "from urllib.error import HTTPError\n",
    "from time import sleep\n",
    "import os \n",
    "from multiprocessing import Pool\n",
    "\n",
    "path_db = \"/home/conchae/databases/depolymerase_building\"\n",
    "\n",
    "HEADER_SEPARATOR = \"|\"\n",
    "LINE_LENGTH = 80\n",
    "\n",
    "\n",
    "IPR_depo = [\n",
    " 'IPR029411', 'IPR002152', 'IPR012480', 'IPR039513', 'IPR002925', 'IPR041624', 'IPR024200', 'IPR029456', 'IPR015220', 'IPR041111', 'IPR036962',\n",
    " 'IPR016285', 'IPR005323', 'IPR016007', 'IPR044505', 'IPR015178', 'IPR011840', 'IPR024561', 'IPR006048', 'IPR009470', 'IPR004197', 'IPR012878',\n",
    " 'IPR015165', 'IPR024732', 'IPR023309', 'IPR040784', 'IPR014551', 'IPR032312', 'IPR000490', 'IPR006103', 'IPR001139', 'IPR031330', 'IPR000400',\n",
    " 'IPR044846', 'IPR011040', 'IPR011050', 'IPR024428', 'IPR000743', 'IPR002241', 'IPR005192', 'IPR011837', 'IPR024430', 'IPR000852', 'IPR006710',\n",
    " 'IPR011839', 'IPR010905', 'IPR013775', 'IPR011838', 'IPR026071', 'IPR032792', 'IPR043534', 'IPR012939', 'IPR037110', 'IPR003469', 'IPR017736',\n",
    " 'IPR015289', 'IPR016283', 'IPR016282', 'IPR039743', 'IPR005197', 'IPR016289', 'IPR018155', 'IPR035669', 'IPR023933', 'IPR036026', 'IPR023296',\n",
    " 'IPR016840', 'IPR013777', 'IPR000165', 'IPR010720', 'IPR001554', 'IPR001661', 'IPR002252', 'IPR027291', 'IPR006215', 'IPR005604', 'IPR019563',\n",
    " 'IPR022844', 'IPR011395', 'IPR011496', 'IPR015883', 'IPR005198', 'IPR017069', 'IPR022859', 'IPR025975', 'IPR018082', 'IPR012334', 'IPR000322',\n",
    " 'IPR001860', 'IPR000805', 'IPR036434', 'IPR038964', 'IPR006046', 'IPR024745', 'IPR013319', 'IPR000334', 'IPR031335', 'IPR003476', 'IPR001382',\n",
    " 'IPR037019', 'IPR005193', 'IPR037398', 'IPR013785', 'IPR010702', 'IPR026856', 'IPR023295', 'IPR039174', 'IPR034641', 'IPR036278', 'IPR045032',\n",
    " 'IPR016590', 'IPR002022', 'IPR012970', 'IPR038970', 'IPR044112', 'IPR000514', 'IPR001724', 'IPR001362', 'IPR025092', 'IPR006775', 'IPR036881',\n",
    " 'IPR033654', 'IPR039279', 'IPR006047', 'IPR006101', 'IPR016288', 'IPR001137', 'IPR004185', 'IPR004888', 'IPR001547', 'IPR005199', 'IPR011583',\n",
    " 'IPR000933', 'IPR011683', 'IPR001722', 'IPR005201', 'IPR001439', 'IPR002037', 'IPR011100', 'IPR027260', 'IPR045857', 'IPR009860', 'IPR016455',\n",
    " 'IPR014895', 'IPR027946', 'IPR008929', 'IPR000125', 'IPR024746', 'IPR001329', 'IPR000726', 'IPR000974', 'IPR011330', 'IPR006633', 'IPR008979',\n",
    " 'IPR023346', 'IPR000974', 'IPR019282', 'IPR000922', 'IPR041351', 'IPR006421', 'IPR003790', 'IPR001088', 'IPR010713', 'IPR023309', 'IPR011839',\n",
    " 'IPR000125', 'IPR035394', 'IPR032091', 'IPR015177', 'IPR039448', 'IPR015331', 'IPR011613', 'IPR015179', 'IPR014635', 'IPR001371', 'IPR016282',\n",
    " 'IPR016714', 'IPR006065', 'IPR001223', 'IPR025706', 'IPR041542', 'IPR033452', 'IPR014718', 'IPR035396', 'IPR044914', 'IPR006425', 'IPR021016',\n",
    " 'IPR008811', 'IPR001286', 'IPR005200', 'IPR024733', 'IPR000757', 'IPR032790', 'IPR008397', 'IPR004898', 'IPR006626', 'IPR003159', 'IPR008902',\n",
    " 'IPR007724', 'IPR046372', 'IPR001000', 'IPR016287', 'IPR013776', 'IPR002594', 'IPR023720', 'IPR000556', 'IPR016286', 'IPR016828', 'IPR029070',\n",
    " 'IPR006584', 'IPR039514', 'IPR026283', 'IPR032979', 'IPR009939', 'IPR039473', 'IPR007781', 'IPR000677', 'IPR038901', 'IPR008291', 'IPR040527',\n",
    " 'IPR036439']\n",
    "\n",
    "urls = [f\"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/{IPR}/?page_size=200&extra_fields=sequence\" for IPR in IPR_depo if IPR]\n",
    "\n",
    "def get_IPR(url) :\n",
    "    IPR = url.split(\"InterPro/\")[1].split(\"/\")[0]\n",
    "    print(f\"{IPR} in the process... \\n\")\n",
    "    final_length = 0\n",
    "    if os.path.isfile(f\"{path_db}/{IPR}.entry.sequences.fasta\") == False :\n",
    "        with open(f\"{path_db}/{IPR}.entry.sequences.fasta\", \"w\") as outfile :\n",
    "            #disable SSL verification to avoid config issues\n",
    "            context = ssl._create_unverified_context()\n",
    "            next = url\n",
    "            last_page = False\n",
    "            attempts = 0\n",
    "            while next:\n",
    "                try:\n",
    "                    req = request.Request(next, headers={\"Accept\": \"application/json\"})\n",
    "                    res = request.urlopen(req, context=context)\n",
    "                    # If the API times out due a long running query\n",
    "                    if res.status == 408:\n",
    "                        # wait just over a minute\n",
    "                        sleep(61)\n",
    "                        # then continue this loop with the same URL\n",
    "                        continue\n",
    "                    elif res.status == 204:\n",
    "                        #no data so leave loop\n",
    "                        break\n",
    "                    payload = json.loads(res.read().decode())\n",
    "                    next = payload[\"next\"]\n",
    "                    attempts = 0\n",
    "                    if not next:\n",
    "                        last_page = True\n",
    "                except HTTPError as e:\n",
    "                    if e.code == 408:\n",
    "                        sleep(61)\n",
    "                        continue\n",
    "                    else:\n",
    "                        # If there is a different HTTP error, it wil re-try 3 times before failing\n",
    "                        if attempts < 3:\n",
    "                            attempts += 1\n",
    "                            sleep(61)\n",
    "                            continue\n",
    "                        else:\n",
    "                            sys.stderr.write(\"LAST URL: \" + next)\n",
    "                            raise e\n",
    "\n",
    "                for i, item in enumerate(payload[\"results\"]):\n",
    "                    if i > 500 :\n",
    "                    start , end = 0, 0\n",
    "\n",
    "                    entries = None\n",
    "                    if (\"entry_subset\" in item):\n",
    "                        entries = item[\"entry_subset\"]\n",
    "                    elif (\"entries\" in item):\n",
    "                        entries = item[\"entries\"]\n",
    "                    if entries is not None:\n",
    "                        entries_header = \"-\".join([entry[\"accession\"] + \"(\" + \";\".join([\",\".join([ str(fragment[\"start\"]) + \"...\" + str(fragment[\"end\"]) \n",
    "                              for fragment in locations[\"fragments\"]]) for locations in entry[\"entry_protein_locations\"]]) + \")\" for entry in entries])\n",
    "                        outfile.write(\">\" + item[\"metadata\"][\"accession\"] + HEADER_SEPARATOR\n",
    "                                      + entries_header + HEADER_SEPARATOR\n",
    "                                      + item[\"metadata\"][\"name\"] + \"\\n\")\n",
    "                    else:\n",
    "                        outfile.write(\">\" + item[\"metadata\"][\"accession\"] + HEADER_SEPARATOR + item[\"metadata\"][\"name\"] + \"\\n\")\n",
    "                        pass\n",
    "                    seq = item[\"extra_fields\"][\"sequence\"]\n",
    "                    fastaSeqFragments = [seq[0+i:LINE_LENGTH+i] for i in range(0, len(seq), LINE_LENGTH)]\n",
    "                    for fastaSeqFragment in fastaSeqFragments:\n",
    "                        outfile.write(fastaSeqFragment + \"\\n\")\n",
    "                    # Don't overload the server, give it time before asking for more\n",
    "                if next:\n",
    "                    sleep(1)\n",
    "                    \n",
    "if __name__ == \"__main__\" :\n",
    "    with Pool(20) as p :\n",
    "        p.map(get_IPR , urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff332ef",
   "metadata": {},
   "source": [
    "***\n",
    "> E. <b>Make the database "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1cbebd",
   "metadata": {},
   "source": [
    "Detect the IPR entries grouping proteins which signature type is a PRINT, domain ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b56816b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "path_db = \"/home/conchae/databases/depolymerase_building\"\n",
    "path_out = \"/home/conchae/databases/depolymerase_building/IPR_domain_multi\"\n",
    "\n",
    "#path_db = \"/home/robbyce/Documents/bioinformatics/depolymerase_building\"\n",
    "#path_out = \"/home/robbyce/Documents/bioinformatics/depolymerase_building/IPR_domain_multi\"\n",
    "\n",
    "# The proteins which signature span one domain ==> get the domain\n",
    "# more than 1 domain ==> keep the full sequence \n",
    "with open(f\"{path_db}/to_be_blamed.txt\", \"w\") as err_out :\n",
    "    for ipr_multi in tqdm(IPR_depo):\n",
    "        if ipr_multi :\n",
    "            with open(f\"{path_out}/{ipr_multi}.domain.sequences.corrected.fasta\", \"w\") as outfile :\n",
    "                for record in SeqIO.parse(f\"{path_db}/IPR_multi_fasta/{ipr_multi}.entry.sequences.fasta\", \"fasta\"):\n",
    "                    mid_info = record.id.split(\"|\")[1]\n",
    "                    # If the signature is a single domain\n",
    "                    if mid_info.count(\";\") == 0 :\n",
    "                        start = int(mid_info.split(\"...\")[0].split(\"(\")[1])\n",
    "                        end = int(mid_info.split(\"...\")[1].split(\")\")[0])\n",
    "                        sequence = record.seq[start : end]\n",
    "                        tag = \"__tag__domain\"\n",
    "                    # If it is more than one domain ==> keep the entire sequence\n",
    "                    else :\n",
    "                        sequence = record.seq\n",
    "                        tag = \"__tag__full\"\n",
    "                    if len(sequence) < 5000 : \n",
    "                        outfile.write(f\">{record.id}{tag}\\n{sequence}\\n\")\n",
    "                    else :\n",
    "                        err_out.write(f\"{ipr_multi}\\t{record.id}\\t{len(sequence)}\\n\")\n",
    "                        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2647d88a",
   "metadata": {},
   "source": [
    "2. Build MSA of the sequences associated with each IPR entry : FAMSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a823672b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "path_db = \"/home/conchae/databases/depolymerase_building\"\n",
    "\n",
    "def run_famsa(path_in, path_out) :\n",
    "    import subprocess\n",
    "    #align_cmd = f\"famsa -gt sl -dist_export {path_in}.dist -pid -t 10 {path_in} {path_out}\"\n",
    "    align_cmd = f\"famsa -gt sl -t 10 {path_in} {path_out}\"\n",
    "    align_subprocess = subprocess.Popen (align_cmd , shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT)\n",
    "    align_out, align_err = align_subprocess.communicate()\n",
    "    return align_out\n",
    "\n",
    "paths = pd.DataFrame()\n",
    "for ipr_multi in tqdm(IPR_depo):\n",
    "    if ipr_multi :\n",
    "        in_path = f\"{path_db}/IPR_domain_multi/{ipr_multi}.domain.sequences.corrected.fasta\"\n",
    "        out_path = f\"{path_db}/IPR_MSA/{ipr_multi}.domain.corrected.MSA.fasta\"\n",
    "        run_famsa(in_path, out_path)\n",
    "        paths = paths.append({\"Ins\" : in_path, \"Outs\" : out_path}, ignore_index = True)\n",
    "    \n",
    "run = list(map(run_famsa , paths[\"Ins\"].to_list(),paths[\"Outs\"].to_list()))\n",
    "# *****************************************************************************************************************************\n",
    "#!/bin/bash\n",
    "#BATCH --job-name=MSA_IPR__\n",
    "#SBATCH --partition=medium\n",
    "#SBATCH --ntasks=1 \n",
    "#SBATCH --cpus-per-task=30 \n",
    "#SBATCH --mem=200gb \n",
    "#SBATCH --time=02-00:00:00 \n",
    "#SBATCH --output=MSA_IPR__%j.log \n",
    "\n",
    "source /storage/apps/ANACONDA/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate bio_phylo\n",
    "\n",
    "python /home/conchae/databases/depolymerase_building/script_files/MSA_ipr_domains.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b137d681",
   "metadata": {},
   "source": [
    "3. Filter the MSA based on the pairwise penrcentage identity : eliminate one sequence of the pairs that have >95% id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fae1e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "path_db = \"/home/conchae/databases/depolymerase_building\"\n",
    "\n",
    "def filter_MSA(i_file) :\n",
    "    import subprocess\n",
    "    out_path = \"/\".join(i_file.split(\"/\")[0:-1])\n",
    "    file_name = i_file.split(\"/\")[-1].split(\".fasta\")[0]\n",
    "    o_file = f\"{out_path}/{file_name}.filtered.fasta\"    \n",
    "    # ***********************************\n",
    "    filter_cmd = f\"hhfilter -i {i_file} -o {o_file} -id 95\"\n",
    "    filter_subprocess = subprocess.Popen (filter_cmd , shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT)\n",
    "    filter_out, filter_err = filter_subprocess.communicate()\n",
    "    \n",
    "list_msa = [path_db+\"/IPR_v2/\"+ipr for ipr in os.listdir(f\"{path_db}/IPR_v2\") if ipr.count(\"corrected.MSA\")>0]\n",
    "results = list(map(filter_MSA, list_msa))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52e09b8",
   "metadata": {},
   "source": [
    "4. Build a hmm profile from the final set of kept sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e8f5eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_hmm(i_file) :\n",
    "    import subprocess\n",
    "    out_path = \"/\".join(i_file.split(\"/\")[0:-1])\n",
    "    file_name = i_file.split(\"/\")[-1].split(\".a3m\")[0]\n",
    "    o_file = f\"{out_path}/{file_name}.hmm\"    \n",
    "    # ***********************************\n",
    "    build_cmd = f\"hmmbuild {o_file} {i_file}\"\n",
    "    build_subprocess = subprocess.Popen (build_cmd , shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT)\n",
    "    filter_out, filter_err = build_subprocess.communicate()\n",
    "    print(o_file , filter_out , filter_err)\n",
    "    \n",
    "list_msa = [path_db+\"/IPR_v2/\"+ipr for ipr in os.listdir(f\"{path_db}/IPR_v2\") if ipr[-3:]==\"a3m\"]\n",
    "results = list(map(build_hmm, list_msa))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08944824",
   "metadata": {},
   "source": [
    "5. Append the hmm profiles into a single file, then press it to make the final database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2748484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "path_db = \"/home/conchae/databases/depolymerase_building\"\n",
    "\n",
    "# Single file with all profiles\n",
    "with open(f\"{path_db}/depolymerase_db.v2301.hmm\", \"w\") as outfile :\n",
    "    for file in os.listdir(f\"{path_db}/IPR_MSA\") :\n",
    "        if file[-3:]==\"hmm\" :\n",
    "            ipr_content = open(f\"{path_db}/IPR_MSA/{file}\").read()\n",
    "            outfile.write(ipr_content+\"\\n\")\n",
    "\n",
    "# Single file with all a3m\n",
    "with open(f\"{path_db}/depolymerase_db.v2301.a3m\", \"w\") as outfile :\n",
    "    for file in os.listdir(f\"{path_db}/IPR_MSA\") :\n",
    "        if file[-3:]==\"a3m\" :\n",
    "            ipr_content = open(f\"{path_db}/IPR_MSA/{file}\").read()\n",
    "            outfile.write(ipr_content+\"\\n\")\n",
    "\n",
    "\n",
    "# ! conda activate HH-suite3\n",
    "export PATH=\"/media/concha-eloko/Linux/softwares/hh-suite/lib/ffindex/src:$PATH\"\n",
    "HHLIB=\"/media/concha-eloko/Linux/conda_envs/HH-suite3\"\n",
    "# modify the script hhsuitedb.py : \n",
    "# l 110 :  hhlib_environment = os.environ['HHLIB'] -- > hhlib_environment = \"/media/concha-eloko/Linux/conda_envs/HH-suite3\"\n",
    "python3 /media/concha-eloko/Linux/softwares/hh-suite/scripts/hhsuitedb.py \\\n",
    "-o /media/concha-eloko/Linux/depolymerase_project/DBsuite_depolymerase/depolymerase_db.suite \\\n",
    "--ihhm=/media/concha-eloko/Linux/depolymerase_project/clean_files/*.hmm \\\n",
    "--ia3m=/media/concha-eloko/Linux/depolymerase_project/clean_files/*.a3m \\\n",
    "--cpu=2 \\\n",
    "--force\n",
    "\n",
    "# ***************************************************************************************************************************************************\n",
    "# Build the databse by hand :\n",
    "/usr/share/hhsuite/bin/ffindex_build pdb_full_a3m.ffdata pdb_full_a3m.ffindex a3m/\n",
    "/usr/share/hhsuite/bin/ffindex_build pdb_full_hhm.ffdata pdb_full_hhm.ffindex hhm/\n",
    "LC_ALL=C sort pdb_full_hhm.ffindex > pdb_full_hhm.ffindex.simpleSort\n",
    "LC_ALL=C sort pdb_full_a3m.ffindex > pdb_full_a3m.ffindex.simpleSort\n",
    "mv pdb_full_a3m.ffindex pdb_full_a3m.ffindex.orig\n",
    "mv pdb_full_hhm.ffindex pdb_full_hhm.ffindex.orig\n",
    "ln -s pdb_full_a3m.ffindex.simpleSort pdb_full_a3m.ffindex\n",
    "ln -s pdb_full_hhm.ffindex.simpleSort pdb_full_hhm.ffindex\n",
    "export OMP_NUM_THREADS=$(nproc)\n",
    "/usr/share/hhsuite/bin/cstranslate  -A /usr/share/hhsuite/data/cs219.lib -D /usr/share/hhsuite/data/context_data.lib -x 0.3 -c 4 -f -i pdb_full_a3m -o pdb_full_cs219 -I a3m -b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee93e25",
   "metadata": {},
   "source": [
    "***\n",
    "## The function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f92f48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import subprocess\n",
    "from multiprocessing import Pool\n",
    "path_classic = \"/home/conchae/77_strains_phage_project/annotation_classic\"\n",
    "\n",
    "paths = [] \n",
    "for phage in os.listdir(f\"{path_classic}\") :\n",
    "    try : \n",
    "        os.mkdir(f\"{path_classic}/{phage}/DBsuite_depo\")\n",
    "    except Exception as e :\n",
    "        pass\n",
    "    paths_phage = [f\"{path_classic}/{phage}/uni30/{file}\" for file in os.listdir(f\"{path_classic}/{phage}/uni30\") if file[-12:]==\"filtered.a3m\"]\n",
    "    #paths = [f\"{path_decipher}/{prot.split('__')[0]}/mmseqs_out/{prot.split('__')[1]}/{prot.split('.fasta')[0]}.MSA.a2m\" for prot in all_proteins[\"proteins\"] \n",
    "         #if os.path.isfile(f\"{path_decipher}/{prot.split('__')[0]}/mmseqs_out/{prot.split('__')[1]}/{prot.split('.fasta')[0]}.MSA.a2m\")==True \n",
    "         #if os.path.isfile(f\"{path_decipher}/{prot.split('__')[0]}/DBsuite_depo/{prot.split('__')[1]}/{prot.split('.fasta')[0]}.suite.hhr\")==False]\n",
    "\n",
    "    paths = paths + paths_phage\n",
    "    \n",
    "def scan_depolymerase(path_query) :\n",
    "    path_db = \"/home/conchae/databases/depolymerase_building/DBsuite_depolymerase/depolymerase_db.suite\"\n",
    "    path_out = f\"{'/'.join(path_query.split('/')[0:-2])}/DBsuite_depo\"\n",
    "    query = path_query.split(\"/\")[-1].split(\".fasta\")[0]\n",
    "    hhmscan_cmmd = f\"hhsearch -i {path_query} -d  {path_db} -o {path_out}/{query}.suite.hhr -blasttab {path_out}/{query}.suite.tab -ohhm {path_out}/{query}.suite.ohhm\"\n",
    "    hhmscan_process = subprocess.Popen(hhmscan_cmmd, shell =True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    scan_out, scan_err = hhmscan_process.communicate()\n",
    "    return (scan_out, scan_err, query)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with Pool(10) as p:\n",
    "        p.map(scan_depolymerase, paths)\n",
    "        \n",
    "# *********************************************************** \n",
    "#!/bin/bash\n",
    "#BATCH --job-name=scan\n",
    "#SBATCH --partition=short\n",
    "#SBATCH --qos=short\n",
    "#SBATCH --ntasks=1 \n",
    "#SBATCH --cpus-per-task=40 \n",
    "#SBATCH --mem=100gb \n",
    "#SBATCH --time=00-05:00:00 \n",
    "#SBATCH --output=debug_MSA_generate__%j.log \n",
    "\n",
    "source /storage/apps/ANACONDA/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate HH-suite3\n",
    "\n",
    "python /home/conchae/77_strains_phage_project/script_files/py_files/depo_hmmscan.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b696bd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import subprocess\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import pandas as pd\n",
    "# Generate a list of path of filtered MSA\n",
    "path_decipher = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_decipher/phageboost/phageboost_70_20102022\"\n",
    "path_files = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/15122022_session\"\n",
    "\n",
    "all_proteins = pd.read_csv(f\"{path_files}/part_III_ptB/all_prophage_proteins.names.db.fasta\", names = [\"proteins\"])\n",
    "\n",
    "paths = [f\"{path_decipher}/{prot.split('__')[0]}/mmseqs_out/{prot.split('__')[1]}/{prot.split('.fasta')[0]}.MSA.a2m\" for prot in all_proteins[\"proteins\"] \n",
    "         if os.path.isfile(f\"{path_decipher}/{prot.split('__')[0]}/mmseqs_out/{prot.split('__')[1]}/{prot.split('.fasta')[0]}.MSA.a2m\")==True \n",
    "         if os.path.isfile(f\"{path_decipher}/{prot.split('__')[0]}/DBsuite_depo/{prot.split('__')[1]}/{prot.split('.fasta')[0]}.suite.hhr\")==False]\n",
    "\n",
    "\n",
    "def scan_depolymerase(path_query) :\n",
    "    path_db = \"/home/conchae/databases/depolymerase_building/DBsuite_depolymerase/depolymerase_db.suite\"\n",
    "    phage = path_query.split(\"__\")[1]\n",
    "    query = path_query.split(\"/\")[-1].split(\".MSA\")[0]\n",
    "    path_dir_short = f\"{'/'.join(path_query.split('/')[0:-3])}/DBsuite_depo\"\n",
    "    path_dir = f\"{'/'.join(path_query.split('/')[0:-3])}/DBsuite_depo/{phage}\"\n",
    "    try :\n",
    "        os.mkdir(path_dir_short)\n",
    "    except FileExistsError :\n",
    "        pass\n",
    "    try :\n",
    "        os.mkdir(path_dir)\n",
    "    except FileExistsError :\n",
    "        pass\n",
    "    path_out = f\"{path_dir}/{query}\"\n",
    "    hhmscan_cmmd = f\"hhsearch -i {path_query} -d  {path_db} -o {path_out}.suite.hhr -blasttab {path_out}.suite.tab\"\n",
    "    hhmscan_process = subprocess.Popen(hhmscan_cmmd, shell =True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    scan_out, scan_err = hhmscan_process.communicate()\n",
    "    with open(f\"{path_files}/hhsearch_done\",\"a+\") as outfile :\n",
    "        outfile.write(f\"{path_query}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with Pool(30) as p:\n",
    "        p.map(scan_depolymerase, paths)\n",
    "        \n",
    "        \n",
    "# *********************************************************** \n",
    "#!/bin/bash\n",
    "#BATCH --job-name=scan_ppt__\n",
    "#SBATCH --qos=long\n",
    "#SBATCH --ntasks=1 \n",
    "#SBATCH --cpus-per-task=60 \n",
    "#SBATCH --mem=200gb \n",
    "#SBATCH --time=10-00:00:00 \n",
    "#SBATCH --output=scan_ppt__%j.log \n",
    "\n",
    "source /storage/apps/ANACONDA/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate HH-suite3\n",
    "\n",
    "python /home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/15122022_session/script_files/part_III/hhsearch.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML_work_v2]",
   "language": "python",
   "name": "conda-env-ML_work_v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
