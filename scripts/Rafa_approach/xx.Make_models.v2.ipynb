{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b57b268-5162-4176-9027-82cd18129737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import random\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import logging\n",
    "import subprocess\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import joblib\n",
    "\n",
    "# SCikitlearn modules :\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report , roc_auc_score, matthews_corrcoef\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Scipy modules : \n",
    "from scipy.stats import fisher_exact\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from statistics import mean\n",
    "\n",
    "path_work = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023/Seqbased_model\"\n",
    "path_models = f\"{path_work}/RF_1302_models\"\n",
    "path_testing = f\"{path_work}/RF_1302_data\"\n",
    "\n",
    "DF_info = pd.read_csv(f\"{path_work}/TropiGATv2.final_df_v2.tsv\", sep = \"\\t\" ,  header = 0)\n",
    "df_prophages = DF_info.drop_duplicates(subset = [\"Phage\"], keep = \"first\")\n",
    "dico_prophage_info = {row[\"Phage\"] : {\"prophage_strain\" : row[\"prophage_id\"] , \"ancestor\" : row[\"Infected_ancestor\"]} for _,row in df_prophages.iterrows()}\n",
    "\n",
    "def get_filtered_prophages(prophage) :\n",
    "    combinations = []\n",
    "    to_exclude = set()\n",
    "    to_keep = set()\n",
    "    to_keep.add(prophage)\n",
    "    df_prophage_group = DF_info[(DF_info[\"prophage_id\"] == dico_prophage_info[prophage][\"prophage_strain\"]) & (DF_info[\"Infected_ancestor\"] == dico_prophage_info[prophage][\"ancestor\"])]\n",
    "    if len(df_prophage_group) == 1 : \n",
    "        pass\n",
    "    else :\n",
    "        depo_set = set(df_prophage_group[df_prophage_group[\"Phage\"] == prophage][\"domain_seq\"].values)\n",
    "        for prophage_tmp in df_prophage_group[\"Phage\"].unique().tolist() :\n",
    "            if prophage_tmp != prophage :\n",
    "                tmp_depo_set = set(df_prophage_group[df_prophage_group[\"Phage\"] == prophage_tmp][\"domain_seq\"].values)\n",
    "                if depo_set == tmp_depo_set :\n",
    "                    to_exclude.add(prophage_tmp)\n",
    "                else :\n",
    "                    if tmp_depo_set not in combinations :\n",
    "                        to_keep.add(prophage_tmp)\n",
    "                        combinations.append(tmp_depo_set)\n",
    "                    else :\n",
    "                        to_exclude.add(prophage_tmp)\n",
    "    return df_prophage_group , to_exclude , to_keep\n",
    "\n",
    "good_prophages = set()\n",
    "excluded_prophages = set()\n",
    "\n",
    "for prophage, info_prophage in tqdm(dico_prophage_info.items()) :\n",
    "    if prophage not in excluded_prophages and prophage not in good_prophages:\n",
    "        _, excluded_members , kept_members = get_filtered_prophages(prophage) \n",
    "        good_prophages.update(kept_members)\n",
    "        excluded_prophages.update(excluded_members)\n",
    "\n",
    "DF_info_lvl_0_filtered = DF_info[DF_info[\"Phage\"].isin(good_prophages)]\n",
    "DF_info_lvl_0_final = DF_info_lvl_0_filtered[~DF_info_lvl_0_filtered[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "\n",
    "DF_info_lvl_0 = DF_info_lvl_0_final.copy()\n",
    "\n",
    "# useful dictionary :\n",
    "KLtype_count = Counter(DF_info_lvl_0[\"KL_type_LCA\"])\n",
    "KLtype_pred = [kltype for kltype in KLtype_count if KLtype_count[kltype] >= 10]\n",
    "\n",
    "dico_prophage_kltype_associated = {}\n",
    "for negative_index,phage in tqdm(enumerate(DF_info_lvl_0[\"Phage\"].unique().tolist())) :\n",
    "    kltypes = set()\n",
    "    dpos = DF_info_lvl_0[DF_info_lvl_0[\"Phage\"] == phage][\"index\"]\n",
    "    for dpo in dpos : \n",
    "        tmp_kltypes = DF_info_lvl_0[DF_info_lvl_0[\"index\"] == dpo][\"KL_type_LCA\"].values\n",
    "        kltypes.update(tmp_kltypes)\n",
    "    dico_prophage_kltype_associated[phage] = kltypes\n",
    "\n",
    "depo_domains_seq = {index: domain_seq for index, domain_seq in zip(DF_info_lvl_0[\"index\"], DF_info_lvl_0['domain_seq'])}\n",
    "with open(f\"{path_work}/Dpo_domains.2912.multi.fasta\" , \"w\") as outfile : \n",
    "    for index,seq in depo_domains_seq.items() : \n",
    "        outfile.write(f\">{index}\\n{seq}\\n\")\n",
    "       \n",
    "\n",
    "# ******************************************************\n",
    "# CD hit step :\n",
    "path_multi_fasta = f\"{path_work}/Dpo_domains.2912.multi.fasta\"\n",
    "path_tmp_cdhit = f\"{path_work}/cdhit_clusters_2912\"\n",
    "\n",
    "def make_cdhit_cluster(threshold) :\n",
    "    cdhit_command = f\"cd-hit -i {path_multi_fasta} -o {path_tmp_cdhit}/{threshold}.out -c {threshold} -G 0 -aL 0.8\"\n",
    "    cdhit_process = subprocess.Popen(cdhit_command, shell =True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) \n",
    "    scan_out, scan_err = cdhit_process.communicate()\n",
    "    print(scan_out, scan_err)\n",
    "\n",
    "def make_cluster_dico(cdhit_out) :\n",
    "    import json\n",
    "    dico_cluster = {}\n",
    "    threshold = cdhit_out.split(\"/\")[-1].split(\".out\")[0]\n",
    "    cluster_file = f\"{cdhit_out}.clstr\"\n",
    "    cluster_out = open(cluster_file).read().split(\">Cluster\")\n",
    "    for index,cluster in enumerate(cluster_out[1:]) :\n",
    "        tmp_dpo = []\n",
    "        id_cluster = f\"Dpo_cdhit_{index}\"\n",
    "        for _,line in enumerate(cluster.split(\"\\n\")[1:-1]) :\n",
    "            dpo = line.split(\">\")[1].split(\".\")[0]\n",
    "            tmp_dpo.append(dpo)\n",
    "        dico_cluster[id_cluster] = tmp_dpo\n",
    "    with open(f\"{path_work}/dico_cluster.cdhit__{threshold}.json\", \"w\") as outfile:\n",
    "        json.dump(dico_cluster, outfile)\n",
    "    return dico_cluster , threshold\n",
    "\n",
    "def make_DF_binaries(df_info , dico_cluster, threshold) :\n",
    "    all_dpo_binaries = []\n",
    "    for phage in df_info.Phage.unique() :\n",
    "        dpo_binary = []\n",
    "        df_phage = df_info[df_info[\"Phage\"] == phage][\"index\"].values\n",
    "        for cluster,dpos in dico_cluster.items() :\n",
    "            shared_item = bool(set(dpos) & set(df_phage))\n",
    "            if shared_item == True :\n",
    "                dpo_binary.append(1)\n",
    "            else :\n",
    "                dpo_binary.append(0)\n",
    "        all_dpo_binaries.append(dpo_binary)\n",
    "    df_dpo_prophages = pd.DataFrame(all_dpo_binaries, index=df_info.Phage.unique(), columns=dico_cluster.keys())\n",
    "    df_dpo_prophages.to_csv(f\"{path_work}/DF_binaries_{threshold}.csv\", sep = \",\", index = True, header = True)\n",
    "    return df_dpo_prophages\n",
    "\n",
    "def make_DF_kltype(df_info, df ,KL_type , dico_cluster,ratio = 5, collapse = False) : \n",
    "    # positive data :\n",
    "    positive_phages = df_info[df_info[\"KL_type_LCA\"] == KL_type][\"Phage\"].unique()\n",
    "    df_positives = df[df.index.isin(positive_phages)]\n",
    "    #df_positives = df_positives.drop_duplicates(subset = [\"Phage\"] , keep = \"first\")\n",
    "    df_positives = df_positives[~df_positives.index.duplicated(keep='first')]\n",
    "    binaries_pos = df_positives.values\n",
    "    labels_pos = [1] * len(binaries_pos)\n",
    "    phages_pos = df_positives.index\n",
    "    # negative data :\n",
    "    n_samples = len(phages_pos)\n",
    "    negative_phages = []\n",
    "    for negative_index,phage in enumerate(df_info[\"Phage\"].unique().tolist()) :\n",
    "        if KL_type not in dico_prophage_kltype_associated[phage] :\n",
    "            negative_phages.append(phage)\n",
    "    negative_phages_selected = random.sample(negative_phages, int(n_samples*ratio))\n",
    "    df_negatives = df[df.index.isin(negative_phages_selected)]\n",
    "    binaries_neg = df_negatives.values\n",
    "    labels_neg = [0] * len(binaries_neg)\n",
    "    all_binaries = np.concatenate((binaries_pos, binaries_neg)) \n",
    "    all_labels = labels_pos + labels_neg\n",
    "    all_indices = list(phages_pos) + list(negative_phages_selected)\n",
    "    df_kl = pd.DataFrame(all_binaries, index=all_indices, columns=dico_cluster.keys())\n",
    "    if collapse == True :\n",
    "        dpo_presence = [dpo for dpo in df_kl.columns if sum(df_kl[dpo]) >0]\n",
    "        df_kl = df_kl[dpo_presence]\n",
    "    return (df_kl , all_labels)\n",
    "\n",
    "\n",
    "def fit_rf_model_random_search(df_kl, all_labels, KL_type, threshold, path_models, n_splits=5, n_iters=100):\n",
    "    mcc_scores = []\n",
    "    auc_scores = []\n",
    "    report_list = []\n",
    "    best_models = []\n",
    "    test_sets = []\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=243)\n",
    "    first_split = True\n",
    "    best_params = None\n",
    "    for train_index, test_index in skf.split(df_kl, all_labels):\n",
    "        X_train, X_test = df_kl.iloc[train_index], df_kl.iloc[test_index]\n",
    "        all_labels_series = pd.Series(all_labels)\n",
    "        y_train = all_labels_series.iloc[train_index]\n",
    "        y_test = all_labels_series.iloc[test_index]        \n",
    "        # Define hyperparameter search space\n",
    "        param_grid = {\n",
    "            'bootstrap': Categorical([True, False]),\n",
    "            'max_depth': Integer(10, 100),\n",
    "            'max_features': Categorical(['auto', 'sqrt']),\n",
    "            'min_samples_leaf': Integer(1, 4),\n",
    "            'min_samples_split': Integer(2, 10),\n",
    "            'n_estimators': Integer(200, 800)\n",
    "        }\n",
    "        if first_split:\n",
    "            # Initialize Random Forest classifier\n",
    "            rf = RandomForestClassifier(random_state=42)\n",
    "            # Initialize BayesSearchCV for hyperparameter tuning\n",
    "            print(rf , param_grid, n_iters)\n",
    "            bayes_search = BayesSearchCV(rf, param_grid, n_iter=n_iters, cv=4, n_jobs=-1)\n",
    "            bayes_search.fit(X_train, y_train)\n",
    "            # Get the best hyperparameters\n",
    "            best_params = bayes_search.best_params_\n",
    "            best_model = bayes_search.best_estimator_\n",
    "            best_models.append(best_model)\n",
    "            first_split = False\n",
    "        else:\n",
    "            # Reuse the best hyperparameters\n",
    "            best_model = RandomForestClassifier(random_state=42, **best_params)\n",
    "            best_model.fit(X_train, y_train)\n",
    "            best_models.append(best_model)\n",
    "        print(\"that round worked\")\n",
    "        print(train_index, test_index, best_models)\n",
    "        # Make predictions on testing data\n",
    "        predictions = best_model.predict(X_test)\n",
    "        # Calculate metrics\n",
    "        mcc = matthews_corrcoef(y_test, predictions)\n",
    "        mcc_scores.append(mcc)\n",
    "        auc = roc_auc_score(y_test, predictions) # Calculate AUC score\n",
    "        auc_scores.append(auc)  # Store AUC score\n",
    "        # Get classification report\n",
    "        report = classification_report(y_test, predictions, output_dict=True)\n",
    "        report_list.append(report)\n",
    "        # Save the testing set\n",
    "        test_sets.append((X_test, y_test))\n",
    "    # Calculate means\n",
    "    mean_mcc = mean(mcc_scores)\n",
    "    mean_auc = mean(auc_scores)  # Calculate mean AUC score\n",
    "    # Calculate average report across all folds\n",
    "    avg_report = {\n",
    "        'precision': {key: mean([r['precision'][key] for r in report_list]) for key in report_list[0]['precision']},\n",
    "        'recall': {key: mean([r['recall'][key] for r in report_list]) for key in report_list[0]['recall']},\n",
    "        'f1-score': {key: mean([r['f1-score'][key] for r in report_list]) for key in report_list[0]['f1-score']},\n",
    "        'support': {key: mean([r['support'][key] for r in report_list]) for key in report_list[0]['support']},\n",
    "        'accuracy': {key: mean([r['accuracy'][key] for r in report_list]) for key in report_list[0]['accuracy']}\n",
    "    }\n",
    "    # Save the best model\n",
    "    joblib.dump(best_model, f'{path_models}/{threshold}_RF_{KL_type}.joblib')\n",
    "    # Save the corresponding testing set\n",
    "    best_model_index = mcc_scores.index(max(mcc_scores))\n",
    "    joblib.dump(test_sets[best_model_index], f'{path_models}/{threshold}_test_data_{KL_type}.joblib')\n",
    "    return mean_mcc, avg_report, mean_auc\n",
    "    \n",
    "#list(map(make_cdhit_cluster , [0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 0.975]))\n",
    "cdhit_files = [f\"{path_tmp_cdhit}/{file}\" for file in os.listdir(path_tmp_cdhit) if file[-3:]==\"out\"]\n",
    "\n",
    "def make_prediction_file (path_file) :\n",
    "    dico_cluster , threshold = make_cluster_dico(f\"{path_file}\")\n",
    "    df_binaries = make_DF_binaries(DF_info_lvl_0 , dico_cluster, threshold)\n",
    "    for KL_type in KLtype_count :\n",
    "        if KLtype_count[KL_type] < 5 :\n",
    "            if os.path.isfile(f'{path_models}/{threshold}_RF_{KL_type}.full_data.joblib') == False :\n",
    "                df_kl , all_labels = make_DF_kltype(DF_info_lvl_0 ,df_binaries, KL_type , dico_cluster, collapse = False)\n",
    "                fit_rf_model_random_search(df_kl , all_labels, KL_type,threshold,path_models)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with ThreadPool(10) as p:\n",
    "        p.map(make_prediction_file, cdhit_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2eb810-ab82-4786-a993-c8ed2114a57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction_file (path_file) :\n",
    "    dico_cluster , threshold = make_cluster_dico(f\"{path_file}\")\n",
    "    df_binaries = make_DF_binaries(DF_info_lvl_0 , dico_cluster, threshold)\n",
    "    for KL_type in KLtype_count :\n",
    "        if KLtype_count[KL_type] < 5 :\n",
    "            if os.path.isfile(f'{path_models}/{threshold}_RF_{KL_type}.full_data.joblib') == False :\n",
    "                df_kl , all_labels = make_DF_kltype(DF_info_lvl_0 ,df_binaries, KL_type , dico_cluster, collapse = False)\n",
    "                fit_rf_model_random_search(df_kl , all_labels, KL_type,threshold,path_models)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20373355-0667-40c1-9947-b55889149950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_rf_model_random_search(df_kl, all_labels, KL_type, threshold, path_models, n_splits=5, n_iters=100):\n",
    "    data_kltype = {}\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=243)\n",
    "    first_split = True\n",
    "    best_params = None\n",
    "    n = 0\n",
    "    for train_index, test_index in skf.split(df_kl, all_labels):\n",
    "        tmp_dico = {}\n",
    "        X_train, X_test = df_kl.iloc[train_index], df_kl.iloc[test_index]\n",
    "        all_labels_series = pd.Series(all_labels)\n",
    "        y_train = all_labels_series.iloc[train_index]\n",
    "        y_test = all_labels_series.iloc[test_index]\n",
    "        # Define hyperparameter search space\n",
    "        param_grid = {\n",
    "            'bootstrap': Categorical([True, False]),\n",
    "            'max_depth': Integer(10, 100),\n",
    "            'max_features': Categorical(['auto', 'sqrt']),\n",
    "            'min_samples_leaf': Integer(1, 4),\n",
    "            'min_samples_split': Integer(2, 10),\n",
    "            'n_estimators': Integer(200, 800)\n",
    "        }\n",
    "        if first_split:\n",
    "            rf = RandomForestClassifier(random_state=42)\n",
    "            # Initialize BayesSearchCV for hyperparameter tuning\n",
    "            print(rf , param_grid, n_iters)\n",
    "            bayes_search = BayesSearchCV(rf, param_grid, n_iter=n_iters, cv=4, n_jobs=-1)\n",
    "            bayes_search.fit(X_train, y_train)\n",
    "            # Get the best hyperparameters\n",
    "            best_params = bayes_search.best_params_\n",
    "            best_model = bayes_search.best_estimator_\n",
    "            first_split = False\n",
    "        else:\n",
    "            # Reuse the best hyperparameters\n",
    "            best_model = RandomForestClassifier(random_state=42, **best_params)\n",
    "            best_model.fit(X_train, y_train)\n",
    "        # Make predictions on testing data\n",
    "        predictions = best_model.predict(X_test)\n",
    "        # Calculate metrics\n",
    "        a = {n : {\"best_parameters\" : bayes_search.best_params_,\n",
    "             \"model\" : best_model,\n",
    "             \"test_data\" : (y_test, predictions),\n",
    "             \"test_&_model_predictions\" : (X_test, y_test),\n",
    "             \"iteration\" : n}}\n",
    "        tmp_dico.update(a)\n",
    "        n += 1\n",
    "        print(KL_type,tmp_dico)\n",
    "    data_kltype[KL_type] = tmp_dico\n",
    "    # Save the best model\n",
    "    joblib.dump(data_kltype, f'{path_models}/{threshold}_RF_{KL_type}.full_data.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da455fa-5f4f-498a-89b5-4b7ca9b3c43a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87de2b5-b50f-44aa-a846-645f95883160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import random\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import logging\n",
    "import subprocess\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import joblib\n",
    "\n",
    "# SCikitlearn modules :\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report , roc_auc_score, matthews_corrcoef\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Scipy modules : \n",
    "from scipy.stats import fisher_exact\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from statistics import mean\n",
    "\n",
    "path_work = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023/Seqbased_model\"\n",
    "path_models = f\"{path_work}/RF_1302_models\"\n",
    "path_testing = f\"{path_work}/RF_1302_data\"\n",
    "\n",
    "\n",
    "joblib.open(f'{path_models}/{threshold}_RF_{KL_type}.full_data.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7ef374-a504-4a2a-88e0-494c9e69b63a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21149cbb-7e9a-4db9-b816-8b904d52a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsync -avzhe ssh \\\n",
    "conchae@garnatxa.srv.cpd:/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023/Seqbased_model/RF_1302_models \\\n",
    "/media/concha-eloko/Linux/PPT_clean/ficheros_28032023\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db18503a-ba49-4213-bf5b-215bf28e0879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e55f088-7d93-4e89-b36d-6b5365a234da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import random\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import logging\n",
    "import subprocess\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import joblib\n",
    "\n",
    "# SCikitlearn modules :\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report , roc_auc_score, matthews_corrcoef\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Scipy modules : \n",
    "from scipy.stats import fisher_exact\n",
    "#from skopt import BayesSearchCV\n",
    "#from skopt.space import Real, Categorical, Integer\n",
    "from statistics import mean\n",
    "\n",
    "path_jobs = \"/media/concha-eloko/Linux/PPT_clean/ficheros_28032023/RF_1302_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc4e44ef-3539-4063-a228-c1eb5997e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_job = joblib.load(f\"{path_jobs}/0.8_RF_KL1.TEST.full_data.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9243aec7-e522-4970-91a3-176ce0e9a772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18      1\n",
       " 21      1\n",
       " 24      1\n",
       " 26      1\n",
       " 28      1\n",
       "        ..\n",
       " 1121    0\n",
       " 1123    0\n",
       " 1129    0\n",
       " 1133    0\n",
       " 1134    0\n",
       " Length: 228, dtype: int64,\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eg_job[2][\"test_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7731951d-d721-444c-99be-29b34eefd88a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_work_v2",
   "language": "python",
   "name": "ml_work_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
