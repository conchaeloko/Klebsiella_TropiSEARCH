{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9ad3df-def3-4180-a0c1-e531b8f948dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsync -avzhe ssh \\\n",
    "conchae@garnatxa.srv.cpd:/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023/train_nn/TropiGATv2.final_df.tsv \\\n",
    "/media/concha-eloko/Linux/PPT_clean/  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cab11f-4c71-44d2-9741-d720e3b2e7ab",
   "metadata": {},
   "source": [
    "***\n",
    "# Load Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b0f74dd6-84d6-46e0-9284-58dda610c393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import random\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import logging\n",
    "import subprocess\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# SCikitlearn modules :\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report , roc_auc_score\n",
    "\n",
    "# Scipy modules : \n",
    "from scipy.stats import fisher_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34a1dbac-1975-468e-b565-e620eaf0b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Load the Dataframes :\n",
    "#path_work = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023/Seqbased_model\"\n",
    "path_work = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "\n",
    "DF_info = pd.read_csv(f\"{path_work}/TropiGATv2.final_df.tsv\", sep = \"\\t\" ,  header = 0)\n",
    "DF_info_lvl_0 = DF_info[~DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "DF_info_lvl_0 = DF_info_lvl_0.drop_duplicates(subset = [\"Infected_ancestor\",\"index\",\"prophage_id\"] , keep = \"first\").reset_index(drop=True)\n",
    "\n",
    "# useful dictionary :\n",
    "dico_prophage_kltype_associated = {}\n",
    "for negative_index,phage in tqdm(enumerate(DF_info_lvl_0[\"Phage\"].unique().tolist())) :\n",
    "    kltypes = set()\n",
    "    dpos = DF_info_lvl_0[DF_info_lvl_0[\"Phage\"] == phage][\"index\"]\n",
    "    for dpo in dpos : \n",
    "        tmp_kltypes = DF_info_lvl_0[DF_info_lvl_0[\"index\"] == dpo][\"KL_type_LCA\"].values\n",
    "        kltypes.update(tmp_kltypes)\n",
    "    dico_prophage_kltype_associated[phage] = kltypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdc1f21-3e78-4942-aa51-86f34bd58d84",
   "metadata": {},
   "source": [
    "> Make fasta file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f7af202-5a07-4f9a-9007-feddb080ac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "depo_domains_seq = {index: domain_seq for index, domain_seq in zip(DF_info_lvl_0[\"index\"], DF_info_lvl_0['domain_seq'])}\n",
    "\n",
    "with open(f\"{path_work}/Dpo_domains.1710.multi.fasta\" , \"w\") as outfile : \n",
    "    for index,seq in depo_domains_seq.items() : \n",
    "        outfile.write(f\">{index}\\n{seq}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8de7aae-d3d8-4958-b311-e697a5b76de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'/bin/sh: 1: cd-hit: not found\\n' None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_multi_fasta = f\"{path_work}/Dpo_domains.1710.multi.fasta\"\n",
    "path_tmp_cdhit = f\"{path_work}/cdhit_clusters_1710\"\n",
    "\n",
    "def make_cdhit_cluster(threshold) :\n",
    "    cdhit_command = f\"cd-hit -i {path_multi_fasta} -o {path_tmp_cdhit}/{threshold}.out -c {threshold} -G 0 -aL 0.8\"\n",
    "    cdhit_process = subprocess.Popen(cdhit_command, shell =True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) \n",
    "    scan_out, scan_err = cdhit_process.communicate()\n",
    "    print(scan_out, scan_err)\n",
    "    \n",
    "list(map(make_cdhit_cluster , [0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 0.975]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c921692a-215c-4e2b-b2b0-d8322c4a6de2",
   "metadata": {},
   "source": [
    "> Make DF for each threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9474a0ac-6c96-4e41-8480-cde89c90cdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cluster_dico(cdhit_out) :\n",
    "    import json\n",
    "    dico_cluster = {}\n",
    "    threshold = cdhit_out.split(\"/\")[-1].split(\".out\")[0]\n",
    "    cluster_out = open(cdhit_out).read().split(\">Cluster\")\n",
    "    for index,cluster in enumerate(cluster_out[1:]) :\n",
    "        tmp_dpo = []\n",
    "        id_cluster = f\"Dpo_cdhit_{index}\"\n",
    "        for _,line in enumerate(cluster.split(\"\\n\")[1:-1]) :\n",
    "            dpo = line.split(\">\")[1].split(\".\")[0]\n",
    "            tmp_dpo.append(dpo)\n",
    "        dico_cluster[id_cluster] = tmp_dpo\n",
    "    with open(f\"{path_work}/dico_cluster.cdhit__{threshold}.json\", \"w\") as outfile:\n",
    "        json.dump(dico_cluster, outfile)\n",
    "    return dico_cluster , threshold\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bda9dde9-6d76-4d38-a4fb-bd33064728d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_DF_binaries(df_info , dico_cluster, threshold) :\n",
    "    all_dpo_binaries = []\n",
    "    for phage in df_info.Phage.unique() :\n",
    "        dpo_binary = []\n",
    "        df_phage = df_info[df_info[\"Phage\"] == phage][\"index\"].values\n",
    "        for cluster,dpos in dico_cluster.items() :\n",
    "            shared_item = bool(set(dpos) & set(df_phage))\n",
    "            if shared_item == True :\n",
    "                dpo_binary.append(1)\n",
    "            else :\n",
    "                dpo_binary.append(0)\n",
    "        all_dpo_binaries.append(dpo_binary)\n",
    "    df_dpo_prophages = pd.DataFrame(all_dpo_binaries, index=df_info.Phage.unique(), columns=dico_cluster.keys())\n",
    "    df_dpo_prophages.to_csv(f\"{path_work}/DF_binaries_{threshold}.csv\", sep = \",\", index = True, header = True)\n",
    "\n",
    "    return df_dpo_prophages\n",
    "\n",
    "#import json\n",
    "#cdhit_json = json.load(open(f\"/media/concha-eloko/Linux/PPT_clean/Rafa_task/sequence_similarity/dico_cluster.cdhit__0.75.json\"))\n",
    "#df = make_DF_binaries(DF_info_lvl_0 ,cdhit_json , 0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c56e866-8e78-44d1-b2ce-c179ec423ec0",
   "metadata": {},
   "source": [
    "> Make DF for modelization :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aea0349e-1edb-47d0-b41c-f3448e59093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_DF_kltype(df_info ,KL_type , dico_cluster,ratio = 5, collapse = True) : \n",
    "    # positive data :\n",
    "    positive_phages = df_info[df_info[\"KL_type_LCA\"] == KL_type][\"Phage\"].unique()\n",
    "    df_positives = df[df.index.isin(positive_phages)]\n",
    "    binaries_pos = df_positives.values\n",
    "    labels_pos = [1] * len(binaries_pos)\n",
    "    phages_pos = df_positives.index\n",
    "    # negative data :\n",
    "    n_samples = len(phages_pos)\n",
    "    negative_phages = []\n",
    "    for negative_index,phage in enumerate(df_info[\"Phage\"].unique().tolist()) :\n",
    "        if KL_type not in dico_prophage_kltype_associated[phage] :\n",
    "            negative_phages.append(phage)\n",
    "    negative_phages_selected = random.sample(negative_phages, int(n_samples*ratio))\n",
    "    df_negatives = df[df.index.isin(negative_phages_selected)]\n",
    "    binaries_neg = df_negatives.values\n",
    "    labels_neg = [0] * len(binaries_neg)\n",
    "\n",
    "    all_binaries = np.concatenate((binaries_pos, binaries_neg)) \n",
    "    all_labels = labels_pos + labels_neg\n",
    "    all_indices = list(phages_pos) + list(negative_phages_selected)\n",
    "\n",
    "    df_kl = pd.DataFrame(all_binaries, index=all_indices, columns=dico_cluster.keys())\n",
    "    if collapse == True :\n",
    "        dpo_presence = [dpo for dpo in df_kl.columns if sum(df_kl[dpo]) >0]\n",
    "        df_kl = df_kl[dpo_presence]\n",
    "        \n",
    "    return (df_kl , all_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "386a250e-ae51-419b-bcc0-ed0fa7056a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df , test_labels = make_DF_kltype(DF_info_lvl_0 , \"KL64\", 2, cdhit_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e2dd74-fa2b-43ca-a74f-f8f2bb73c528",
   "metadata": {},
   "source": [
    "> add the negative sequences :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e832e6-c310-45db-9efa-d22e95073743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df_type(path_df) :\n",
    "    df = pd.read_csv(f\"{path_df}\" , sep = \"\\t\" , header = 0 , index_col = 0)\n",
    "    method = index_dico[path_df.split(\"/\")[-1].split(\"_\")[1].split(\".\")[1]]\n",
    "    threshold = path_df.split(\"/\")[-1].split(\"_\")[-1].split(\".csv\")[0]\n",
    "    dico_KL_prophage_id = json.load(open(f\"{path_task}/dico_prophage_ID.KLtypes.json\"))\n",
    "    labels = []\n",
    "    for index in df.index:\n",
    "        label = dico_KL_prophage_id.get(index, np.nan)\n",
    "        if len(label) == 1 :\n",
    "            labels.append(label[0])\n",
    "        else :\n",
    "            labels.append(\"Multiple\")\n",
    "    df[\"label\"] = labels\n",
    "    df = df[df['label'] != 'Multiple']\n",
    "    return df , method , threshold\n",
    "\n",
    "    \n",
    "def fit_rf_model_random_search(df):\n",
    "    le = LabelEncoder()\n",
    "    df[\"label\"] = le.fit_transform(df[\"label\"])\n",
    "    n_iter=2\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, df[\"label\"], test_size=0.2, random_state=42)\n",
    "    param_grid = {\n",
    "        'bootstrap': [True, False],\n",
    "        'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "        'max_features': ['auto', 'sqrt'],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'n_estimators': [200, 400, 600, 800]\n",
    "    }\n",
    "    rf = RandomForestClassifier()\n",
    "    rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, \n",
    "                                   n_iter=n_iter, cv=4, verbose=2, random_state=42, n_jobs=-1)\n",
    "    rf_random.fit(X_train, y_train)\n",
    "    # Print the best parameters from the Randomized Search\n",
    "    predictions = rf_random.predict(X_test)\n",
    "    proba = rf_random.predict_proba(X_test)\n",
    "    report = classification_report(y_test, predictions, output_dict=True)\n",
    "    #auc = roc_auc_score(y_test, proba, multi_class='ovr')\n",
    "    accuracy = report[\"accuracy\"]\n",
    "    weighted_precision = report[\"weighted avg\"][\"precision\"]\n",
    "    weighted_recall = report[\"weighted avg\"][\"recall\"]\n",
    "    weighted_F1 = report[\"weighted avg\"][\"f1-score\"]\n",
    "    return weighted_precision ,  weighted_recall , weighted_F1\n",
    "    \n",
    "        \n",
    "\n",
    "def fit_RF_file(path_df) :\n",
    "    df , method , threshold = make_df_type(path_df)\n",
    "    weighted_precision ,  weighted_recall , weighted_F1  = fit_rf_model_random_search(df)\n",
    "    with open(f\"{path_task}/{method}_RF_report.tsv\", \"a+\") as outfile :\n",
    "        outfile.write(f\"{method}\\t{threshold}\\t{weighted_precision}\\t{weighted_recall}\\t{weighted_F1}\\n\")\n",
    "    #return weighted_precision ,  weighted_recall , weighted_F1  , method\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with ThreadPool(10) as p:\n",
    "        p.map(fit_RF_file, path_dfs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ab1671-5707-4341-a806-db8cb4f76347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_similarity_df(cdhit_out) :\n",
    "    dico_cluster , threshold = make_cluster_dico(cdhit_out)\n",
    "    prophage_dico = make_Dpo_prophage_dico(dico_cluster)\n",
    "    df_dpo_prophages = build_dataframe(prophage_dico)\n",
    "    df_dpo_prophages.sort_index(inplace = True, ascending=True)\n",
    "    df_dpo_prophages.to_csv(f\"{path_task}/sequence_similarity/prophage_Dpo.cdhit_{threshold}.csv\", sep = \"\\t\")\n",
    "    return df_dpo_prophages\n",
    "    \n",
    "path_cdhit = [f\"{path_tmp_cdhit}/{file}\" for file in os.listdir(path_tmp_cdhit) if file[-5:] == \"clstr\"]   \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    with ThreadPool(10) as p:\n",
    "        p.map(seq_similarity_df, path_cdhit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660cf4a3-dca9-46ee-bca6-c81f1364bbaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb64207d-a653-4b11-b819-06c07c526732",
   "metadata": {},
   "source": [
    "***\n",
    "# Webserver version :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de2a18-7d92-40be-98b3-2ab34e19fa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import random\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import logging\n",
    "import subprocess\n",
    "import joblib\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# SCikitlearn modules :\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report , roc_auc_score, matthews_corrcoef\n",
    "\n",
    "# Scipy modules : \n",
    "from scipy.stats import fisher_exact\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "\n",
    "path_work = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023/Seqbased_model\"\n",
    "path_models = f\"{path_work}/RF_models\"\n",
    "\n",
    "DF_info = pd.read_csv(f\"{path_work}/TropiGATv2.final_df.tsv\", sep = \"\\t\" ,  header = 0)\n",
    "DF_info_lvl_0 = DF_info[~DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "DF_info_lvl_0 = DF_info_lvl_0.drop_duplicates(subset = [\"Infected_ancestor\",\"index\",\"prophage_id\"] , keep = \"first\").reset_index(drop=True)\n",
    "\n",
    "# useful dictionary :\n",
    "KLtype_count = Counter(DF_info_lvl_0[\"KL_type_LCA\"])\n",
    "KLtype_pred = [kltype for kltype in KLtype_count if KLtype_count[kltype] >= 10]\n",
    "\n",
    "dico_prophage_kltype_associated = {}\n",
    "for negative_index,phage in tqdm(enumerate(DF_info_lvl_0[\"Phage\"].unique().tolist())) :\n",
    "    kltypes = set()\n",
    "    dpos = DF_info_lvl_0[DF_info_lvl_0[\"Phage\"] == phage][\"index\"]\n",
    "    for dpo in dpos : \n",
    "        tmp_kltypes = DF_info_lvl_0[DF_info_lvl_0[\"index\"] == dpo][\"KL_type_LCA\"].values\n",
    "        kltypes.update(tmp_kltypes)\n",
    "    dico_prophage_kltype_associated[phage] = kltypes\n",
    "\n",
    "depo_domains_seq = {index: domain_seq for index, domain_seq in zip(DF_info_lvl_0[\"index\"], DF_info_lvl_0['domain_seq'])}\n",
    "with open(f\"{path_work}/Dpo_domains.1710.multi.fasta\" , \"w\") as outfile : \n",
    "    for index,seq in depo_domains_seq.items() : \n",
    "        outfile.write(f\">{index}\\n{seq}\\n\")\n",
    "        \n",
    "# ******************************************************\n",
    "# CD hit step :\n",
    "\n",
    "path_multi_fasta = f\"{path_work}/Dpo_domains.1710.multi.fasta\"\n",
    "path_tmp_cdhit = f\"{path_work}/cdhit_clusters_1710\"\n",
    "\n",
    "def make_cdhit_cluster(threshold) :\n",
    "    cdhit_command = f\"cd-hit -i {path_multi_fasta} -o {path_tmp_cdhit}/{threshold}.out -c {threshold} -G 0 -aL 0.8\"\n",
    "    cdhit_process = subprocess.Popen(cdhit_command, shell =True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) \n",
    "    scan_out, scan_err = cdhit_process.communicate()\n",
    "    print(scan_out, scan_err)\n",
    "    \n",
    "\n",
    "def make_cluster_dico(cdhit_out) :\n",
    "    import json\n",
    "    dico_cluster = {}\n",
    "    threshold = cdhit_out.split(\"/\")[-1].split(\".out\")[0]\n",
    "    cluster_out = open(cdhit_out).read().split(\">Cluster\")\n",
    "    for index,cluster in enumerate(cluster_out[1:]) :\n",
    "        tmp_dpo = []\n",
    "        id_cluster = f\"Dpo_cdhit_{index}\"\n",
    "        for _,line in enumerate(cluster.split(\"\\n\")[1:-1]) :\n",
    "            dpo = line.split(\">\")[1].split(\".\")[0]\n",
    "            tmp_dpo.append(dpo)\n",
    "        dico_cluster[id_cluster] = tmp_dpo\n",
    "    with open(f\"{path_work}/dico_cluster.cdhit__{threshold}.json\", \"w\") as outfile:\n",
    "        json.dump(dico_cluster, outfile)\n",
    "    return dico_cluster , threshold\n",
    "\n",
    "def make_DF_binaries(df_info , dico_cluster, threshold) :\n",
    "    all_dpo_binaries = []\n",
    "    for phage in df_info.Phage.unique() :\n",
    "        dpo_binary = []\n",
    "        df_phage = df_info[df_info[\"Phage\"] == phage][\"index\"].values\n",
    "        for cluster,dpos in dico_cluster.items() :\n",
    "            shared_item = bool(set(dpos) & set(df_phage))\n",
    "            if shared_item == True :\n",
    "                dpo_binary.append(1)\n",
    "            else :\n",
    "                dpo_binary.append(0)\n",
    "        all_dpo_binaries.append(dpo_binary)\n",
    "    df_dpo_prophages = pd.DataFrame(all_dpo_binaries, index=df_info.Phage.unique(), columns=dico_cluster.keys())\n",
    "    df_dpo_prophages.to_csv(f\"{path_work}/DF_binaries_{threshold}.csv\", sep = \",\", index = True, header = True)\n",
    "    return df_dpo_prophages\n",
    "\n",
    "\n",
    "def make_DF_kltype(df_info, df ,KL_type , dico_cluster,ratio = 5, collapse = True) : \n",
    "    # positive data :\n",
    "    positive_phages = df_info[df_info[\"KL_type_LCA\"] == KL_type][\"Phage\"].unique()\n",
    "    df_positives = df[df.index.isin(positive_phages)]\n",
    "    df_positives = df_positives[~df_positives.index.duplicated(keep='first')]\n",
    "    binaries_pos = df_positives.values\n",
    "    labels_pos = [1] * len(binaries_pos)\n",
    "    phages_pos = df_positives.index\n",
    "    # negative data :\n",
    "    n_samples = len(phages_pos)\n",
    "    negative_phages = []\n",
    "    for negative_index,phage in enumerate(df_info[\"Phage\"].unique().tolist()) :\n",
    "        if KL_type not in dico_prophage_kltype_associated[phage] :\n",
    "            negative_phages.append(phage)\n",
    "    negative_phages_selected = random.sample(negative_phages, int(n_samples*ratio))\n",
    "    df_negatives = df[df.index.isin(negative_phages_selected)]\n",
    "    binaries_neg = df_negatives.values\n",
    "    labels_neg = [0] * len(binaries_neg)\n",
    "    all_binaries = np.concatenate((binaries_pos, binaries_neg)) \n",
    "    all_labels = labels_pos + labels_neg\n",
    "    all_indices = list(phages_pos) + list(negative_phages_selected)\n",
    "    df_kl = pd.DataFrame(all_binaries, index=all_indices, columns=dico_cluster.keys())\n",
    "    if collapse == True :\n",
    "        dpo_presence = [dpo for dpo in df_kl.columns if sum(df_kl[dpo]) >0]\n",
    "        df_kl = df_kl[dpo_presence]\n",
    "    return (df_kl , all_labels)\n",
    "\n",
    "\n",
    "def fit_rf_model_random_search(df_kl , all_labels,KL_type, threshold):\n",
    "    n_iter = 150\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_kl, all_labels, test_size=0.2, random_state=243)\n",
    "    param_grid = {\n",
    "        'bootstrap': Categorical([True, False]),\n",
    "        'max_depth': Integer(10, 100),\n",
    "        'max_features': Categorical(['auto', 'sqrt']),\n",
    "        'min_samples_leaf': Integer(1, 4),\n",
    "        'min_samples_split': Integer(2, 10),\n",
    "        'n_estimators': Integer(200, 800)\n",
    "    }\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    bayes_search = BayesSearchCV(rf, param_grid, n_iter=n_iter, cv=4, n_jobs=-1)\n",
    "    bayes_search.fit(X_train, y_train)\n",
    "    predictions = bayes_search.predict(X_test)\n",
    "    report = classification_report(y_test, predictions, output_dict=True)\n",
    "    mcc = matthews_corrcoef(y_test, predictions)\n",
    "    #joblib.dump(bayes_search.best_estimator_, f'{path_models}/{threshold}_RF_{KL_type}.joblib')\n",
    "    with open(f'{path_models}/{threshold}_RF_{KL_type}.pkl', 'wb') as model_file:\n",
    "        pickle.dump(bayes_search.best_estimator_, model_file)\n",
    "    \n",
    "    return mcc , report\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c164e22-0f0e-4c12-b0c7-0669b1c62415",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list(map(make_cdhit_cluster , [0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 0.975]))\n",
    "cdhit_files = [f\"{path_tmp_cdhit}/{file}\" for file in os.listdir(path_tmp_cdhit) if file[-3:]==\"out\"]\n",
    "        \n",
    "def make_prediction_file (path_file, path_binaries) :\n",
    "    dico_cluster , threshold = make_cluster_dico(f\"{path_file}\")\n",
    "    df_binaries = make_DF_binaries(DF_info_lvl_0 , dico_cluster, threshold)\n",
    "    for KL_type in KLtype_count :\n",
    "        with open(f\"{path_work}/RF_report.{threshold}.tsv\", \"a+\") as outfile :\n",
    "            if KLtype_count[KL_type] < 10 :\n",
    "                outfile.write(f\"{KL_type}\\t{KLtype_count[KL_type]}\\t***No sufficient Data\\n\")\n",
    "            else :\n",
    "                df_kl , all_labels = make_DF_kltype(DF_info_lvl_0 ,df_binaries, KL_type , dico_cluster)\n",
    "                mcc , report = fit_rf_model_random_search(df_kl , all_labels, KL_type,threshold)\n",
    "                outfile.write(f\"{KL_type}\\t{KLtype_count[KL_type]}\\t{mcc}\\t{report['1']['f1-score']}\\t{report['accuracy']}\\n\")\n",
    "                    \n",
    "if __name__ == '__main__':\n",
    "    with ThreadPool(10) as p:\n",
    "        p.map(make_prediction_file, cdhit_files)         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909b61b7-d1a4-41d9-8726-5ec5697c32e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babb62a2-19dd-408a-8eeb-f1a3e18a0df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the model from the file\n",
    "loaded_model = joblib.load('path_to_your_saved_model.joblib')\n",
    "\n",
    "# Now you can use `loaded_model` to make predictions\n",
    "predictions = loaded_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb01d052-f2d3-4165-bd4d-8676693b175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ****************************************************************************************\n",
    "#!/bin/bash\n",
    "#BATCH --job-name=RF_fitting__\n",
    "#SBATCH --qos=short\n",
    "#SBATCH --ntasks=1 \n",
    "#SBATCH --cpus-per-task=16 \n",
    "#SBATCH --mem=50gb \n",
    "#SBATCH --time=1-00:00:00 \n",
    "#SBATCH --output=RF_fitting__%j.log \n",
    "\n",
    "source /storage/apps/ANACONDA/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate python_311\n",
    "\n",
    "/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023/Seqbased_model/script_files/RF_fitting.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5932f1d-f5ec-431f-b2c2-618a59c5a185",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsync -avzhe ssh \\\n",
    "conchae@garnatxa.srv.cpd:/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023/Seqbased_model \\\n",
    "/media/concha-eloko/Linux/PPT_clean/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3a8249-049f-4319-baeb-2d9a7acc66d2",
   "metadata": {},
   "source": [
    "### Slight change in the code to reapeat only the 0.85 version (which is the best), but save into pickle bc joblib does not work well :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94686459-7b3f-4a54-b639-f0f9d84585cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction_fast (KL_type) :\n",
    "    dico_cluster = json.load(open(f\"{path_work}/dico_cluster.cdhit__0.85.json\"))\n",
    "    threshold = 0.85\n",
    "    df_binaries = pd.read_csv(f\"{path_work}/DF_binaries_0.85.csv\", sep = \",\", header = 0, index_col = 0)\n",
    "    with open(f\"{path_work}/RF_report.{threshold}.deep.no_collapse.pkl.tsv\", \"a+\") as outfile :\n",
    "        if KLtype_count[KL_type] < 10 :\n",
    "            outfile.write(f\"{KL_type}\\t{KLtype_count[KL_type]}\\t***No sufficient Data\\n\")\n",
    "        else :\n",
    "            df_kl , all_labels = make_DF_kltype(DF_info_lvl_0 ,df_binaries, KL_type , dico_cluster)\n",
    "            mcc , report = fit_rf_model_random_search(df_kl , all_labels, KL_type,threshold)\n",
    "            outfile.write(f\"{KL_type}\\t{KLtype_count[KL_type]}\\t{mcc}\\t{report['1']['f1-score']}\\t{report['accuracy']}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with ThreadPool(25) as p:\n",
    "        p.map(make_prediction_fast, KLtype_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1ccf0a-c26f-4f3c-86d7-cd7fc3fd6c3a",
   "metadata": {},
   "source": [
    "***\n",
    "# Debugg : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b61609b3-444c-47d0-989b-817bce001b22",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023/Seqbased_model/cdhit_clusters_1710/0.8.out.clstr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#with open(f\"{path_work}/dico_cluster.cdhit__{threshold}.json\", \"w\") as outfile:\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;66;03m#json.dump(dico_cluster, outfile)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dico_cluster , threshold\n\u001b[0;32m---> 20\u001b[0m dico_cluster , threshold \u001b[38;5;241m=\u001b[39m \u001b[43mmake_cluster_dico\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m, in \u001b[0;36mmake_cluster_dico\u001b[0;34m(cdhit_out)\u001b[0m\n\u001b[1;32m      6\u001b[0m threshold \u001b[38;5;241m=\u001b[39m cdhit_out\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.out\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      7\u001b[0m cluster_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcdhit_out\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.clstr\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m cluster_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcluster_file\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>Cluster\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index,cluster \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(cluster_out[\u001b[38;5;241m1\u001b[39m:]) :\n\u001b[1;32m     10\u001b[0m     tmp_dpo \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/media/concha-eloko/Linux/conda_envs/ML_work/lib/python3.11/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023/Seqbased_model/cdhit_clusters_1710/0.8.out.clstr'"
     ]
    }
   ],
   "source": [
    "file_path = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023/Seqbased_model/cdhit_clusters_1710/0.8.out\"\n",
    "\n",
    "def make_cluster_dico(cdhit_out) :\n",
    "    import json\n",
    "    dico_cluster = {}\n",
    "    threshold = cdhit_out.split(\"/\")[-1].split(\".out\")[0]\n",
    "    cluster_file = f\"{cdhit_out}.clstr\"\n",
    "    cluster_out = open(cluster_file).read().split(\">Cluster\")\n",
    "    for index,cluster in enumerate(cluster_out[1:]) :\n",
    "        tmp_dpo = []\n",
    "        id_cluster = f\"Dpo_cdhit_{index}\"\n",
    "        for _,line in enumerate(cluster.split(\"\\n\")[1:-1]) :\n",
    "            dpo = line.split(\">\")[1].split(\".\")[0]\n",
    "            tmp_dpo.append(dpo)\n",
    "        dico_cluster[id_cluster] = tmp_dpo\n",
    "    #with open(f\"{path_work}/dico_cluster.cdhit__{threshold}.json\", \"w\") as outfile:\n",
    "        #json.dump(dico_cluster, outfile)\n",
    "    return dico_cluster , threshold\n",
    "\n",
    "dico_cluster , threshold = make_cluster_dico(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c7716c-be21-43c4-92fc-4005d0a24781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_DF_binaries(df_info , dico_cluster, threshold) :\n",
    "    all_dpo_binaries = []\n",
    "    for phage in df_info.Phage.unique() :\n",
    "        dpo_binary = []\n",
    "        df_phage = df_info[df_info[\"Phage\"] == phage][\"index\"].values\n",
    "        for cluster,dpos in dico_cluster.items() :\n",
    "            shared_item = bool(set(dpos) & set(df_phage))\n",
    "            if shared_item == True :\n",
    "                dpo_binary.append(1)\n",
    "            else :\n",
    "                dpo_binary.append(0)\n",
    "        all_dpo_binaries.append(dpo_binary)\n",
    "    df_dpo_prophages = pd.DataFrame(all_dpo_binaries, index=df_info.Phage.unique(), columns=dico_cluster.keys())\n",
    "    #df_dpo_prophages.to_csv(f\"{path_work}/DF_binaries_{threshold}.csv\", sep = \",\", index = True, header = True)\n",
    "    return df_dpo_prophages\n",
    "\n",
    "df_binaries = make_DF_binaries(DF_info_lvl_0 , dico_cluster, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1cfa57-dead-4019-9e70-ab3983e7553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_DF_kltype(df_info, df ,KL_type , dico_cluster,ratio = 5, collapse = True) : \n",
    "    # positive data :\n",
    "    positive_phages = df_info[df_info[\"KL_type_LCA\"] == KL_type][\"Phage\"].unique()\n",
    "    df_positives = df[df.index.isin(positive_phages)]\n",
    "    binaries_pos = df_positives.values\n",
    "    labels_pos = [1] * len(binaries_pos)\n",
    "    phages_pos = df_positives.index\n",
    "    # negative data :\n",
    "    n_samples = len(phages_pos)\n",
    "    negative_phages = []\n",
    "    for negative_index,phage in enumerate(df_info[\"Phage\"].unique().tolist()) :\n",
    "        if KL_type not in dico_prophage_kltype_associated[phage] :\n",
    "            negative_phages.append(phage)\n",
    "    negative_phages_selected = random.sample(negative_phages, int(n_samples*ratio))\n",
    "    df_negatives = df[df.index.isin(negative_phages_selected)]\n",
    "    binaries_neg = df_negatives.values\n",
    "    labels_neg = [0] * len(binaries_neg)\n",
    "    all_binaries = np.concatenate((binaries_pos, binaries_neg)) \n",
    "    all_labels = labels_pos + labels_neg\n",
    "    all_indices = list(phages_pos) + list(negative_phages_selected)\n",
    "    df_kl = pd.DataFrame(all_binaries, index=all_indices, columns=dico_cluster.keys())\n",
    "    if collapse == True :\n",
    "        dpo_presence = [dpo for dpo in df_kl.columns if sum(df_kl[dpo]) >0]\n",
    "        df_kl = df_kl[dpo_presence]\n",
    "    return (df_kl , all_labels)\n",
    "\n",
    "df_kl , all_labels = make_DF_kltype(DF_info_lvl_0 ,df_binaries, \"KL64\" , dico_cluster)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_work",
   "language": "python",
   "name": "ml_work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
