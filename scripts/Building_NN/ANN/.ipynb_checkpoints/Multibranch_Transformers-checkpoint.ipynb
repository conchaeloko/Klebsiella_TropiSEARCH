{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder , label_binarize , OneHotEncoder\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, confusion_matrix\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Load the Dataframes :\n",
    "#path_work = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023\"\n",
    "path_work = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "\n",
    "    # Open the DF\n",
    "DF_info = pd.read_csv(f\"{path_work}/DF_Dpo.final.2705.tsv\", sep = \"\\t\" ,  header = 0 )\n",
    "    # Open the embeddings\n",
    "DF_embeddings = pd.read_csv(f\"{path_work}/Dpo.2705.embeddings.ultimate.csv\", sep = \",\", header= None )\n",
    "DF_embeddings.rename(columns={0: 'index'}, inplace=True)\n",
    "\n",
    "    # Filter the DF :\n",
    "DF_info_filtered = DF_info[~DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "DF_info_ToReLabel = DF_info[DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "all_data = pd.merge(DF_info_filtered , DF_embeddings , on = \"index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Preprocess the dataframe :\n",
    "\n",
    "    # Remove \"same\" Dpos\n",
    "all_data = all_data.drop_duplicates(subset = [\"Infected_ancestor\",\"index\"] , keep = \"first\").reset_index(drop=True)\n",
    "\n",
    "    # Create new categories for the punctual phages and ancestors :\n",
    "dico_infected_a = dict(Counter(all_data[\"Infected_ancestor\"]))\n",
    "dico_phage = dict(Counter(all_data[\"Phage\"]))\n",
    "\n",
    "all_data[\"labeled_ancestor\"] = all_data[\"Infected_ancestor\"].apply(lambda x : x if dico_infected_a[x] > 1 else \"punctual_ancestor\")\n",
    "all_data[\"labeled_phage\"] = all_data[\"Phage\"].apply(lambda x : x if dico_phage[x] > 1 else \"punctual_phage\")\n",
    "\n",
    "    # Label / OneHotEncode encode the categories and the labels :\n",
    "LE  = LabelEncoder()\n",
    "OHE = OneHotEncoder()\n",
    "\n",
    "all_data[\"KL_type_LCA\"] = LE.fit_transform(all_data[\"KL_type_LCA\"])\n",
    "labeled_ancestor_OHE = OHE.fit_transform(all_data[[\"labeled_ancestor\"]]).toarray()\n",
    "labeled_phage_OHE = OHE.fit_transform(all_data[[\"labeled_phage\"]]).toarray()\n",
    "\n",
    "df_labeled_ancestor_OHE = pd.DataFrame(labeled_ancestor_OHE, columns=[f\"OHE_ancestor_{i}\" for i in range(labeled_ancestor_OHE.shape[1])])\n",
    "df_labeled_phage_OHE = pd.DataFrame(labeled_phage_OHE, columns=[f\"OHE_phage_{i}\" for i in range(labeled_phage_OHE.shape[1])])\n",
    "\n",
    "final_all_data = pd.concat([all_data ,df_labeled_ancestor_OHE, df_labeled_phage_OHE], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2179, 1625)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labeled_ancestor_OHE[0]) , len(labeled_phage_OHE[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# The dataloader : \n",
    "\n",
    "class MultiDomainDataset(Dataset):\n",
    "    def __init__(self, final_all_data):\n",
    "        self.embeddings = torch.tensor(final_all_data[[int(i) for i in range(1, 1281)]].values, dtype=torch.float)\n",
    "        self.ancestor = torch.tensor(final_all_data[[f\"OHE_ancestor_{i}\" for i in range(labeled_ancestor_OHE.shape[1])]].values, dtype=torch.float)\n",
    "        self.prophage_instance = torch.tensor(final_all_data[[f\"OHE_phage_{i}\" for i in range(labeled_phage_OHE.shape[1])]].values, dtype=torch.float)\n",
    "        self.labels = torch.tensor(final_all_data[\"KL_type_LCA\"].values, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item_domain1 = self.embeddings[idx]\n",
    "        item_domain2 = self.ancestor[idx]\n",
    "        item_domain3 = self.prophage_instance[idx]\n",
    "        item_domain4 = self.labels[idx]\n",
    "        return item_domain1, item_domain2, item_domain3 , item_domain4\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_all_data, final_all_data[\"KL_type_LCA\"], test_size=0.25, random_state=42)\n",
    "\n",
    "train_singledata = MultiDomainDataset(X_train)\n",
    "test_singledata = MultiDomainDataset(X_test)\n",
    "\n",
    "train_loader = DataLoader(train_singledata, batch_size=12, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_singledata, batch_size=12, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 1280]) torch.Size([12, 2179]) torch.Size([12, 1625]) torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "for item_domain1, item_domain2, item_domain3, item_domain4 in train_loader:\n",
    "    print(item_domain1.shape, item_domain2.shape, item_domain3.shape, item_domain4.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0284,  0.0405, -0.0099,  ..., -0.0841,  0.0540,  0.1031]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor(69))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_singledata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# The model : \n",
    "\n",
    "class MultiBranchModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiBranchModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1280, hidden_size=512, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.lstm_branch = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512*2, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=256)\n",
    "        )\n",
    "        self.fnn_branch1 = nn.Sequential(\n",
    "            nn.Linear(2179, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fnn_branch2 = nn.Sequential(\n",
    "            nn.Linear(1625, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pre_classifier = nn.Sequential(\n",
    "            nn.Linear(256*3, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Linear(256, 127)  # as we concatenate the output from all three branches\n",
    "    def forward(self, embeddings, ancestor, prophage):\n",
    "        # unsqueeze embeddings to add sequence length dimension\n",
    "        embeddings = embeddings.unsqueeze(1)  # assuming embeddings have shape (batch_size, embedding_dim)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        lstm_out = lstm_out.squeeze(1)  # squeeze out the sequence length dimension\n",
    "        lstm_out = self.lstm_branch(lstm_out)  # Apply the remaining layers\n",
    "        fnn_out1 = self.fnn_branch1(ancestor)\n",
    "        fnn_out2 = self.fnn_branch2(prophage)\n",
    "        out = torch.cat((lstm_out, fnn_out1, fnn_out2), dim=1)\n",
    "        out = self.pre_classifier(out)\n",
    "        out = self.classifier(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Train / Eval : \n",
    "\n",
    "# Adding the weights : \n",
    "class_samples = np.bincount(all_data.KL_type_LCA)\n",
    "class_weights = 1. / torch.tensor(np.sqrt(class_samples), dtype=torch.float)\n",
    "# define loss function\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available\n",
    "\n",
    "model = MultiBranchModel()\n",
    "model = model.to(device)  # Move model to device\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Increase initial learning rate\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)  # Decrease learning rate every 30 epochs by 0.1\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_accuracy\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_accuracy\": [],\n",
    "    \"val_auprc\": {},  # Store AUPRC for each class\n",
    "    \"val_auroc\": {},  # Store AUROC for each class\n",
    "    \"confusion_matrix\": {},  # Store confusion matrix for each class\n",
    "}\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Switch model to training mode\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for i, (embeddings, ancestor, prophage, labels) in enumerate(train_loader):\n",
    "        embeddings = embeddings.to(device)\n",
    "        ancestor = ancestor.to(device)\n",
    "        prophage = prophage.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(embeddings, ancestor, prophage)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    epoch_loss /= len(train_loader)\n",
    "    history[\"train_loss\"].append(epoch_loss)\n",
    "    scheduler.step()\n",
    "\n",
    "    # Switch model to evaluation mode\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_predictions = []\n",
    "    val_true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for embeddings, ancestor, prophage, labels in test_loader:\n",
    "            embeddings = embeddings.to(device)\n",
    "            ancestor = ancestor.to(device)\n",
    "            prophage = prophage.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(embeddings, ancestor, prophage)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            val_predictions.append(outputs.cpu().numpy())\n",
    "            val_true_labels.append(labels.cpu().numpy())\n",
    "    val_loss /= len(val_loader)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    val_predictions = np.concatenate(val_predictions, axis=0)\n",
    "    val_true_labels = np.concatenate(val_true_labels, axis=0)\n",
    "    #auprc = average_precision_score(val_true_labels, val_predictions, average=\"macro\")  # Use macro-average to handle multiclass/multilabel\n",
    "    #history[\"val_auprc\"].append(auprc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Save model after training\n",
    "torch.save(model.state_dict(), f\"{path_work}/train_nn/MultiDomain.LSTM.model\")\n",
    "\n",
    "import json\n",
    "with open(f\"{path_work}/train_nn/MultiDomain.LSTM.model.out\" , \"w\") as outfile :\n",
    "    outfile.write(json.dumps(history))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "sklearn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
