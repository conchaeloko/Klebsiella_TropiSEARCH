{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder , label_binarize , OneHotEncoder\n",
    "from sklearn.metrics import matthews_corrcoef, precision_score, recall_score, accuracy_score\n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Load the Dataframes :\n",
    "#path_work = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023\"\n",
    "path_work = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "\n",
    "    # Open the DF\n",
    "DF_info = pd.read_csv(f\"{path_work}/DF_Dpo.final.2705.tsv\", sep = \"\\t\" ,  header = 0 )\n",
    "    # Open the embeddings\n",
    "DF_embeddings = pd.read_csv(f\"{path_work}/Dpo.2705.embeddings.ultimate.csv\", sep = \",\", header= None )\n",
    "DF_embeddings.rename(columns={0: 'index'}, inplace=True)\n",
    "\n",
    "    # Filter the DF :\n",
    "DF_info_filtered = DF_info[~DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "DF_info_ToReLabel = DF_info[DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "all_data = pd.merge(DF_info_filtered , DF_embeddings , on = \"index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Preprocess the dataframe :\n",
    "\n",
    "    # Remove \"same\" Dpos\n",
    "all_data = all_data.drop_duplicates(subset = [\"Infected_ancestor\",\"index\"] , keep = \"first\").reset_index(drop=True)\n",
    "\n",
    "    # Create new categories for the punctual phages and ancestors :\n",
    "dico_infected_a = dict(Counter(all_data[\"Infected_ancestor\"]))\n",
    "dico_phage = dict(Counter(all_data[\"Phage\"]))\n",
    "\n",
    "all_data[\"labeled_ancestor\"] = all_data[\"Infected_ancestor\"].apply(lambda x : x if dico_infected_a[x] > 1 else \"punctual_ancestor\")\n",
    "all_data[\"labeled_phage\"] = all_data[\"Phage\"].apply(lambda x : x if dico_phage[x] > 1 else \"punctual_phage\")\n",
    "\n",
    "    # Label / OneHotEncode encode the categories and the labels :\n",
    "LE  = LabelEncoder()\n",
    "OHE = OneHotEncoder()\n",
    "\n",
    "embeddings = all_data[[int(i) for i in range(1, 1281)]].values \n",
    "KL_type_OHE = OHE.fit_transform(all_data[[\"KL_type_LCA\"]]).toarray()\n",
    "labeled_ancestor_OHE = OHE.fit_transform(all_data[[\"labeled_ancestor\"]]).toarray()\n",
    "labeled_phage_OHE = OHE.fit_transform(all_data[[\"labeled_phage\"]]).toarray()\n",
    "\n",
    "positive_data = {\"embeddings\" : embeddings, \"KL_types\" : KL_type_OHE, \"ancestor\" : labeled_ancestor_OHE , \"phage\" : labeled_phage_OHE}\n",
    "positive_data[\"labels\"] = [1] * len(data[\"embeddings\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_negative_samples(data, neg_pos_ratio):\n",
    "    pos_samples = len(data['embeddings'])\n",
    "    neg_samples = int(pos_samples * neg_pos_ratio)\n",
    "\n",
    "    negative_data = {\"embeddings\": [], \"KL_types\": [], \"ancestor\": [], \"phage\": []}\n",
    "\n",
    "    for _ in range(neg_samples):\n",
    "        # Randomly select a pair from embeddings and phage\n",
    "        emb_phage_index = random.randint(0, pos_samples-1)\n",
    "        negative_data[\"embeddings\"].append(data[\"embeddings\"][emb_phage_index])\n",
    "        negative_data[\"phage\"].append(data[\"phage\"][emb_phage_index])\n",
    "\n",
    "        # Randomly select a pair from ancestor and KL_types ensuring the KL_type is different\n",
    "        while True:\n",
    "            anc_kl_index = random.randint(0, pos_samples-1)\n",
    "            if not np.all(data[\"KL_types\"][anc_kl_index] == data[\"KL_types\"][emb_phage_index]):\n",
    "                negative_data[\"ancestor\"].append(data[\"ancestor\"][anc_kl_index])\n",
    "                negative_data[\"KL_types\"].append(data[\"KL_types\"][anc_kl_index])\n",
    "                break\n",
    "\n",
    "    # Convert lists to numpy arrays for consistency\n",
    "    for key in negative_data.keys():\n",
    "        negative_data[key] = np.array(negative_data[key])\n",
    "\n",
    "    return negative_data\n",
    "\n",
    "negative_data = generate_negative_samples(data, 2)\n",
    "negative_data[\"labels\"] = [0] * len(negative_data[\"embeddings\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embeddings': array([[-0.01841583,  0.02238694,  0.00239867, ..., -0.07089869,\n",
       "          0.0160682 ,  0.06533931],\n",
       "        [ 0.03601578,  0.00593843, -0.0435346 , ..., -0.13660194,\n",
       "         -0.19137819,  0.13565759],\n",
       "        [ 0.02600367,  0.02437204, -0.00023745, ..., -0.02442309,\n",
       "         -0.02799782,  0.0280894 ],\n",
       "        ...,\n",
       "        [-0.01253937,  0.04425263, -0.02174671, ..., -0.07589217,\n",
       "         -0.08787988,  0.10967518],\n",
       "        [ 0.00080826,  0.01944812, -0.01075577, ..., -0.1129315 ,\n",
       "         -0.11068641,  0.1909886 ],\n",
       "        [ 0.02741208,  0.0601121 , -0.02414271, ..., -0.08106059,\n",
       "          0.060077  ,  0.07810764]]),\n",
       " 'KL_types': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'ancestor': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'phage': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.]]),\n",
       " 'labels': array([1, 1, 1, ..., 0, 0, 0])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {item : np.concatenate((positive_data[item], negative_data[item])) for item in positive_data}\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# The dataloader : \n",
    "class MultiDomainDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.embeddings = torch.tensor(data[\"embeddings\"], dtype=torch.float)\n",
    "        self.ancestor = torch.tensor(data[\"ancestor\"], dtype=torch.float)\n",
    "        self.phage = torch.tensor(data[\"phage\"], dtype=torch.float)\n",
    "        self.KLtypes = torch.tensor(data[\"KL_types\"], dtype=torch.float)\n",
    "        self.labels = torch.tensor(data[\"labels\"], dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item_domain1 = self.embeddings[idx]\n",
    "        item_domain2 = self.ancestor[idx]\n",
    "        item_domain3 = self.phage[idx]\n",
    "        item_domain4 = self.KLtypes[idx]\n",
    "        item_domain5 = self.labels[idx]\n",
    "        return item_domain1, item_domain2, item_domain3 , item_domain4, item_domain5\n",
    "\n",
    "X_train, X_tmp, X_test, X_val = {}, {}, {}\n",
    "for key in data.keys():\n",
    "    X_train[key], X_tmp[key], _, _ = train_test_split(data[key], data[\"labels\"], test_size=0.30, random_state=42)\n",
    "\n",
    "for key in X_train.keys():\n",
    "    X_val[key], X_test[key], _, _ = train_test_split(X_tmp[key], X_tmp[\"labels\"], test_size=0.25, random_state=42)\n",
    "\n",
    "train_singledata = MultiDomainDataset(X_train)\n",
    "test_singledata = MultiDomainDataset(X_test)\n",
    "val_singledata = MultiDomainDataset(X_val)\n",
    "\n",
    "train_loader = DataLoader(train_singledata, batch_size=12, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_singledata, batch_size=12, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_singledata, batch_size=12, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[\"KL_types\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# The model : \n",
    "class MultiBranchModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiBranchModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1280, hidden_size=512, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.lstm_branch = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512*2, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=256)\n",
    "        )\n",
    "        self.fnn_branch2 = nn.Sequential(\n",
    "            nn.Linear(2179, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fnn_branch3 = nn.Sequential(\n",
    "            nn.Linear(1625, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fnn_branch4 = nn.Sequential(\n",
    "            nn.Linear(127, 127),\n",
    "        )        \n",
    "        self.pre_classifier = nn.Sequential(\n",
    "            nn.Linear(256*3+127, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Linear(256, 2)  \n",
    "    def forward(self, embeddings, ancestor, phage, KLtypes):\n",
    "        # unsqueeze embeddings to add sequence length dimension\n",
    "        embeddings = embeddings.unsqueeze(1)  # assuming embeddings have shape (batch_size, embedding_dim)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        lstm_out = lstm_out.squeeze(1)  # squeeze out the sequence length dimension\n",
    "        lstm_out = self.lstm_branch(lstm_out)  # Apply the remaining layers\n",
    "        fnn_out2 = self.fnn_branch2(ancestor)\n",
    "        fnn_out3 = self.fnn_branch3(phage)\n",
    "        fnn_out4 = self.fnn_branch4(KLtypes)\n",
    "        out = torch.cat((lstm_out, fnn_out4,fnn_out2, fnn_out3), dim=1)\n",
    "        out = self.pre_classifier(out)\n",
    "        out = self.classifier(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Your Model\n",
    "model = MultiBranchModel().to(device)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss with sigmoid function applied\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(preds, labels):\n",
    "    preds = preds.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return mcc, precision, recall, accuracy\n",
    "\n",
    "# Training Loop\n",
    "n_epochs = 20\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for embeddings, ancestor, phage, KLtypes, labels in train_loader:\n",
    "        embeddings, ancestor, phage, KLtypes, labels = embeddings.to(device), ancestor.to(device), phage.to(device), KLtypes.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(embeddings, ancestor, phage, KLtypes)\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation Loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = []\n",
    "        truths = []\n",
    "        for embeddings, ancestor, phage, KLtypes, labels in val_loader:\n",
    "            embeddings, ancestor, phage, KLtypes, labels = embeddings.to(device), ancestor.to(device), phage.to(device), KLtypes.to(device), labels.to(device)\n",
    "            outputs = model(embeddings, ancestor, phage, KLtypes)\n",
    "            predicted = torch.sigmoid(outputs).data > 0.5\n",
    "            preds.extend(predicted.cpu().numpy())\n",
    "            truths.extend(labels.cpu().numpy())\n",
    "        mcc, precision, recall, accuracy = calculate_metrics(np.array(preds), np.array(truths))\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}')\n",
    "        print(f'Validation MCC: {mcc}, Precision: {precision}, Recall: {recall}, Accuracy: {accuracy}')\n",
    "\n",
    "# Testing Loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = []\n",
    "    truths = []\n",
    "    for embeddings, ancestor, phage, KLtypes, labels in test_loader:\n",
    "        embeddings, ancestor, phage, KLtypes, labels = embeddings.to(device), ancestor.to(device), phage.to(device), KLtypes.to(device), labels.to(device)\n",
    "        outputs = model(embeddings, ancestor, phage, KLtypes)\n",
    "        predicted = torch.sigmoid(outputs).data > 0.5\n",
    "        preds.extend(predicted.cpu().numpy())\n",
    "        truths.extend(labels.cpu().numpy())\n",
    "    mcc, precision, recall, accuracy = calculate_metrics(np.array(preds), np.array(truths))\n",
    "    print(f'Test MCC: {mcc}, Precision: {precision}, Recall: {recall}, Accuracy: {accuracy}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Save model after training\n",
    "torch.save(model.state_dict(), f\"{path_work}/train_nn/MultiDomain.LSTM.2207.model\")\n",
    "\n",
    "import json\n",
    "with open(f\"{path_work}/train_nn/MultiDomain.LSTM.model.out\" , \"w\") as outfile :\n",
    "    outfile.write(json.dumps(history))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#BATCH --job-name=MultibranchNN\n",
    "#SBATCH --qos=short\n",
    "#SBATCH --ntasks=1 \n",
    "#SBATCH --cpus-per-task=10 \n",
    "#SBATCH --mem=80gb \n",
    "#SBATCH --time=1-00:00:00 \n",
    "#SBATCH --output=MultibranchNN%j.log \n",
    "\n",
    "source /storage/apps/ANACONDA/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate torch_geometric\n",
    "\n",
    "python /home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023/train_nn/script_files/NN/multibranchmodel.LSTM.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geometric",
   "language": "python",
   "name": "torch_geometric"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
