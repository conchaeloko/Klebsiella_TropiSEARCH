{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal here is to correct the label before the finetuning of ESM2\n",
    "***\n",
    "## I. Generate the DF_info and the DF_embeddings \n",
    "## II. Tryout multiple algoritms : \n",
    "> NN : Graph Neural Networks  <br>\n",
    "***\n",
    "https://theaisummer.com/gnn-architectures/\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder , label_binarize , OneHotEncoder\n",
    "from sklearn.metrics import average_precision_score\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Open the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_work = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023\"\n",
    "path_work = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "\n",
    "# Open the DF\n",
    "DF_info = pd.read_csv(f\"{path_work}/DF_Dpo.final.2705.tsv\", sep = \"\\t\" ,  header = 0 )\n",
    "# Open the embeddings\n",
    "DF_embeddings = pd.read_csv(f\"{path_work}/Dpo.2705.embeddings.ultimate.csv\", sep = \",\", header= None )\n",
    "DF_embeddings.rename(columns={0: 'index'}, inplace=True)\n",
    "\n",
    "# Filter the DF :\n",
    "DF_info_filtered = DF_info[~DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "DF_info_ToReLabel = DF_info[DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "all_data = pd.merge(DF_info_filtered , DF_embeddings , on = \"index\")\n",
    "\n",
    "# Mind the over representation of outbreaks :\n",
    "all_data = all_data.drop_duplicates(subset = [\"Infected_ancestor\",\"index\",\"prophage_id\"] , keep = \"first\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1271</th>\n",
       "      <th>1272</th>\n",
       "      <th>1273</th>\n",
       "      <th>1274</th>\n",
       "      <th>1275</th>\n",
       "      <th>1276</th>\n",
       "      <th>1277</th>\n",
       "      <th>1278</th>\n",
       "      <th>1279</th>\n",
       "      <th>1280</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ppt__2930</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-0.017329</td>\n",
       "      <td>0.012884</td>\n",
       "      <td>0.037123</td>\n",
       "      <td>-0.123747</td>\n",
       "      <td>0.004186</td>\n",
       "      <td>-0.061367</td>\n",
       "      <td>-0.056718</td>\n",
       "      <td>-0.037215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098806</td>\n",
       "      <td>0.012989</td>\n",
       "      <td>-0.001155</td>\n",
       "      <td>0.139749</td>\n",
       "      <td>-0.030987</td>\n",
       "      <td>0.059306</td>\n",
       "      <td>0.107041</td>\n",
       "      <td>-0.041463</td>\n",
       "      <td>-0.085581</td>\n",
       "      <td>0.114973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ppt__3300</td>\n",
       "      <td>0.004044</td>\n",
       "      <td>0.040011</td>\n",
       "      <td>-0.001234</td>\n",
       "      <td>-0.095745</td>\n",
       "      <td>-0.058056</td>\n",
       "      <td>-0.002394</td>\n",
       "      <td>0.007648</td>\n",
       "      <td>-0.059740</td>\n",
       "      <td>0.060850</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020369</td>\n",
       "      <td>0.016287</td>\n",
       "      <td>0.062586</td>\n",
       "      <td>-0.024336</td>\n",
       "      <td>0.019276</td>\n",
       "      <td>0.069623</td>\n",
       "      <td>0.035261</td>\n",
       "      <td>-0.118962</td>\n",
       "      <td>0.035672</td>\n",
       "      <td>0.085582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ppt__1182</td>\n",
       "      <td>0.018767</td>\n",
       "      <td>0.068116</td>\n",
       "      <td>-0.009109</td>\n",
       "      <td>-0.012598</td>\n",
       "      <td>-0.107001</td>\n",
       "      <td>0.011569</td>\n",
       "      <td>-0.030943</td>\n",
       "      <td>-0.045359</td>\n",
       "      <td>0.048923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014524</td>\n",
       "      <td>-0.024645</td>\n",
       "      <td>0.071878</td>\n",
       "      <td>0.018206</td>\n",
       "      <td>0.042790</td>\n",
       "      <td>0.088410</td>\n",
       "      <td>0.031970</td>\n",
       "      <td>-0.124592</td>\n",
       "      <td>0.070040</td>\n",
       "      <td>0.065348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ppt__3540</td>\n",
       "      <td>-0.028261</td>\n",
       "      <td>-0.047253</td>\n",
       "      <td>-0.027340</td>\n",
       "      <td>-0.052824</td>\n",
       "      <td>-0.089644</td>\n",
       "      <td>-0.023079</td>\n",
       "      <td>0.094861</td>\n",
       "      <td>0.026104</td>\n",
       "      <td>0.024001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051728</td>\n",
       "      <td>0.005634</td>\n",
       "      <td>-0.077874</td>\n",
       "      <td>0.030336</td>\n",
       "      <td>-0.037648</td>\n",
       "      <td>0.050625</td>\n",
       "      <td>0.046142</td>\n",
       "      <td>-0.158841</td>\n",
       "      <td>-0.007670</td>\n",
       "      <td>0.034556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ppt__942</td>\n",
       "      <td>0.014863</td>\n",
       "      <td>0.028030</td>\n",
       "      <td>0.014927</td>\n",
       "      <td>-0.025997</td>\n",
       "      <td>-0.096138</td>\n",
       "      <td>0.016290</td>\n",
       "      <td>0.015008</td>\n",
       "      <td>-0.066254</td>\n",
       "      <td>0.077959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008521</td>\n",
       "      <td>-0.019820</td>\n",
       "      <td>0.123201</td>\n",
       "      <td>-0.040306</td>\n",
       "      <td>0.030893</td>\n",
       "      <td>0.051362</td>\n",
       "      <td>0.047316</td>\n",
       "      <td>-0.102698</td>\n",
       "      <td>0.044830</td>\n",
       "      <td>0.084530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3603</th>\n",
       "      <td>anubis__304</td>\n",
       "      <td>0.006264</td>\n",
       "      <td>0.006471</td>\n",
       "      <td>-0.031665</td>\n",
       "      <td>0.078502</td>\n",
       "      <td>-0.131247</td>\n",
       "      <td>0.077167</td>\n",
       "      <td>0.043005</td>\n",
       "      <td>-0.183636</td>\n",
       "      <td>-0.022181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044299</td>\n",
       "      <td>-0.061847</td>\n",
       "      <td>0.017696</td>\n",
       "      <td>0.054798</td>\n",
       "      <td>-0.035830</td>\n",
       "      <td>-0.030202</td>\n",
       "      <td>0.039051</td>\n",
       "      <td>-0.127020</td>\n",
       "      <td>-0.113630</td>\n",
       "      <td>0.211258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3604</th>\n",
       "      <td>anubis__1273</td>\n",
       "      <td>-0.019114</td>\n",
       "      <td>0.063302</td>\n",
       "      <td>0.006635</td>\n",
       "      <td>-0.060343</td>\n",
       "      <td>-0.034054</td>\n",
       "      <td>-0.003895</td>\n",
       "      <td>0.033920</td>\n",
       "      <td>-0.080352</td>\n",
       "      <td>0.073579</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004504</td>\n",
       "      <td>-0.007906</td>\n",
       "      <td>0.075141</td>\n",
       "      <td>-0.052423</td>\n",
       "      <td>0.027127</td>\n",
       "      <td>0.073984</td>\n",
       "      <td>0.030664</td>\n",
       "      <td>-0.096409</td>\n",
       "      <td>0.011906</td>\n",
       "      <td>0.124885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3605</th>\n",
       "      <td>anubis__1311</td>\n",
       "      <td>0.051261</td>\n",
       "      <td>0.067942</td>\n",
       "      <td>0.005061</td>\n",
       "      <td>-0.019131</td>\n",
       "      <td>-0.060296</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.037515</td>\n",
       "      <td>-0.033887</td>\n",
       "      <td>0.091774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044678</td>\n",
       "      <td>0.052609</td>\n",
       "      <td>0.112994</td>\n",
       "      <td>-0.000592</td>\n",
       "      <td>0.027122</td>\n",
       "      <td>0.086020</td>\n",
       "      <td>0.013660</td>\n",
       "      <td>-0.055491</td>\n",
       "      <td>0.021665</td>\n",
       "      <td>0.049301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3606</th>\n",
       "      <td>anubis__1525</td>\n",
       "      <td>-0.010655</td>\n",
       "      <td>0.083864</td>\n",
       "      <td>0.009084</td>\n",
       "      <td>-0.042220</td>\n",
       "      <td>-0.066479</td>\n",
       "      <td>0.008724</td>\n",
       "      <td>0.010109</td>\n",
       "      <td>-0.078033</td>\n",
       "      <td>0.065285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020752</td>\n",
       "      <td>0.024543</td>\n",
       "      <td>0.071302</td>\n",
       "      <td>0.035980</td>\n",
       "      <td>0.012171</td>\n",
       "      <td>0.054399</td>\n",
       "      <td>0.032167</td>\n",
       "      <td>-0.151018</td>\n",
       "      <td>0.042541</td>\n",
       "      <td>0.035221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3607</th>\n",
       "      <td>anubis__794</td>\n",
       "      <td>0.003410</td>\n",
       "      <td>0.035987</td>\n",
       "      <td>-0.006781</td>\n",
       "      <td>-0.006114</td>\n",
       "      <td>-0.096747</td>\n",
       "      <td>0.055312</td>\n",
       "      <td>0.020362</td>\n",
       "      <td>-0.106669</td>\n",
       "      <td>0.007063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040264</td>\n",
       "      <td>-0.011636</td>\n",
       "      <td>0.056235</td>\n",
       "      <td>0.041319</td>\n",
       "      <td>-0.008228</td>\n",
       "      <td>0.036652</td>\n",
       "      <td>0.048385</td>\n",
       "      <td>-0.104287</td>\n",
       "      <td>0.016014</td>\n",
       "      <td>0.068212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3608 rows × 1281 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             index         1         2         3         4         5  \\\n",
       "0        ppt__2930 -0.000061 -0.017329  0.012884  0.037123 -0.123747   \n",
       "1        ppt__3300  0.004044  0.040011 -0.001234 -0.095745 -0.058056   \n",
       "2        ppt__1182  0.018767  0.068116 -0.009109 -0.012598 -0.107001   \n",
       "3        ppt__3540 -0.028261 -0.047253 -0.027340 -0.052824 -0.089644   \n",
       "4         ppt__942  0.014863  0.028030  0.014927 -0.025997 -0.096138   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "3603   anubis__304  0.006264  0.006471 -0.031665  0.078502 -0.131247   \n",
       "3604  anubis__1273 -0.019114  0.063302  0.006635 -0.060343 -0.034054   \n",
       "3605  anubis__1311  0.051261  0.067942  0.005061 -0.019131 -0.060296   \n",
       "3606  anubis__1525 -0.010655  0.083864  0.009084 -0.042220 -0.066479   \n",
       "3607   anubis__794  0.003410  0.035987 -0.006781 -0.006114 -0.096747   \n",
       "\n",
       "             6         7         8         9  ...      1271      1272  \\\n",
       "0     0.004186 -0.061367 -0.056718 -0.037215  ...  0.098806  0.012989   \n",
       "1    -0.002394  0.007648 -0.059740  0.060850  ... -0.020369  0.016287   \n",
       "2     0.011569 -0.030943 -0.045359  0.048923  ...  0.014524 -0.024645   \n",
       "3    -0.023079  0.094861  0.026104  0.024001  ...  0.051728  0.005634   \n",
       "4     0.016290  0.015008 -0.066254  0.077959  ...  0.008521 -0.019820   \n",
       "...        ...       ...       ...       ...  ...       ...       ...   \n",
       "3603  0.077167  0.043005 -0.183636 -0.022181  ...  0.044299 -0.061847   \n",
       "3604 -0.003895  0.033920 -0.080352  0.073579  ... -0.004504 -0.007906   \n",
       "3605  0.000984  0.037515 -0.033887  0.091774  ...  0.044678  0.052609   \n",
       "3606  0.008724  0.010109 -0.078033  0.065285  ...  0.020752  0.024543   \n",
       "3607  0.055312  0.020362 -0.106669  0.007063  ...  0.040264 -0.011636   \n",
       "\n",
       "          1273      1274      1275      1276      1277      1278      1279  \\\n",
       "0    -0.001155  0.139749 -0.030987  0.059306  0.107041 -0.041463 -0.085581   \n",
       "1     0.062586 -0.024336  0.019276  0.069623  0.035261 -0.118962  0.035672   \n",
       "2     0.071878  0.018206  0.042790  0.088410  0.031970 -0.124592  0.070040   \n",
       "3    -0.077874  0.030336 -0.037648  0.050625  0.046142 -0.158841 -0.007670   \n",
       "4     0.123201 -0.040306  0.030893  0.051362  0.047316 -0.102698  0.044830   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3603  0.017696  0.054798 -0.035830 -0.030202  0.039051 -0.127020 -0.113630   \n",
       "3604  0.075141 -0.052423  0.027127  0.073984  0.030664 -0.096409  0.011906   \n",
       "3605  0.112994 -0.000592  0.027122  0.086020  0.013660 -0.055491  0.021665   \n",
       "3606  0.071302  0.035980  0.012171  0.054399  0.032167 -0.151018  0.042541   \n",
       "3607  0.056235  0.041319 -0.008228  0.036652  0.048385 -0.104287  0.016014   \n",
       "\n",
       "          1280  \n",
       "0     0.114973  \n",
       "1     0.085582  \n",
       "2     0.065348  \n",
       "3     0.034556  \n",
       "4     0.084530  \n",
       "...        ...  \n",
       "3603  0.211258  \n",
       "3604  0.124885  \n",
       "3605  0.049301  \n",
       "3606  0.035221  \n",
       "3607  0.068212  \n",
       "\n",
       "[3608 rows x 1281 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1280"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DF_embeddings[DF_embeddings[\"index\"] == \"ppt__942\"].values[0][1:1281])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Build the Graph Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Indexation process (shall add the N phages to predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexation = all_data[\"Infected_ancestor\"].unique().tolist() + all_data[\"Phage\"].unique().tolist() + all_data[\"index\"].unique().tolist() + [f\"Dpo_to_predict_{n}\" for n in DF_info[\"index\"].unique().tolist()]\n",
    "\n",
    "dico_ID = {item:index for index, item in enumerate(indexation)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Make edge file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = []\n",
    "\n",
    "# Node A (bacteria) - Node B1 (prophage) :\n",
    "for _, row in all_data.iterrows() :\n",
    "    edge_index.append([dico_ID[row[\"Infected_ancestor\"]], dico_ID[row[\"Phage\"]]])\n",
    "    edge_index.append([dico_ID[row[\"Infected_ancestor\"]], dico_ID[row[\"Infected_ancestor\"]]])\n",
    "    \n",
    "# Node B1 - Node B2 (depolymerase) :\n",
    "for phage in all_data.Phage.unique() :\n",
    "    all_data_phage = all_data[all_data[\"Phage\"] == phage]\n",
    "    for _, row in all_data_phage.iterrows() :\n",
    "        edge_index.append([dico_ID[row[\"Phage\"]], dico_ID[row[\"index\"]]])\n",
    "\n",
    "# Transform into tensor : \n",
    "edge_index_tensor = torch.tensor(edge_index , dtype=torch.long)\n",
    "\n",
    "# Write file : \n",
    "numpy_array = edge_index_tensor.numpy()\n",
    "df = pd.DataFrame(numpy_array)\n",
    "df.to_csv(f\"{path_work}/edge_index.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Make the node feature file : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'KL1': 0,\n",
       " 'KL10': 1,\n",
       " 'KL101': 2,\n",
       " 'KL102': 3,\n",
       " 'KL103': 4,\n",
       " 'KL104': 5,\n",
       " 'KL105': 6,\n",
       " 'KL106': 7,\n",
       " 'KL107': 8,\n",
       " 'KL108': 9,\n",
       " 'KL109': 10,\n",
       " 'KL11': 11,\n",
       " 'KL110': 12,\n",
       " 'KL111': 13,\n",
       " 'KL112': 14,\n",
       " 'KL113': 15,\n",
       " 'KL114': 16,\n",
       " 'KL115': 17,\n",
       " 'KL116': 18,\n",
       " 'KL117': 19,\n",
       " 'KL118': 20,\n",
       " 'KL119': 21,\n",
       " 'KL12': 22,\n",
       " 'KL121': 23,\n",
       " 'KL122': 24,\n",
       " 'KL123': 25,\n",
       " 'KL124': 26,\n",
       " 'KL125': 27,\n",
       " 'KL126': 28,\n",
       " 'KL127': 29,\n",
       " 'KL128': 30,\n",
       " 'KL13': 31,\n",
       " 'KL130': 32,\n",
       " 'KL131': 33,\n",
       " 'KL132': 34,\n",
       " 'KL134': 35,\n",
       " 'KL136': 36,\n",
       " 'KL137': 37,\n",
       " 'KL139': 38,\n",
       " 'KL14': 39,\n",
       " 'KL140': 40,\n",
       " 'KL141': 41,\n",
       " 'KL142': 42,\n",
       " 'KL143': 43,\n",
       " 'KL144': 44,\n",
       " 'KL145': 45,\n",
       " 'KL146': 46,\n",
       " 'KL147': 47,\n",
       " 'KL148': 48,\n",
       " 'KL149': 49,\n",
       " 'KL15': 50,\n",
       " 'KL150': 51,\n",
       " 'KL151': 52,\n",
       " 'KL152': 53,\n",
       " 'KL153': 54,\n",
       " 'KL154': 55,\n",
       " 'KL155': 56,\n",
       " 'KL157': 57,\n",
       " 'KL158': 58,\n",
       " 'KL159': 59,\n",
       " 'KL16': 60,\n",
       " 'KL162': 61,\n",
       " 'KL163': 62,\n",
       " 'KL164': 63,\n",
       " 'KL166': 64,\n",
       " 'KL169': 65,\n",
       " 'KL17': 66,\n",
       " 'KL170': 67,\n",
       " 'KL18': 68,\n",
       " 'KL19': 69,\n",
       " 'KL2': 70,\n",
       " 'KL20': 71,\n",
       " 'KL21': 72,\n",
       " 'KL22': 73,\n",
       " 'KL23': 74,\n",
       " 'KL24': 75,\n",
       " 'KL25': 76,\n",
       " 'KL26': 77,\n",
       " 'KL27': 78,\n",
       " 'KL28': 79,\n",
       " 'KL29': 80,\n",
       " 'KL3': 81,\n",
       " 'KL30': 82,\n",
       " 'KL31': 83,\n",
       " 'KL33': 84,\n",
       " 'KL34': 85,\n",
       " 'KL35': 86,\n",
       " 'KL36': 87,\n",
       " 'KL37': 88,\n",
       " 'KL38': 89,\n",
       " 'KL39': 90,\n",
       " 'KL4': 91,\n",
       " 'KL40': 92,\n",
       " 'KL41': 93,\n",
       " 'KL42': 94,\n",
       " 'KL43': 95,\n",
       " 'KL45': 96,\n",
       " 'KL46': 97,\n",
       " 'KL47': 98,\n",
       " 'KL48': 99,\n",
       " 'KL49': 100,\n",
       " 'KL5': 101,\n",
       " 'KL51': 102,\n",
       " 'KL52': 103,\n",
       " 'KL53': 104,\n",
       " 'KL54': 105,\n",
       " 'KL55': 106,\n",
       " 'KL56': 107,\n",
       " 'KL57': 108,\n",
       " 'KL58': 109,\n",
       " 'KL59': 110,\n",
       " 'KL6': 111,\n",
       " 'KL60': 112,\n",
       " 'KL61': 113,\n",
       " 'KL62': 114,\n",
       " 'KL63': 115,\n",
       " 'KL64': 116,\n",
       " 'KL66': 117,\n",
       " 'KL67': 118,\n",
       " 'KL7': 119,\n",
       " 'KL70': 120,\n",
       " 'KL71': 121,\n",
       " 'KL74': 122,\n",
       " 'KL8': 123,\n",
       " 'KL81': 124,\n",
       " 'KL82': 125,\n",
       " 'KL9': 126}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LE  = LabelEncoder()\n",
    "di = LE.fit_transform(all_data[\"KL_type_LCA\"])\n",
    "label_mapping = dict(zip(LE.classes_, LE.transform(LE.classes_)))\n",
    "label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26766it [02:01, 219.57it/s]\n"
     ]
    }
   ],
   "source": [
    "node_feature = []\n",
    "\n",
    "for index, item in tqdm(enumerate(indexation)) :\n",
    "    features = [index]\n",
    "    if item in all_data[\"Infected_ancestor\"].unique() : \n",
    "        KL_type = all_data[all_data[\"Infected_ancestor\"] == item][\"KL_type_LCA\"].values[0]\n",
    "        features = features + [label_mapping[KL_type]] + [-1]*1280\n",
    "    elif item in all_data[\"Phage\"].unique() : \n",
    "        features = features + [-1]*1281\n",
    "    elif item in all_data[\"index\"].unique() : \n",
    "        features = features + [-1] + DF_embeddings[DF_embeddings[\"index\"] == item].values[0][1:1281].tolist()\n",
    "    elif item in [f\"Dpo_to_predict_{n}\" for n in DF_info[\"index\"].unique().tolist()] : \n",
    "        features = features + [-1]*1281\n",
    "    node_feature.append(features)\n",
    "    \n",
    "# Transform into tensor : \n",
    "node_feature_tensor = torch.tensor(node_feature , dtype=torch.float)\n",
    "\n",
    "# Write file : \n",
    "numpy_array = node_feature_tensor.numpy()\n",
    "df = pd.DataFrame(numpy_array)\n",
    "df.to_csv(f\"{path_work}/node_features.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Make the Y file : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_file = [1] * len(edge_index)\n",
    "\n",
    "# Transform into tensor : \n",
    "y_tensor = torch.tensor(y_file , dtype=torch.float)\n",
    "\n",
    "# Write file : \n",
    "numpy_array = y_tensor.numpy()\n",
    "df = pd.DataFrame(numpy_array)\n",
    "df.to_csv(f\"{path_work}/y_file.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Create the Data instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[26766, 1282], edge_index=[2, 19354], y=[19354])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "edge_index = edge_index_tensor\n",
    "\n",
    "x = node_feature_tensor\n",
    "y = y_tensor\n",
    "\n",
    "# create Data instance\n",
    "data = Data(x=x, edge_index=edge_index.t().contiguous(), y=y)\n",
    "\n",
    "# print out the data instance\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Dataset, Data\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(CustomDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['ancestors.csv', 'prophages.csv', 'edges.csv']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        # This method is where the raw data files would be downloaded to.\n",
    "        # If you already have the files, you don't need to implement this.\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # This method reads the raw files and saves them into a Data object.\n",
    "\n",
    "        # Read the data files\n",
    "        ancestors = pd.read_csv(self.raw_paths[0])\n",
    "        prophages = pd.read_csv(self.raw_paths[1])\n",
    "        edges = pd.read_csv(self.raw_paths[2])\n",
    "\n",
    "        # Prepare OneHotEncoder for ancestor features\n",
    "        OHE = OneHotEncoder(sparse=False)\n",
    "        OHE.fit(ancestors[['KL_type']].values)\n",
    "\n",
    "        # Convert the ancestor and prophage features to a PyG x feature matrix\n",
    "        ancestor_features = OHE.transform(ancestors[['KL_type']].values)\n",
    "        prophage_features = np.array(prophages[[str(i) for i in range(1, 1281)]].values.tolist())\n",
    "        \n",
    "        # Combine the two sets of features\n",
    "        x = torch.tensor(np.concatenate([ancestor_features, prophage_features], axis=0), dtype=torch.float)\n",
    "\n",
    "        # Convert the edge list to a PyG edge index\n",
    "        edge_index = torch.tensor(edges.values, dtype=torch.long).t().contiguous()\n",
    "\n",
    "        # In this case, there are no edge labels, so we'll just use a placeholder\n",
    "        y = torch.tensor([0]*edges.shape[0], dtype=torch.long)\n",
    "\n",
    "        # Save the data\n",
    "        data = Data(x=x, edge_index=edge_index, y=y)\n",
    "        torch.save(data, self.processed_paths[0])\n",
    "\n",
    "    def len(self):\n",
    "        return 1\n",
    "\n",
    "    def get(self, idx):\n",
    "        # This method loads and returns a Data object.\n",
    "        data = torch.load(self.processed_paths[0])\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# let's assume you have 4 nodes: 1 bacterial strain (A), 1 prophage (B1), and 2 depolymerases (B2)\n",
    "# and they are assigned indices as follows: bacterial strain: 0, prophage: 1, depolymerases: 2, 3\n",
    "\n",
    "# edge_index tensor\n",
    "edge_index = torch.tensor([[0, 1],  # edge from bacterial strain to prophage\n",
    "                           [1, 2],  # edge from prophage to first depolymerase\n",
    "                           [1, 3]],  # edge from prophage to second depolymerase\n",
    "                          dtype=torch.long)\n",
    "\n",
    "# node feature tensor\n",
    "x = torch.tensor([[1.0],  # feature of bacterial strain (e.g., KL type)\n",
    "                  [0.0],  # feature of prophage (no specific feature)\n",
    "                  [-1.0],  # feature of first depolymerase (embedding representation)\n",
    "                  [-1.0]],  # feature of second depolymerase (embedding representation)\n",
    "                 dtype=torch.float)\n",
    "\n",
    "# target variable tensor\n",
    "y = torch.tensor([1,  # connection between bacterial strain and prophage\n",
    "                  0,  # no specific target variable for prophage\n",
    "                  0, 0],  # no specific target variable for depolymerases\n",
    "                 dtype=torch.float)\n",
    "\n",
    "# create Data instance\n",
    "data = Data(x=x, edge_index=edge_index.t().contiguous(), y=y)\n",
    "\n",
    "# print out the data instance\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# edge_index tensor\n",
    "edge_index = torch.tensor([\n",
    "    [0, 2],  # edge from 1st bacterial strain to 1st prophage\n",
    "    [0, 3],  # edge from 1st bacterial strain to 2nd prophage\n",
    "    [1, 3],  # edge from 2nd bacterial strain to 2nd prophage\n",
    "    [1, 4],  # edge from 2nd bacterial strain to 3rd prophage\n",
    "    [2, 5], [2, 6],  # edges from 1st prophage to 1st and 2nd depolymerases\n",
    "    [3, 7], [3, 8], [3, 9],  # edges from 2nd prophage to 3rd, 4th, 5th depolymerases\n",
    "    [4, 10], [4, 11], [4, 12], [4, 13], [4, 14],  # edges from 3rd prophage to 6th-10th depolymerases\n",
    "], dtype=torch.long)\n",
    "\n",
    "# node feature tensor\n",
    "# pad with zeros so that all nodes have the same number of features\n",
    "x = torch.zeros(15, 1280)  # 15 nodes, maximum F is 1280\n",
    "x[0, 0] = 1  # 1st bacterial strain with KL type 1\n",
    "x[1, 0] = 2  # 2nd bacterial strain with KL type 2\n",
    "x[5:15] = torch.randn(10, 1280)  # depolymerases with random embeddings\n",
    "\n",
    "# target variable tensor\n",
    "# here, we assume a binary link prediction task: does a link exist between a prophage and a bacterial strain?\n",
    "# we need to structure this according to our specific needs and model\n",
    "# for this example, let's assume we have a binary label for each prophage-bacterial strain pair\n",
    "y = torch.tensor([1,  # link exists between 1st bacterial strain and 1st prophage\n",
    "                  1,  # link exists between 1st bacterial strain and 2nd prophage\n",
    "                  0,  # no link between 1st bacterial strain and 3rd prophage\n",
    "                  1,  # link exists between 2nd bacterial strain and 2nd prophage\n",
    "                  0,  # no link between 2nd bacterial strain and 1st prophage\n",
    "                  1],  # link exists between 2nd bacterial strain and 3rd prophage\n",
    "                 dtype=torch.float)\n",
    "\n",
    "# create Data instance\n",
    "data = Data(x=x, edge_index=edge_index.t().contiguous(), y=y)\n",
    "\n",
    "# print out the data instance\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 128)\n",
    "        self.conv2 = GCNConv(128, num_classes)\n",
    "        self.classifier = Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        # Here, we assume that we're only interested in the output of the B1 nodes\n",
    "        # For this, we'll gather their outputs using their indices in `data`\n",
    "        x_b1 = x[data.b1_indices]\n",
    "        out = self.classifier(x_b1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A1: Inductive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "# Define your model and optimizer\n",
    "model = GCN(num_node_features=1280, num_classes=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Create DataLoader instances for your training and test datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):  # 100 epochs\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = torch.nn.functional.cross_entropy(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "model.eval()\n",
    "for data in test_loader:\n",
    "    with torch.no_grad():\n",
    "        predictions = model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2: Transductive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(num_node_features=1280, num_classes=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(100):  # 100 epochs\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = torch.nn.functional.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(data)\n",
    "unlabeled_predictions = predictions[data.test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ok I do not think your answer makes sense to me for the following reasons : \n",
    "It seems to me that you forgot that the goal is to predict the link between the nodes A (bacteria to which are assigned KL_types) and the nodes B1 (prophages, with no features). So that renders your node feature file incorrect (unless I am wrong).\n",
    "Secondly, I thought the y file had only \"1\" and \"0\" with n rows equal to the number of edges, and \"0\" not a label and \"1\" a label. Can you confirm/ correct the y file ? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It is clearer. However I still have questions : \n",
    "1. If I am trying to predict the link between the node B1 and a node A that I ignore, what index for the node A should I put ?\n",
    "2. So if I understand correctly, as I only know the edges that exist, all the values in the y file are going to be \"1\" ?\n",
    "That leads to another question : how should I encode the fact that the links that I want to predict are between the nodes A and B1 ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's dive into that a little more as it does not make complete sense to me. Each node A is assigned a KL_type. There are 127 KL_types in total. If I want the score, not only between a node B1 and a given node A but a node B1 with a KL_type, can I get the mean score of all the nodes for a given KL_type ? Or is there a better way to go about it ?\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "sklearn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
