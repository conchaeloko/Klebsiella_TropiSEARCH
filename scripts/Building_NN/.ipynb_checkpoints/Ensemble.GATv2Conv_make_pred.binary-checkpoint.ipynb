{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27546928-2429-4e11-a984-fdfb6a143bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsync -avzhe ssh \\\n",
    "conchae@garnatxa.srv.cpd:/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023/ensemble_1908 \\\n",
    "/media/concha-eloko/Linux/PPT_clean/ficheros_28032023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c3f7386-2112-49cc-99fd-bf8be42dfade",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData, DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import to_hetero , HeteroConv , GATv2Conv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder , label_binarize , OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score , matthews_corrcoef\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import random\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import logging\n",
    "from multiprocessing.pool import ThreadPool\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# *****************************************************************************\n",
    "# Load the Dataframes :\n",
    "path_work = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "path_ensemble = f\"{path_work}/ficheros_28032023/ensemble_1908\"\n",
    "#graph_data = torch.load(f'{path_work}/graph_file.2607.OHE.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd62344e-ca02-4a9f-a878-168489975a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "#logging.basicConfig(filename = f\"{path_work}/train_nn/GATv2Conv.1608.log\",format='%(asctime)s | %(levelname)s: %(message)s', level=logging.NOTSET, filemode='w')\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, edge_type , conv, hidden_channels, heads, dropout): \n",
    "        super().__init__()\n",
    "        self.conv = conv((-1,-1), hidden_channels, add_self_loops = False, heads = heads, dropout = dropout, shared_weights = True)\n",
    "        self.hetero_conv = HeteroConv({edge_type: self.conv})\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x = self.hetero_conv(x_dict, edge_index_dict)\n",
    "        return x\n",
    "\n",
    "# Classifier, Binary :\n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, heads):\n",
    "        super().__init__()\n",
    "        self.lin1 = torch.nn.Linear(heads*hidden_channels + 127, 512)\n",
    "        self.lin2 = torch.nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x_dict_A , x_dict_B1, graph_data):\n",
    "        edge_type = (\"B1\", \"infects\", \"A\")\n",
    "        edge_feat_A = x_dict_A[\"A\"][graph_data[edge_type].edge_label_index[1]]\n",
    "        edge_feat_B1 = x_dict_B1[\"B1\"][graph_data[edge_type].edge_label_index[0]]\n",
    "        features_phage = torch.cat((edge_feat_A ,edge_feat_B1), dim=-1)\n",
    "        x = self.lin1(features_phage).relu()\n",
    "        x = self.lin2(x)\n",
    "        return x.view(-1)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, conv, hidden_channels, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.single_layer_model = GNN((\"B2\", \"expressed\", \"B1\") ,conv, hidden_channels,heads,dropout)\n",
    "        self.EdgeDecoder = EdgeDecoder(hidden_channels,heads)\n",
    "\n",
    "    def forward(self, graph_data):\n",
    "        b1_nodes = self.single_layer_model(graph_data.x_dict , graph_data.edge_index_dict)\n",
    "        a_nodes =  graph_data.x_dict\n",
    "        out = self.EdgeDecoder(a_nodes ,b1_nodes , graph_data)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aeaa715-72cd-4f0a-b720-85e59400293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Relevant functions :\n",
    "@torch.no_grad()\n",
    "def make_predictions(model, data):\n",
    "    model.eval() \n",
    "    output = model(data)\n",
    "    probabilities = torch.sigmoid(output)  # Convert output to probabilities\n",
    "    predictions = probabilities.round()  # Convert probabilities to class labels\n",
    "    return predictions, probabilities\n",
    "\n",
    "# models object : \n",
    "Dpo_classifier_models = {}\n",
    "hidden_channels = 1000\n",
    "conv = GATv2Conv\n",
    "heads = 1\n",
    "dropout = 0.1\n",
    "\n",
    "ensemble = {i : f\"model_ratio_{i}\" for i in [1,2,3,4]}\n",
    "for file in os.listdir(path_ensemble) : \n",
    "    if file[-2:] == \"pt\" and int(file.split(\".\")[3].split(\"Neg\")[0]) in ensemble :\n",
    "        ratio = int(file.split(\".\")[3].split(\"Neg\")[0])\n",
    "        model = Model(conv, hidden_channels, heads, dropout)\n",
    "        model.load_state_dict(torch.load(f\"{path_ensemble}/{file}\"))\n",
    "        Dpo_classifier_models[ensemble[ratio]] = model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452274b6-c2b0-46e1-a1bd-bc9ccdb8e736",
   "metadata": {},
   "source": [
    "> pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f22f9ee-c320-46f2-88d9-547a506aa1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "path_work = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "\n",
    "    # Open the DF\n",
    "DF_info = pd.read_csv(f\"{path_work}/DF_Dpo.final.2705.tsv\", sep = \"\\t\" ,  header = 0 )\n",
    "    # Open the embeddings\n",
    "DF_embeddings = pd.read_csv(f\"{path_work}/Dpo.2705.embeddings.ultimate.csv\", sep = \",\", header= None )\n",
    "DF_embeddings.rename(columns={0: 'index'}, inplace=True)\n",
    "\n",
    "    # Filter the DF :\n",
    "DF_info_filtered = DF_info[~DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "DF_info_ToReLabel = DF_info[DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "all_data = pd.merge(DF_info_filtered , DF_embeddings , on = \"index\")\n",
    "\n",
    "# Mind the over representation of outbreaks :\n",
    "all_data = all_data.drop_duplicates(subset = [\"Infected_ancestor\",\"index\",\"prophage_id\"] , keep = \"first\").reset_index(drop=True)\n",
    "\n",
    "df_kltype = all_data[all_data[\"KL_type_LCA\"] == \"KL27\"]\n",
    "df_kltype = df_kltype.drop_duplicates(subset = [\"Phage\"] , keep = \"first\").reset_index(drop=True)\n",
    "\n",
    "indexation_nodes_A = all_data[\"Infected_ancestor\"].unique().tolist()  \n",
    "indexation_nodes_B1 = all_data[\"Phage\"].unique().tolist() + [f\"Dpo_to_predict_{n}\" for n in DF_embeddings[\"index\"].unique().tolist()]\n",
    "indexation_nodes_B2 = DF_embeddings[\"index\"].unique().tolist() \n",
    "\n",
    "ID_nodes_A = {item:index for index, item in enumerate(indexation_nodes_A)}\n",
    "ID_nodes_A_r = {index:item for index, item in enumerate(indexation_nodes_A)}\n",
    "\n",
    "ID_nodes_B1 = {item:index for index, item in enumerate(indexation_nodes_B1)}\n",
    "ID_nodes_B1_r = {index:item for index, item in enumerate(indexation_nodes_B1)}\n",
    "\n",
    "ID_nodes_B2 = {item:index for index, item in enumerate(indexation_nodes_B2)}\n",
    "ID_nodes_B2_r = {index:item for index, item in enumerate(indexation_nodes_B2)}\n",
    "\n",
    "instances_bacteria = all_data.drop_duplicates(subset = [\"KL_type_LCA\"] , keep = \"first\").reset_index(drop=True)\n",
    "index_interest = []\n",
    "for ancestor in instances_bacteria.Infected_ancestor :\n",
    "    index = ID_nodes_A[ancestor]\n",
    "    index_interest.append(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efa862f5-8245-4562-9f97-02d54bc6cbbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mA\u001b[0m={ x=[4530, 127] },\n",
       "  \u001b[1mB1\u001b[0m={ x=[11339, 0] },\n",
       "  \u001b[1mB2\u001b[0m={ x=[3608, 1280] },\n",
       "  \u001b[1m(B1, infects, A)\u001b[0m={\n",
       "    edge_index=[2, 7731],\n",
       "    y=[7731]\n",
       "  },\n",
       "  \u001b[1m(B2, expressed, B1)\u001b[0m={\n",
       "    edge_index=[2, 13285],\n",
       "    y=[13285]\n",
       "  },\n",
       "  \u001b[1m(A, harbors, B1)\u001b[0m={\n",
       "    edge_index=[2, 7731],\n",
       "    y=[7731]\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# *****************************************************************************\n",
    "# Load the Dataframes :\n",
    "path_work = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "graph_data = torch.load(f'{path_work}/graph_file.2607.OHE.pt')\n",
    "graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99ffd156-ee8d-44e9-adb4-70ce97cfe540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_id(B1A_index_file) :\n",
    "    B1A_index_file = B1A_index_file.numpy()\n",
    "    B1A_index_file = tuple(zip(B1A_index_file[0],B1A_index_file[1]))\n",
    "    id_file = [(ID_nodes_B1_r[tup[0]] , ID_nodes_A_r[tup[1]]) for tup in B1A_index_file]\n",
    "    return id_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87f8bb36-594c-4e7e-9008-1f415eed79fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "\n",
    "path_project = \"/media/concha-eloko/Linux/77_strains_phage_project\"\n",
    "path_Dpo_domain_org = \"/media/concha-eloko/Linux/depolymerase_building/clean_77_phages_depo\"\n",
    "\n",
    "dpo_embeddings = pd.read_csv(f\"{path_project}/rbp_work/Dpo_domains_77.esm2.embedding.csv\", sep = \",\" , header = None)\n",
    "dpo_embeddings = dpo_embeddings.drop([1281] , axis = 1)\n",
    "dpo_embeddings.set_index([0], inplace = True)\n",
    "\n",
    "# Adding the phage column : \n",
    "dpo_embeddings[\"phage\"] = dpo_embeddings.index.map(lambda x: x.split(\"__\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98477770-12e4-42f0-a8e6-888e6da05d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the nodes A features : \n",
    "instances_bacteria = all_data.drop_duplicates(subset = [\"KL_type_LCA\"] , keep = \"first\").reset_index(drop=True)\n",
    "index_interest = []\n",
    "for ancestor in instances_bacteria.Infected_ancestor :\n",
    "    index = ID_nodes_A[ancestor]\n",
    "    index_interest.append(index)\n",
    "    \n",
    "tensor_interest = [graph_data.x_dict[\"A\"][i] for _,i in enumerate(index_interest)]\n",
    "stacked_tensor = torch.stack(tensor_interest)\n",
    "dico_kltype = {tuple(graph_data.x_dict[\"A\"][i].numpy()) : all_data[all_data[\"Infected_ancestor\"] == ID_nodes_A_r[i]][\"KL_type_LCA\"].values[0]  for _,i in enumerate(index_interest)}\n",
    "\n",
    "def graph_single_Dpo_pred(df_embeddings) : \n",
    "    pred_data_single = HeteroData()\n",
    "    # Defining the nodes :\n",
    "    l_dpos = len(df_embeddings)\n",
    "    pred_data_single[\"A\"].x = stacked_tensor\n",
    "    pred_data_single[\"B1\"].x = torch.empty((l_dpos, 0))\n",
    "    pred_data_single[\"B2\"].x = torch.tensor(df_embeddings.iloc[:, :1280].values , dtype=torch.float)\n",
    "    # Defining the edge_file :\n",
    "    edge_index_B2_B1 = torch.tensor([[i , i] for i in range(l_dpos)] , dtype=torch.long)\n",
    "    pred_data_single['B2', 'expressed', 'B1'].edge_index = edge_index_B2_B1.t().contiguous()\n",
    "    edge_index_B1_A = torch.tensor([[i,j] for i in range(l_dpos) for j in range(len(pred_data_single[\"A\"].x))] , dtype=torch.long)\n",
    "    pred_data_single['B1', 'infects', 'A'].edge_label_index = edge_index_B1_A.t().contiguous()\n",
    "    return pred_data_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3283490-32d7-430a-a2f4-2d262481e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the graph prediction : \n",
    "pred_data_single = HeteroData()\n",
    "\n",
    "# Defining the nodes :\n",
    "l_dpos = len(dpo_embeddings)\n",
    "pred_data_single[\"A\"].x = stacked_tensor\n",
    "pred_data_single[\"B1\"].x = torch.empty((l_dpos, 0))\n",
    "pred_data_single[\"B2\"].x = torch.tensor(dpo_embeddings.iloc[:, :1280].values , dtype=torch.float)\n",
    "\n",
    "# Defining the edge_file :\n",
    "edge_index_B2_B1 = torch.tensor([[i , i] for i in range(l_dpos)] , dtype=torch.long)\n",
    "pred_data_single['B2', 'expressed', 'B1'].edge_index = edge_index_B2_B1.t().contiguous()\n",
    "edge_index_B1_A = torch.tensor([[i,j] for i in range(l_dpos) for j in range(len(pred_data_single[\"A\"].x))] , dtype=torch.long)\n",
    "pred_data_single['B1', 'infects', 'A'].edge_label_index = edge_index_B1_A.t().contiguous()\n",
    "\n",
    "import json\n",
    "\n",
    "class CustomEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.float32):\n",
    "            return float(obj)\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "        \n",
    "def get_nodes_id_single(B1A_index_file) :\n",
    "    B1A_index_file = B1A_index_file.numpy()\n",
    "    B1A_index_file = tuple(zip(B1A_index_file[0],B1A_index_file[1]))\n",
    "    id_file = [(dpo_embeddings.index[tup[0]] , dico_kltype[tuple(tensor_interest[tup[1]].numpy())]) for tup in B1A_index_file]\n",
    "    return id_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b4c9ba-389d-48f1-bef9-e75b9bdba8d6",
   "metadata": {},
   "source": [
    "> run the predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f088dd61-6394-46ae-b05d-f4b8261b935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "round_prediction = {}\n",
    "\n",
    "with open(f\"{path_work}/77_Dpos.PPT_pred.1408.json\", \"w\") as outfile :\n",
    "    clean_results = {}\n",
    "    predictions, probabilities = make_predictions(model, pred_data_single)\n",
    "    ids = get_nodes_id_single(pred_data_single[(\"B1\", \"infects\", \"A\")].edge_label_index)\n",
    "    results = tuple(zip(ids,predictions.numpy(),probabilities.numpy()))\n",
    "    positive_results = [pred for pred in results if int(pred[1]) == 1]\n",
    "    for pos_res in positive_results : \n",
    "        prot = pos_res[0][0]\n",
    "        kltype = pos_res[0][1]\n",
    "        score = pos_res[2]\n",
    "        a = {}\n",
    "        a[kltype] = score\n",
    "        if score > 0.0 : \n",
    "            #a = {\"KLtype\" : kltype , \"Score\" : score}\n",
    "            if prot not in clean_results : \n",
    "                clean_results[prot] = a\n",
    "            else :\n",
    "                clean_results[prot].update(a)\n",
    "    json.dump(clean_results , outfile, cls=CustomEncoder)\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(width = 250, sort_dicts = True, compact = True)\n",
    "pp.pprint(clean_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c18d56e-bf8d-456d-b17d-639c5434f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = {}\n",
    "\n",
    "for protein,hits in clean_results.items() : \n",
    "    phage = protein.split(\"__\")[0]\n",
    "    if phage not in final_results : \n",
    "        tmp_hits = {}\n",
    "        for kltype in hits : \n",
    "            if kltype in tmp_hits and hits[kltype] > tmp_hits[kltype]:\n",
    "                tmp_hits[kltype] = hits[kltype]\n",
    "            elif kltype in tmp_hits and hits[kltype] < tmp_hits[kltype]:\n",
    "                pass\n",
    "            elif kltype not in tmp_hits : \n",
    "                tmp_hits[kltype] = hits[kltype]\n",
    "        final_results[phage] = tmp_hits\n",
    "    else :\n",
    "        for kltype in hits : \n",
    "            if kltype in final_results[phage] and hits[kltype] > final_results[phage][kltype]:\n",
    "                final_results[phage][kltype] = hits[kltype]\n",
    "            elif kltype in final_results[phage] and hits[kltype] < final_results[phage][kltype]:\n",
    "                pass\n",
    "            elif kltype not in final_results[phage] : \n",
    "                final_results[phage][kltype] = hits[kltype]\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(width = 150, sort_dicts = True, compact = True)\n",
    "pp.pprint(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647155c8-5f65-4ad6-a441-b3b6b32c9209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3509c4d4-de72-4101-ba30-1317b6cd2e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "kltype_interest = [\"KL112\", \"KL17\", \"KL2\", \"KL24\", \"KL27\",\"KL64\"]\n",
    "\n",
    "for kltype in instances_bacteria.KL_type_LCA : \n",
    "    if kltype in kltype_interest :\n",
    "        print(kltype)\n",
    "        for phage in final_results : \n",
    "            if kltype in final_results[phage] : \n",
    "                print(phage , final_results[phage][kltype])\n",
    "        print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geometric",
   "language": "python",
   "name": "torch_geometric"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
