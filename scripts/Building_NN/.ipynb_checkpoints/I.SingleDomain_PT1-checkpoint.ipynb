{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal here is to correct the label before the finetuning of ESM2\n",
    "***\n",
    "## I. Generate the DF_info and the DF_embeddings \n",
    "## II. Tryout multiple algoritms : \n",
    "> A. Ensemble method : Random Forest <br>\n",
    "> B. Ensemble method : XGboost (?) <br>\n",
    "> C. NN : Feed-forward Neural Network (FNN) <br>\n",
    "> D. NN : Recurrent Neural Networks ; LSTM <br>\n",
    "> E. NN : Graph Neural Networks  <br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/concha-eloko/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Open the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_work = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023\"\n",
    "path_work = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "\n",
    "# Open the DF\n",
    "DF_info = pd.read_csv(f\"{path_work}/DF_Dpo.final.2705.tsv\", sep = \"\\t\" ,  header = 0 )\n",
    "# Open the embeddings\n",
    "DF_embeddings = pd.read_csv(f\"{path_work}/Dpo.2705.embeddings.ultimate.csv\", sep = \",\", header= None )\n",
    "DF_embeddings.rename(columns={0: 'index'}, inplace=True)\n",
    "\n",
    "# Filter the DF :\n",
    "DF_info_filtered = DF_info[~DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "DF_info_ToReLabel = DF_info[DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "all_data = pd.merge(DF_info_filtered , DF_embeddings , on = \"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Label encoder ; define test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102417/1843363355.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nn_test[\"KL_type_LCA\"] = LE.fit_transform(nn_test[\"KL_type_LCA\"])\n"
     ]
    }
   ],
   "source": [
    "all_data = all_data.drop_duplicates(subset = [\"Infected_ancestor\",\"index\"] , keep = \"first\")\n",
    "nn_test = all_data[0:1000]\n",
    "\n",
    "LE  = LabelEncoder()\n",
    "all_data[\"KL_type_LCA\"] = LE.fit_transform(all_data[\"KL_type_LCA\"])\n",
    "nn_test[\"KL_type_LCA\"] = LE.fit_transform(nn_test[\"KL_type_LCA\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 100,   97,    1,  247,   12,    4,  130,  542, 1241,   27,   13,\n",
       "          6,   51,   91,   68,    7,   24,    1,   23,   17,   26,    7,\n",
       "         50,    1,   34,   36,   12,   21,   20,   47,   34,   82,    1,\n",
       "          3,    3,    3,   26,    6,   13,  131,   15,    3,   13,    9,\n",
       "          1,   42,    7,    4,    1,   55,  132,    7,   41,    9,   18,\n",
       "          1,   17,    8,    7,   13,   22,    2,    5,    7,    7,   24,\n",
       "        597,    3,   26,   75,  347,   30,   72,   90,  128,  304,  352,\n",
       "         14,  117,  105,   38,  148,   90,   15,    1,   16,   21,   99,\n",
       "          5,   88,   26,   15,    1,   14,    9,   54,   75,   80,  691,\n",
       "         25,    3,   23,  107,   45,   29,   12,   23,   16,   60,    6,\n",
       "          1,    9,   72,    5,  142,   65,  968,   10,   13,   39,   30,\n",
       "         12,   88,   11,   46,    9,   19])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the number of samples per class\n",
    "class_samples = np.bincount(all_data.KL_type_LCA)  # replace 'dataset.y' with your labels\n",
    "class_weights = 1. / torch.tensor(class_samples, dtype=torch.float) \n",
    "\n",
    "# create a weight for each sample\n",
    "all_data_tensor = torch.tensor(all_data.KL_type_LCA.values)\n",
    "weights = class_weights[all_data_tensor]\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "class_samples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class MultiDomainDataset(Dataset):\n",
    "    def __init__(self, all_data):\n",
    "        self.embeddings = all_data[[int(i) for i in range(1, 1281)]]\n",
    "        self.ancestor = all_data[\"Infected_ancestor\"]\n",
    "        self.prophage_id = all_data[\"prophage_id\"]\n",
    "        self.prophage_instance = all_data[\"Phage\"]\n",
    "        self.labels = all_data[\"KL_type_LCA\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item_domain1 = self.embeddings.iloc[idx]\n",
    "        item_domain2 = self.ancestor.iloc[idx]\n",
    "        item_domain3 = self.prophage_id.iloc[idx]\n",
    "        item_domain4 = self.prophage_instance.iloc[idx]\n",
    "        item_domain5 = self.labels.iloc[idx]\n",
    "        return item_domain1, item_domain2, item_domain3 , item_domain4 , item_domain5\n",
    "\n",
    "\n",
    "class SingleDomainDataset(Dataset):\n",
    "    def __init__(self, all_data):\n",
    "        self.embeddings = torch.tensor(all_data[[int(i) for i in range(1, 1281)]].values, dtype=torch.float)\n",
    "        self.labels = torch.tensor(all_data[\"KL_type_LCA\"].values, dtype=torch.long)  # Subtract 1 if labels start from 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item_domain1 = self.embeddings[idx]\n",
    "        item_domain2 = self.labels[idx]\n",
    "        return item_domain1, item_domain2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp, X_test, y_temp, y_test = train_test_split(all_data, all_data[\"KL_type_LCA\"], test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "train_singledata = SingleDomainDataset(X_train)\n",
    "test_singledata = SingleDomainDataset(X_test)\n",
    "\n",
    "train_single_loader = DataLoader(train_singledata, batch_size=4, shuffle=True, num_workers=4)\n",
    "test_single_loader = DataLoader(test_singledata, batch_size=4, shuffle=True, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### II. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">FNN (single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleDomainModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SingleDomainModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1280, hidden_size=512, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.layer1 = nn.Linear(in_features=512*2, out_features=256)  # 512*2 because LSTM is bidirectional\n",
    "        self.layer2 = nn.Linear(in_features=256, out_features=127)\n",
    "        \n",
    "    def forward(self, domain1_data):\n",
    "        out, _ = self.lstm(domain1_data.unsqueeze(1))  # Add extra dimension for sequence length\n",
    "        out = F.relu(out.squeeze(1))\n",
    "        out = F.relu(self.layer1(out)) # Remove the sequence length dimension\n",
    "        out = self.layer2(out) \n",
    "        out = F.softmax(out, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class SingleDomainModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SingleDomainModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(in_features=1280, out_features=512)\n",
    "        self.layer2 = nn.Linear(in_features=512, out_features=127)\n",
    "        \n",
    "    # def forward\n",
    "    def forward(self, domain1_data):\n",
    "        out = F.relu(self.layer1(domain1_data))\n",
    "        out = self.layer2(out)\n",
    "        out = F.softmax(out, dim=1)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SingleDomainModel(\n",
      "  (lstm): LSTM(1280, 512, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (layer1): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (layer2): Linear(in_features=256, out_features=127, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SingleDomainModel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_125619/3351448039.py\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Compute accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             adam(params_with_grad,\n\u001b[0m\u001b[1;32m    158\u001b[0m                  \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                  \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    214\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available\n",
    "\n",
    "model = SingleDomainModel()\n",
    "model = model.to(device)  # Move model to GPU\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_accuracy\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_accuracy\": [],\n",
    "}\n",
    "\n",
    "for epoch in range(10):\n",
    "    # Switch model to training mode\n",
    "    model.train()\n",
    "    epoch_correct = 0\n",
    "    epoch_all = 0\n",
    "    epoch_loss = 0.0\n",
    "    for i, (embeddings , labels) in enumerate(train_single_loader):\n",
    "        embeddings = embeddings.to(device)  # Move data to GPU\n",
    "        labels = labels.to(device)  # Move labels to GPU\n",
    "\n",
    "        outputs = model(embeddings)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        epoch_correct += correct\n",
    "        epoch_all += labels.size(0)        \n",
    "        # Accumulate loss\n",
    "        epoch_loss += loss.item()\n",
    "    # Compute metrics for this epoch\n",
    "    epoch_loss = epoch_loss / len(train_single_loader)\n",
    "    epoch_accuracy = epoch_correct / epoch_all\n",
    "    # Switch model to evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_correct = 0\n",
    "        val_all = 0\n",
    "        val_loss = 0.0\n",
    "        for embeddings, labels in test_single_loader :\n",
    "            embeddings = embeddings.to(device)  # Move data to GPU\n",
    "            labels = labels.to(device)  # Move labels to GPU\n",
    "\n",
    "            outputs = model(embeddings)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            val_correct += correct\n",
    "            val_all += labels.size(0) \n",
    "            val_loss += loss.item()\n",
    "        val_loss = val_loss / len(test_single_loader)\n",
    "        val_accuracy = val_correct / val_all\n",
    "    # Printing\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "    history[\"train_loss\"].append(epoch_loss)\n",
    "    history[\"train_accuracy\"].append(epoch_accuracy)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_accuracy\"].append(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f\"{path_work}/train_nn/single.out\" , \"w\") as outfile :\n",
    "    outfile.write(json.dumps(history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#BATCH --job-name=NN_trials__\n",
    "#SBATCH --qos=short\n",
    "#SBATCH --ntasks=1 \n",
    "#SBATCH --cpus-per-task=10 \n",
    "#SBATCH --mem=50gb \n",
    "#SBATCH --time=1-00:00:00 \n",
    "#SBATCH --output=NN_trials__%j.log \n",
    "\n",
    "source /storage/apps/ANACONDA/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate embeddings\n",
    "\n",
    "python /home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023/train_nn/script_files/train_single_domain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To include the weights : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_work = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023\"\n",
    "path_work = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "\n",
    "DF_info = pd.read_csv(f\"{path_work}/DF_Dpo.final.2705.tsv\", sep = \"\\t\" ,  header = 0 )\n",
    "\n",
    "DF_embeddings = pd.read_csv(f\"{path_work}/Dpo.2705.embeddings.ultimate.csv\", sep = \",\", header= None )\n",
    "DF_embeddings.rename(columns={0: 'index'}, inplace=True)\n",
    "\n",
    "DF_info_filtered = DF_info[~DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "DF_info_ToReLabel = DF_info[DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "\n",
    "all_data = pd.merge(DF_info_filtered , DF_embeddings , on = \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.drop_duplicates(subset = [\"Infected_ancestor\",\"index\"] , keep = \"first\")\n",
    "nn_test = all_data[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LE  = LabelEncoder()\n",
    "all_data[\"KL_type_LCA\"] = LE.fit_transform(all_data[\"KL_type_LCA\"])\n",
    "\n",
    "len(all_data[\"KL_type_LCA\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import torch\n",
    "# calculate the number of samples per class\n",
    "class_samples = np.bincount(all_data.KL_type_LCA)  # replace 'dataset.y' with your labels\n",
    "class_weights = 1. / torch.tensor(class_samples, dtype=torch.float) \n",
    "\n",
    "# create a weight for each sample\n",
    "all_data_tensor = torch.tensor(all_data.KL_type_LCA.values)\n",
    "weights = class_weights[all_data_tensor]\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "# Filter out the classes with less than "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os \n",
    "import pandas as pd\n",
    "import json\n",
    "import warnings\n",
    "from skopt import gp_minimize\n",
    "import time\n",
    "\n",
    "path_work = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023\"\n",
    "\n",
    "# Load the data : \n",
    "DF_info = pd.read_csv(f\"{path_work}/DF_Dpo.final.2705.tsv\", sep = \"\\t\" ,  header = 0 )\n",
    "\n",
    "DF_embeddings = pd.read_csv(f\"{path_work}/Dpo.2705.embeddings.ultimate.csv\", sep = \",\", header= None )\n",
    "DF_embeddings.rename(columns={0: 'index'}, inplace=True)\n",
    "DF_info_filtered = DF_info[~DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "DF_info_ToReLabel = DF_info[DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "all_data = pd.merge(DF_info_filtered , DF_embeddings , on = \"index\")\n",
    "\n",
    "# Encode the label : \n",
    "LE  = LabelEncoder()\n",
    "all_data[\"KL_type_LCA\"] = LE.fit_transform(all_data[\"KL_type_LCA\"])\n",
    "\n",
    "    # Define the classes : \n",
    "# The dataset : \n",
    "class SingleDomainDataset(Dataset):\n",
    "    def __init__(self, all_data):\n",
    "        self.embeddings = torch.tensor(all_data[[int(i) for i in range(1, 1281)]].values, dtype=torch.float)\n",
    "        self.labels = torch.tensor(all_data[\"KL_type_LCA\"].values, dtype=torch.long)  # Subtract 1 if labels start from 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item_domain1 = self.embeddings[idx]\n",
    "        item_domain2 = self.labels[idx]\n",
    "        return item_domain1, item_domain2\n",
    "\n",
    "# The Model : \n",
    "class SingleDomainModel(nn.Module):\n",
    "    def __init__(self ,hidden_size_para, num_layers_para ):\n",
    "        super(SingleDomainModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1280, hidden_size=hidden_size_para, num_layers=num_layers_para, batch_first=True, dropout=0.5, bidirectional=True)\n",
    "        self.layer1 = nn.Linear(in_features=512*2, out_features=256)  # 512*2 because LSTM is bidirectional\n",
    "        self.bn1 = nn.BatchNorm1d(256)  # Batch normalization\n",
    "        self.layer2 = nn.Linear(in_features=256, out_features=127)\n",
    "        self.bn2 = nn.BatchNorm1d(127)  # Batch normalization\n",
    "\n",
    "    def forward(self, domain1_data):\n",
    "        out, _ = self.lstm(domain1_data.unsqueeze(1))  # Add extra dimension for sequence length\n",
    "        out = F.relu(out.squeeze(1))\n",
    "        out = self.bn1(F.relu(self.layer1(out)))  # Apply batch normalization after ReLU\n",
    "        out = self.bn2(self.layer2(out))  # Apply batch normalization to the output of layer2\n",
    "        return out  # Don't apply softmax if you use nn.CrossEntropyLoss in the training\n",
    "\n",
    "# Load the dataset :\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(all_data, all_data[\"KL_type_LCA\"], test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "train_singledata = SingleDomainDataset(X_train)\n",
    "test_singledata = SingleDomainDataset(X_test)\n",
    "\n",
    "train_single_loader = DataLoader(train_singledata, batch_size=4, shuffle=True, num_workers=4, drop_last=True)\n",
    "test_single_loader = DataLoader(test_singledata, batch_size=4, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "\n",
    "space  = [Real(1e-6, 1e-2, \"log-uniform\", name='learning_rate'),\n",
    "          Integer(1, 3, name='num_layers'),\n",
    "          Integer(50, 500, name='num_units')]\n",
    "\n",
    "def train_and_evaluate(params):\n",
    "    t = time.localtime()\n",
    "    current_time = time.strftime(\"%H:%M:%S\", t)\n",
    "    learning_rate, num_layers, num_units = params\n",
    "    # Build the model with the given parameters\n",
    "    model = SingleDomainModel(num_layers, num_units)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)    \n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_accuracy\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_accuracy\": [],\n",
    "    }\n",
    "\n",
    "    for epoch in range(10):\n",
    "        # Switch model to training mode\n",
    "        model.train()\n",
    "        epoch_correct = 0\n",
    "        epoch_all = 0\n",
    "        epoch_loss = 0.0\n",
    "        for i, (embeddings , labels) in enumerate(train_single_loader):\n",
    "            embeddings = embeddings.to(device)  # Move data to GPU\n",
    "            labels = labels.to(device)  # Move labels to GPU\n",
    "\n",
    "            outputs = model(embeddings)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            epoch_correct += correct\n",
    "            epoch_all += labels.size(0)        \n",
    "            # Accumulate loss\n",
    "            epoch_loss += loss.item()\n",
    "        # Compute metrics for this epoch\n",
    "        epoch_loss = epoch_loss / len(train_single_loader)\n",
    "        epoch_accuracy = epoch_correct / epoch_all\n",
    "        # Switch model to evaluation mode\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_correct = 0\n",
    "            val_all = 0\n",
    "            val_loss = 0.0\n",
    "            for embeddings, labels in test_single_loader:\n",
    "                embeddings = embeddings.to(device)  # Move data to GPU\n",
    "                labels = labels.to(device)  # Move labels to GPU\n",
    "\n",
    "                outputs = model(embeddings)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                correct = (predicted == labels).sum().item()\n",
    "                val_correct += correct\n",
    "                val_all += labels.size(0) \n",
    "                val_loss += loss.item()\n",
    "            val_loss = val_loss / len(test_single_loader)\n",
    "            val_accuracy = val_correct / val_all\n",
    "        # Printing\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "        history[\"train_loss\"].append(epoch_loss)\n",
    "        history[\"train_accuracy\"].append(epoch_accuracy)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_accuracy\"].append(val_accuracy)    \n",
    "    with open(f\"{path_work}/train_nn/single.optimized{current_time}_{epoch}.out\" , \"w\") as outfile :\n",
    "        outfile.write(json.dumps(history))\n",
    "    # Train the model...\n",
    "    # Evaluate the model...\n",
    "    # Return the metric to optimize\n",
    "    return validation_loss\n",
    "\n",
    "res_gp = gp_minimize(train_and_evaluate, space, n_calls=50, random_state=0)\n",
    "best_params = res_gp.x\n",
    "\n",
    "\n",
    "with open(f\"{path_work}/train_nn/single.optimized.best_model.out\" , \"w\") as outfile :\n",
    "    outfile.write(best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">FNN (multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiDomainDataset(Dataset):\n",
    "    def __init__(self, all_data):\n",
    "        self.embeddings = all_data[[int(i) for i in range(1, 1281)]]\n",
    "        self.ancestor = all_data[\"Infected_ancestor\"]\n",
    "        self.prophage_id = all_data[\"prophage_id\"]\n",
    "        self.prophage_instance = all_data[\"Phage\"]\n",
    "        self.labels = all_data[\"KL_type_LCA\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item_domain1 = self.embeddings.iloc[idx]\n",
    "        item_domain2 = self.ancestor.iloc[idx]\n",
    "        item_domain3 = self.prophage_id.iloc[idx]\n",
    "        item_domain4 = self.prophage_instance.iloc[idx]\n",
    "        item_domain5 = self.labels.iloc[idx]\n",
    "        return item_domain1, item_domain2, item_domain3 , item_domain4 , item_domain5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all_data, all_data[\"KL_type_LCA\"], test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = MultiDomainDataset(X_train)\n",
    "test_data = MultiDomainDataset(X_test)\n",
    "\n",
    "train_load = DataLoader(train_data, batch_size=4, shuffle=True, num_workers=4)\n",
    "test_load = DataLoader(test_data, batch_size=4, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiDomainModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiDomainModel, self).__init__()\n",
    "        self.branch_embeddings = nn.Linear(in_features=10, out_features=20)\n",
    "        self.branch_ancestor = nn.Linear(in_features=10, out_features=20)\n",
    "        self.branch_prophage_id = nn.Linear(in_features=10, out_features=20)\n",
    "        self.branch_prophage_instance = nn.Linear(in_features=10, out_features=20)\n",
    "        self.final_layer = nn.Linear(in_features=80, out_features=1)\n",
    "        \n",
    "    def forward(self, domain1_data, domain2_data, domain3_data):\n",
    "        out1 = F.relu(self.branch_embeddings(domain1_data))\n",
    "        out2 = F.relu(self.branch_ancestor(domain2_data))\n",
    "        out3 = F.relu(self.branch_prophage_id(domain3_data))\n",
    "        out = torch.cat((out1, out2, out3), dim=1)\n",
    "        out = self.final_layer(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, prophage_vocab_size, prophage_embedding_dim, ancestor_vocab_size, ancestor_embedding_dim, depolymerase_embedding_dim, lstm_hidden_dim, lstm_layers, output_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        # Embedding layers for the categorical inputs\n",
    "        self.prophage_embedding = nn.Embedding(prophage_vocab_size, prophage_embedding_dim)\n",
    "        self.ancestor_embedding = nn.Embedding(ancestor_vocab_size, ancestor_embedding_dim)\n",
    "        \n",
    "        # LSTM for the depolymerase embeddings\n",
    "        self.lstm = nn.LSTM(depolymerase_embedding_dim, lstm_hidden_dim, num_layers=lstm_layers, batch_first=True)\n",
    "\n",
    "        # Fully connected layer for output\n",
    "        self.fc = nn.Linear(prophage_embedding_dim + ancestor_embedding_dim + lstm_hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, prophage, ancestor, depolymerase):\n",
    "        prophage_embed = self.prophage_embedding(prophage)\n",
    "        ancestor_embed = self.ancestor_embedding(ancestor)\n",
    "\n",
    "        # The LSTM expects inputs of the form (batch_size, seq_length, num_features)\n",
    "        lstm_out, _ = self.lstm(depolymerase.unsqueeze(0))\n",
    "\n",
    "        # Concatenate the embeddings\n",
    "        x = torch.cat((prophage_embed, ancestor_embed, lstm_out[0, -1]), dim=-1)\n",
    "\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "search_space = {\n",
    "    'prophage_embedding_dim': Integer(10, 300),\n",
    "    'ancestor_embedding_dim': Integer(10, 300),\n",
    "    'depolymerase_embedding_dim': Integer(10, 300),\n",
    "    'lstm_hidden_dim': Integer(10, 300),\n",
    "    'lstm_layers': Integer(1, 3),\n",
    "    'output_dim': Integer(1, 10),\n",
    "    'learning_rate': Real(1e-6, 1e-2, prior='log-uniform'),\n",
    "}\n",
    "\n",
    "# Wrap your PyTorch model in a scikit-learn compatible estimator\n",
    "estimator = PyTorchEstimator(\n",
    "    build_fn=lambda: SimpleModel(prophage_vocab_size=PROPHAGE_VOCAB_SIZE, ancestor_vocab_size=ANCESTOR_VOCAB_SIZE),\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    optimizer=lambda params: torch.optim.Adam(params, lr=0.001),\n",
    "    train_split=None,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Initialize the BayesSearchCV object\n",
    "bayes_search = BayesSearchCV(estimator, search_space, n_iter=50, cv=3)\n",
    "\n",
    "# Perform the search\n",
    "bayes_search.fit([prophage_data, ancestor_data, depolymerase_data], labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "prophage_data = torch.tensor(prophage_data)  # Shape: [num_samples]\n",
    "ancestor_data = torch.tensor(ancestor_data)  # Shape: [num_samples]\n",
    "depolymerase_data = torch.tensor(depolymerase_data)  # Shape: [num_samples, seq_length, num_features]\n",
    "labels = torch.tensor(labels)  # Shape: [num_samples]\n",
    "\n",
    "# Create the model\n",
    "model = SimpleModel(prophage_vocab_size=PROPHAGE_VOCAB_SIZE, prophage_embedding_dim=PROPHAGE_EMBEDDING_DIM, ancestor_vocab_size=ANCESTOR_VOCAB_SIZE, ancestor_embedding_dim=ANCESTOR_EMBEDDING_DIM, depolymerase_embedding_dim=DEPOLYMERASE_EMBEDDING_DIM, lstm_hidden_dim=LSTM_HIDDEN_DIM, lstm_layers=LSTM_LAYERS, output_dim=OUTPUT_DIM)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(prophage_data, ancestor_data, depolymerase_data)\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, NUM_EPOCHS, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, depolymerase_test_data, labels_test_data, criterion):\n",
    "    model.eval()  # switch the model to evaluation mode\n",
    "    with torch.no_grad():  # don't compute gradients while evaluating\n",
    "        outputs_test = model(depolymerase_test_data)\n",
    "        loss_test = criterion(outputs_test, labels_test_data)\n",
    "    print('Test Loss: {:.4f}'.format(loss_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# After initial training\n",
    "evaluate_model(model, depolymerase_test_data, labels_test_data, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transfer learning part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class TransferLearningModel(nn.Module):\n",
    "    def __init__(self, pretrained_model, new_input_dim, new_output_dim):\n",
    "        super(TransferLearningModel, self).__init__()\n",
    "        \n",
    "        # Reuse the LSTM and fully connected layer from the pretrained model\n",
    "        self.lstm = pretrained_model.lstm\n",
    "        self.fc = nn.Linear(new_input_dim, new_output_dim)\n",
    "\n",
    "    def forward(self, depolymerase):\n",
    "        # Pass the data through the LSTM\n",
    "        lstm_out, _ = self.lstm(depolymerase.unsqueeze(0))\n",
    "\n",
    "        # Pass the output through the fully connected layer\n",
    "        return self.fc(lstm_out[0, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "evaluate_model(transfer_learning_model, depolymerase_test_data, labels_test_data, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "> Graph Convolutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=-UjytpbqX4A <br>\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/notes/heterogeneous.html<br>\n",
    "https://www.youtube.com/watch?v=-UjytpbqX4A<br>\n",
    "Inputs Explanation:\n",
    "\n",
    "    data.x: This should be a [num_nodes, num_node_features] matrix containing your node features. In your case, these are the amino acid sequence and embedding representation of the depolymerase sequences.\n",
    "    \n",
    "    data.edge_index: This is your graph adjacency matrix, represented as a [2, num_edges] matrix where each column represents the two nodes that define an edge. The entries in edge_index are the indices of the nodes in the node feature matrix data.x.\n",
    "    \n",
    "    data.y: This should be a [num_nodes] vector containing your target labels. In your case, these are the true KL_type of each node.\n",
    "    \n",
    "    data.train_mask: This should be a [num_nodes] boolean vector indicating which nodes should be used for training. You could create training, validation, and test masks depending on how you want to split your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Define a Graph Convolutional Network\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x  # This is now your embedding\n",
    "\n",
    "# Define a Classifier\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.fc(x), dim=1)\n",
    "\n",
    "# Bayesian search\n",
    "def objective(trial):\n",
    "    # Suggest values of the hyperparameters using a trial object.\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "    layers = trial.suggest_int('layers', 1, 3)\n",
    "\n",
    "    # Define and train your model here\n",
    "    gcn_model = GCN(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, layers=layers)\n",
    "    classifier_model = Classifier(input_dim=HIDDEN_DIM, output_dim=OUTPUT_DIM)\n",
    "\n",
    "    gcn_optimizer = torch.optim.Adam(gcn_model.parameters(), lr=lr)\n",
    "    classifier_optimizer = torch.optim.Adam(classifier_model.parameters(), lr=lr)\n",
    "\n",
    "    # The rest of your training code here...\n",
    "    # Be sure to return a value to minimize (e.g., validation loss)\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(' Value: ', trial.value)\n",
    "print(' Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print('    {}: {}'.format(key, value))\n",
    "\n",
    "# Instantiate the models\n",
    "gcn_model = GCN(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM)\n",
    "classifier_model = Classifier(input_dim=HIDDEN_DIM, output_dim=OUTPUT_DIM)\n",
    "\n",
    "# Define your optimizer and loss function for each model\n",
    "gcn_optimizer = torch.optim.Adam(gcn_model.parameters(), lr=0.01)\n",
    "classifier_optimizer = torch.optim.Adam(classifier_model.parameters(), lr=0.01)\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop for GCN\n",
    "for epoch in range(200): # adjust number of epochs as needed\n",
    "    gcn_model.train()\n",
    "    gcn_optimizer.zero_grad()\n",
    "    embeddings = gcn_model(data)  # forward pass\n",
    "\n",
    "    # We don't have a loss for the GCN itself, because it's unsupervised. We just need the embeddings\n",
    "\n",
    "# Training loop for Classifier\n",
    "for epoch in range(200): # adjust number of epochs as needed\n",
    "    classifier_model.train()\n",
    "    classifier_optimizer.zero_grad()\n",
    "\n",
    "    out = classifier_model(embeddings)  # forward pass using the embeddings as input\n",
    "\n",
    "    # Compute the loss only for training nodes\n",
    "    loss = loss_func(out[data.train_mask], data.y[data.train_mask]) \n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    classifier_optimizer.step()\n",
    "\n",
    "    # Print loss\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With heterogenous nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "# Create feature matrices for prophages and bacteria nodes\n",
    "prophage_features = torch.randn((num_prophages, num_prophage_features))  # Including depolymerase embeddings\n",
    "bacteria_features = torch.randn((num_bacteria, num_bacteria_features))  # Including KL_type\n",
    "\n",
    "# Create edge index for infection events\n",
    "edge_index = torch.tensor([\n",
    "    prophage_indices,  # Source nodes (prophages)\n",
    "    bacteria_indices,  # Target nodes (bacteria)\n",
    "], dtype=torch.long)\n",
    "\n",
    "# Create data object\n",
    "data = Data(\n",
    "    x={'prophage': prophage_features, 'bacteria': bacteria_features},\n",
    "    edge_index={'infection': edge_index},\n",
    "    # Add any additional data here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "        self.conv_prophage = GCNConv(input_dim['prophage'], hidden_dim)\n",
    "        self.conv_bacteria = GCNConv(input_dim['bacteria'], hidden_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x['prophage'] = self.conv_prophage(x['prophage'], edge_index['infection'])\n",
    "        x['bacteria'] = self.conv_bacteria(x['bacteria'], edge_index['infection'])\n",
    "\n",
    "        return x  # This is now your embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Instantiate the model with input, hidden and output dimensions\n",
    "model = GCN(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, output_dim=OUTPUT_DIM)\n",
    "\n",
    "# Define your optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(200): # adjust number of epochs as needed\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data) # forward pass\n",
    "\n",
    "    # Compute the loss only for training nodes\n",
    "    loss = loss_func(out[data.train_mask], data.y[data.train_mask]) \n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class GCNWithEmbeddings(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCNWithEmbeddings, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)  # The output is now an embedding\n",
    "        self.classifier = torch.nn.Linear(hidden_dim, output_dim)  # This is a separate layer for classification\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)  # This is now your embedding\n",
    "\n",
    "        out = self.classifier(x)  # This is your KL_type prediction\n",
    "\n",
    "        return out, x  # Return both the prediction and the embedding\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "sklearn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
