{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData, DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, to_hetero , SAGEConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder , label_binarize , OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import random\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Load the Dataframes :\n",
    "path_work = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023\"\n",
    "\n",
    "graph_data = torch.load(f'{path_work}/train_nn/graph_file.1107.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# The model :\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(-1, hidden_channels, add_self_loops=False)\n",
    "        self.conv2 = GCNConv(-1, out_channels, add_self_loops=False)\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def forward(self, x, edge_index):\n",
    "        edge_feat_B1 = x[\"B1\"][edge_index[(\"B1\", \"infects\", \"A\")][0]]\n",
    "        edge_feat_A = x[\"A\"][edge_index[(\"B1\", \"infects\", \"A\")][1]]\n",
    "        return (edge_feat_B1 * edge_feat_A).sum(dim=-1)\n",
    "    \n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.gnn_B2_B1 = GNN(hidden_channels, out_channels)\n",
    "        self.gnn_B1_A = GNN(hidden_channels, out_channels)\n",
    "        self.classifier = Classifier()\n",
    "    def forward(self, graph_data):\n",
    "        # Propagate B2 features to B1\n",
    "        x_B2 = graph_data['B2'].x\n",
    "        edge_index_B2_B1 = graph_data[('B2', 'expressed', 'B1')].edge_index\n",
    "        x_B1_from_B2 = self.gnn_B2_B1(x_B2, edge_index_B2_B1)\n",
    "        # Propagate new B1 features to A\n",
    "        x_B1 = x_B1_from_B2  \n",
    "        edge_index_B1_A = graph_data[('B1', 'infects', 'A')].edge_index\n",
    "        x_A_from_B1 = self.gnn_B1_A(x_B1, edge_index_B1_A)\n",
    "        # Classification based on new features\n",
    "        x = {'B1': x_B1_from_B2, 'A': x_A_from_B1}\n",
    "        pred = self.classifier(x['B1'], x['A'])\n",
    "        return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12661/3247577671.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m transform = T.RandomLinkSplit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mnum_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnum_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#disjoint_train_ratio=...,  # TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mneg_sampling_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'T' is not defined"
     ]
    }
   ],
   "source": [
    "# *****************************************************************************\n",
    "# Pre-process data :\n",
    "transform = T.RandomLinkSplit(\n",
    "    num_val=0.1, \n",
    "    num_test=0.2, \n",
    "    #disjoint_train_ratio=...,  \n",
    "    neg_sampling_ratio=1.0,  \n",
    "    add_negative_train_samples=True, \n",
    "    edge_types=(\"B1\", \"infects\", \"A\"),\n",
    "    rev_edge_types=(\"A\", \"harbors\", \"B1\"), \n",
    ")\n",
    "\n",
    "train_data, val_data, test_data = transform(graph_data)\n",
    "\n",
    "train_loader = LinkNeighborLoader(\n",
    "    data=train_data,  \n",
    "    num_neighbors= [-1],  \n",
    "    edge_label_index=((\"B1\", \"infects\", \"A\"), train_data[\"B1\", \"infects\", \"A\"].edge_label_index),\n",
    "    edge_label=train_data[\"B1\", \"infects\", \"A\"].edge_label,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = LinkNeighborLoader(\n",
    "    data=val_data,  \n",
    "    num_neighbors= [-1],  \n",
    "    edge_label_index=((\"B1\", \"infects\", \"A\"), val_data[\"B1\", \"infects\", \"A\"].edge_label_index),\n",
    "    edge_label=val_data[\"B1\", \"infects\", \"A\"].edge_label,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = LinkNeighborLoader(\n",
    "    data=test_data,  \n",
    "    num_neighbors= [-1],  \n",
    "    edge_label_index=((\"B1\", \"infects\", \"A\"), test_data[\"B1\", \"infects\", \"A\"].edge_label_index),\n",
    "    edge_label=test_data[\"B1\", \"infects\", \"A\"].edge_label,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Training :\n",
    "def train(model, loader, optimizer, criterion, edge_type):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        edge_labels = data[edge_type].y[data[edge_type].train_mask]\n",
    "        loss = criterion(out[data[edge_type].train_mask], edge_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, edge_type):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data)\n",
    "        edge_labels = data[edge_type].y[data[edge_type].val_mask]\n",
    "        val_loss = criterion(pred[data[edge_type].val_mask], edge_labels)\n",
    "        total_loss += val_loss.item()\n",
    "        _, pred_class = pred.max(dim=1)\n",
    "        all_preds.extend(pred_class.cpu().numpy())\n",
    "        all_labels.extend(edge_labels.cpu().numpy())\n",
    "    # Calculate the metrics\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    return total_loss / len(loader), f1, precision, accuracy, auc\n",
    "\n",
    "def main():\n",
    "    hidden_channels = 580 \n",
    "    out_channels = 100 \n",
    "    model = GNN(hidden_channels, out_channels).to(device)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    edge_type = (\"B1\", \"infects\", \"A\")  # adjust this as per your requirement\n",
    "    for epoch in range(100): \n",
    "        train_loss = train(model, train_loader, optimizer, criterion, edge_type)\n",
    "        if epoch % 10 == 0:\n",
    "            test_loss, f1, precision, accuracy, auc = evaluate(model, test_loader, criterion, edge_type)\n",
    "            print(f'Epoch: {epoch}, Train Loss: {train_loss}, Test Loss: {test_loss}, F1 Score: {f1}, Precision: {precision}, Accuracy: {accuracy}, AUC: {auc}')\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), f\"{path_work}/GCNConv.model.1307.pt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "for fold, (train_idx, test_idx) in enumerate(kfold.split(graph_data.node_items['A'], graph_data.y)):\n",
    "    train_data = graph_data[train_idx]\n",
    "    test_data = graph_data[test_idx]\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=32)\n",
    "    for epoch in range(100):  # adjust as needed\n",
    "        train_loss = train(model, train_loader, optimizer, criterion)\n",
    "        print(f\"Fold: {fold+1}, Epoch: {epoch+1}, Train loss: {train_loss}\")\n",
    "    acc, prec, rec, f1, auroc = test(model, test_loader)\n",
    "    print(f\"Fold: {fold+1}, Accuracy: {acc}, Precision: {prec}, Recall: {rec}, F1-score: {f1}, AUROC: {auroc}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "sklearn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
