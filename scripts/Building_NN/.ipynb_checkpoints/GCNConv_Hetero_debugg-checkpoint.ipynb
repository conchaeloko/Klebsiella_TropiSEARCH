{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData, DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, to_hetero , SAGEConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder , label_binarize , OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import random\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mA\u001b[0m={ x=[4530, 127] },\n",
       "  \u001b[1mB1\u001b[0m={ x=[11339, 0] },\n",
       "  \u001b[1mB2\u001b[0m={ x=[3608, 1280] },\n",
       "  \u001b[1m(B1, infects, A)\u001b[0m={\n",
       "    edge_index=[2, 9677],\n",
       "    y=[9677]\n",
       "  },\n",
       "  \u001b[1m(B2, expressed, B1)\u001b[0m={\n",
       "    edge_index=[2, 13285],\n",
       "    y=[13285]\n",
       "  },\n",
       "  \u001b[1m(A, harbors, B1)\u001b[0m={\n",
       "    edge_index=[2, 9677],\n",
       "    y=[9677]\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_work = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "graph_data = torch.load(f'{path_work}/graph_file.1107.pt')\n",
    "\n",
    "graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1909,  2314,   122,  ...,  3605,  3606,  3607],\n",
       "        [    0,     0,     1,  ..., 11336, 11337, 11338]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_data[('B2', 'expressed', 'B1')].edge_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Pre-process data :\n",
    "transform = T.RandomLinkSplit(\n",
    "    num_val=0.1, \n",
    "    num_test=0.2, \n",
    "    #disjoint_train_ratio=...,  \n",
    "    neg_sampling_ratio=1.0,  \n",
    "    add_negative_train_samples=True, \n",
    "    edge_types=(\"B1\", \"infects\", \"A\"),\n",
    "    rev_edge_types=(\"A\", \"harbors\", \"B1\"), \n",
    ")\n",
    "\n",
    "train_data, val_data, test_data = transform(graph_data)\n",
    "\n",
    "train_loader = LinkNeighborLoader(\n",
    "    data=train_data,  \n",
    "    num_neighbors= [-1],  \n",
    "    edge_label_index=((\"B1\", \"infects\", \"A\"), train_data[\"B1\", \"infects\", \"A\"].edge_label_index),\n",
    "    edge_label=train_data[\"B1\", \"infects\", \"A\"].edge_label,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = LinkNeighborLoader(\n",
    "    data=val_data,  \n",
    "    num_neighbors= [-1],  \n",
    "    edge_label_index=((\"B1\", \"infects\", \"A\"), val_data[\"B1\", \"infects\", \"A\"].edge_label_index),\n",
    "    edge_label=val_data[\"B1\", \"infects\", \"A\"].edge_label,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = LinkNeighborLoader(\n",
    "    data=test_data,  \n",
    "    num_neighbors= [-1],  \n",
    "    edge_label_index=((\"B1\", \"infects\", \"A\"), test_data[\"B1\", \"infects\", \"A\"].edge_label_index),\n",
    "    edge_label=test_data[\"B1\", \"infects\", \"A\"].edge_label,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_heterograph\u001b[39m(data):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_heterograph(data):\n",
    "    G = nx.DiGraph()\n",
    "    node_labels = {}\n",
    "    # Adding nodes\n",
    "    for node_type in ['A', 'B1', 'B2']:\n",
    "        for node_id in range(data[node_type].num_nodes):\n",
    "            G.add_node(f'{node_type}_{node_id}')\n",
    "            node_labels[f'{node_type}_{node_id}'] = f'{node_type}_{node_id}'\n",
    "\n",
    "    # Adding edges\n",
    "    for edge_type in [('B1', 'infects', 'A'), ('B2', 'expressed', 'B1'), ('A', 'harbors', 'B1')]:\n",
    "        for i in range(data[edge_type].num_edges):\n",
    "            src, dest = data[edge_type].edge_index[:, i]\n",
    "            G.add_edge(f'{edge_type[0]}_{src}', f'{edge_type[2]}_{dest}', label=edge_type[1])\n",
    "\n",
    "    # Plotting\n",
    "    pos = nx.spring_layout(G)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    nx.draw(G, pos, labels=node_labels, with_labels=True, node_size=1000, node_color=\"skyblue\", node_shape=\"s\", alpha=0.5, linewidths=40)\n",
    "    edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mA\u001b[0m={\n",
       "    x=[162, 127],\n",
       "    n_id=[162]\n",
       "  },\n",
       "  \u001b[1mB1\u001b[0m={\n",
       "    x=[299, 0],\n",
       "    n_id=[299]\n",
       "  },\n",
       "  \u001b[1mB2\u001b[0m={\n",
       "    x=[133, 1280],\n",
       "    n_id=[133]\n",
       "  },\n",
       "  \u001b[1m(B1, infects, A)\u001b[0m={\n",
       "    edge_index=[2, 293],\n",
       "    y=[293],\n",
       "    edge_label=[128],\n",
       "    edge_label_index=[2, 128],\n",
       "    e_id=[293],\n",
       "    input_id=[128]\n",
       "  },\n",
       "  \u001b[1m(B2, expressed, B1)\u001b[0m={\n",
       "    edge_index=[2, 167],\n",
       "    y=[167],\n",
       "    e_id=[167]\n",
       "  },\n",
       "  \u001b[1m(A, harbors, B1)\u001b[0m={\n",
       "    edge_index=[2, 125],\n",
       "    y=[125],\n",
       "    e_id=[125]\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   1,   2,   3,   3,   4,   4,   5,   5,   6,   6,   7,   8,   9,\n",
       "         10,  11,  12,  13,  14,  14,  14,  15,  15,  15,  16,  16,  16,  17,\n",
       "         17,  17,  18,  18,  19,  19,  20,  20,  21,  21,  22,  22,  23,  23,\n",
       "         24,  24,  25,  25,  26,  26,  27,  28,  29,  30,  31,  32,  32,  33,\n",
       "         34,  35,  36,  36,  37,  37,  38,  39,  40,  40,  41,  42,  42,  43,\n",
       "         43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  52,  53,  54,  54,\n",
       "         55,  56,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  65,  66,\n",
       "         67,  68,  68,  69,  70,  71,  72,  72,  73,  74,  75,  76,  77,  77,\n",
       "         78,  79,  80,  80,  81,  82,  83,  84,  85,  86,  87,  87,  88,  89,\n",
       "         90,  91,  91,  92,  93,  94,  95,  95,  96,  97,  98,  99, 100, 101,\n",
       "        102, 103, 104, 105, 106, 107, 108, 108, 109, 110, 111, 112, 113, 114,\n",
       "        115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data = next(iter(train_loader))\n",
    "sampled_data.x_dict[\"A\"]\n",
    "sampled_data.edge_index_dict[('B2','expressed','B1')][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.2863, -0.1950,  0.0128,  0.1964,  0.0717, -0.1568, -0.2021, -0.0185,\n",
       "         -0.1320, -0.2648], grad_fn=<SelectBackward0>),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_nn = GNN(20,10) \n",
    "out = debug_nn(sampled_data.x_dict[\"B2\"] , sampled_data.edge_index_dict[('B2','expressed','B1')])\n",
    "\n",
    "out[0] , out[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# The model : Dot product\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(-1, hidden_channels, add_self_loops=False)\n",
    "        self.conv2 = GCNConv(-1, out_channels, add_self_loops=False)\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def forward(self, x, edge_index):\n",
    "        edge_feat_B1 = x[\"B1\"][edge_index[(\"B1\", \"infects\", \"A\")][0]]\n",
    "        edge_feat_A = x[\"A\"][edge_index[(\"B1\", \"infects\", \"A\")][1]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        return (edge_feat_B1 * edge_feat_A).sum(dim=-1)\n",
    "    \n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.gnn_B2_B1 = GNN(hidden_channels, out_channels)\n",
    "        self.gnn_B1_A = GNN(hidden_channels, out_channels)\n",
    "        self.classifier = Classifier()\n",
    "    def forward(self, graph_data):\n",
    "        # Propagate B2 features to B1\n",
    "        x_B2 = graph_data['B2'].x\n",
    "        edge_index_B2_B1 = graph_data[('B2', 'expressed', 'B1')].edge_index\n",
    "        x_B1_from_B2 = self.gnn_B2_B1(x_B2, edge_index_B2_B1)  # Added edge_index_B2_B1\n",
    "        # Propagate new B1 features to A\n",
    "        edge_index_B1_A = graph_data[('B1', 'infects', 'A')].edge_index\n",
    "        x_A_from_B1 = self.gnn_B1_A(x_B1_from_B2, edge_index_B1_A)  # Added edge_index_B1_A\n",
    "        # Classification based on new features\n",
    "        x = {'B1': x_B1_from_B2, 'A': x_A_from_B1}\n",
    "        pred = self.classifier(x, edge_index_B1_A)  # Passed x dictionary and edge_index_B1_A to classifier\n",
    "        return pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Training :\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available\n",
    "\n",
    "def train(model, loader, optimizer, criterion, edge_type):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        edge_labels = data[edge_type].y[data[edge_type].train_mask]\n",
    "        loss = criterion(out[data[edge_type].train_mask], edge_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, edge_type):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data)\n",
    "        edge_labels = data[edge_type].y[data[edge_type].val_mask]\n",
    "        val_loss = criterion(pred[data[edge_type].val_mask], edge_labels)\n",
    "        total_loss += val_loss.item()\n",
    "        _, pred_class = pred.max(dim=1)\n",
    "        all_preds.extend(pred_class.cpu().numpy())\n",
    "        all_labels.extend(edge_labels.cpu().numpy())\n",
    "    # Calculate the metrics\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    return total_loss / len(loader), f1, precision, accuracy, auc\n",
    "\n",
    "def main():\n",
    "    hidden_channels = 580 \n",
    "    out_channels = 100 \n",
    "    model = Model(hidden_channels, out_channels).to(device)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    edge_type = (\"B1\", \"infects\", \"A\")  # adjust this as per your requirement\n",
    "    for epoch in range(100): \n",
    "        train_loss = train(model, train_loader, optimizer, criterion, edge_type)\n",
    "        if epoch % 10 == 0:\n",
    "            test_loss, f1, precision, accuracy, auc = evaluate(model, test_loader, criterion, edge_type)\n",
    "            print(f'Epoch: {epoch}, Train Loss: {train_loss}, Test Loss: {test_loss}, F1 Score: {f1}, Precision: {precision}, Accuracy: {accuracy}, AUC: {auc}')\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), f\"{path_work}/GCNConv.model.1307.pt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#BATCH --job-name=GCNConv__\n",
    "#SBATCH --qos=short\n",
    "#SBATCH --ntasks=1 \n",
    "#SBATCH --cpus-per-task=10 \n",
    "#SBATCH --mem=100gb \n",
    "#SBATCH --time=1-00:00:00 \n",
    "#SBATCH --output=GCNConv__%j.log \n",
    "\n",
    "source /storage/apps/ANACONDA/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate torch_geometric\n",
    "\n",
    "python /home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023/train_nn/script_files/GCNConv_Hetero.dot.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "for fold, (train_idx, test_idx) in enumerate(kfold.split(graph_data.node_items['A'], graph_data.y)):\n",
    "    train_data = graph_data[train_idx]\n",
    "    test_data = graph_data[test_idx]\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=32)\n",
    "    for epoch in range(100):  # adjust as needed\n",
    "        train_loss = train(model, train_loader, optimizer, criterion)\n",
    "        print(f\"Fold: {fold+1}, Epoch: {epoch+1}, Train loss: {train_loss}\")\n",
    "    acc, prec, rec, f1, auroc = test(model, test_loader)\n",
    "    print(f\"Fold: {fold+1}, Accuracy: {acc}, Precision: {prec}, Recall: {rec}, F1-score: {f1}, AUROC: {auroc}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geometric",
   "language": "python",
   "name": "torch_geometric"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
