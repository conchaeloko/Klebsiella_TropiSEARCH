{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData, DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, to_hetero , SAGEConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder , label_binarize , OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import random\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Load the Dataframes :\n",
    "path_work = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "#path_work = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023\"\n",
    "\n",
    "    # Open the DF\n",
    "DF_info = pd.read_csv(f\"{path_work}/DF_Dpo.final.2705.tsv\", sep = \"\\t\" ,  header = 0 )\n",
    "    # Open the embeddings\n",
    "DF_embeddings = pd.read_csv(f\"{path_work}/Dpo.2705.embeddings.ultimate.csv\", sep = \",\", header= None )\n",
    "DF_embeddings.rename(columns={0: 'index'}, inplace=True)\n",
    "\n",
    "    # Filter the DF :\n",
    "DF_info_filtered = DF_info[~DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "DF_info_ToReLabel = DF_info[DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "all_data = pd.merge(DF_info_filtered , DF_embeddings , on = \"index\")\n",
    "\n",
    "# Mind the over representation of outbreaks :\n",
    "all_data = all_data.drop_duplicates(subset = [\"Infected_ancestor\",\"index\",\"prophage_id\"] , keep = \"first\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mA\u001b[0m={ x=[4530, 127] },\n",
       "  \u001b[1mB1\u001b[0m={ x=[11339, 0] },\n",
       "  \u001b[1mB2\u001b[0m={ x=[3608, 1280] },\n",
       "  \u001b[1m(B1, infects, A)\u001b[0m={\n",
       "    edge_index=[2, 9677],\n",
       "    y=[9677]\n",
       "  },\n",
       "  \u001b[1m(B2, expressed, B1)\u001b[0m={\n",
       "    edge_index=[2, 13285],\n",
       "    y=[13285]\n",
       "  },\n",
       "  \u001b[1m(A, harbors, B1)\u001b[0m={\n",
       "    edge_index=[2, 9677],\n",
       "    y=[9677]\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_data = torch.load(f'{path_work}/graph_file.1107.pt')\n",
    "#graph_data = torch.load(f'{path_work}/train_nn/graph_file.1107.pt')\n",
    "\n",
    "graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phage</th>\n",
       "      <th>KL_type_LCA</th>\n",
       "      <th>Infected_ancestor</th>\n",
       "      <th>Protein_name</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>index</th>\n",
       "      <th>seq</th>\n",
       "      <th>prophage_id</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>...</th>\n",
       "      <th>1271</th>\n",
       "      <th>1272</th>\n",
       "      <th>1273</th>\n",
       "      <th>1274</th>\n",
       "      <th>1275</th>\n",
       "      <th>1276</th>\n",
       "      <th>1277</th>\n",
       "      <th>1278</th>\n",
       "      <th>1279</th>\n",
       "      <th>1280</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9673</th>\n",
       "      <td>GCF_004311345.1__phage11</td>\n",
       "      <td>KL34</td>\n",
       "      <td>GCF_004311345.1</td>\n",
       "      <td>GCF_004311345.1__phage11__99</td>\n",
       "      <td>anubis</td>\n",
       "      <td>anubis__1644</td>\n",
       "      <td>MTANYPASILPPNATAVERAIDRASAAALERLPVYLIRWVKDPDSC...</td>\n",
       "      <td>prophage_11944</td>\n",
       "      <td>-0.001691</td>\n",
       "      <td>-0.067717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119783</td>\n",
       "      <td>0.073706</td>\n",
       "      <td>0.057788</td>\n",
       "      <td>0.004341</td>\n",
       "      <td>0.007389</td>\n",
       "      <td>-0.081588</td>\n",
       "      <td>0.100995</td>\n",
       "      <td>-0.045545</td>\n",
       "      <td>0.021685</td>\n",
       "      <td>0.013167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 1288 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Phage KL_type_LCA Infected_ancestor  \\\n",
       "9673  GCF_004311345.1__phage11        KL34   GCF_004311345.1   \n",
       "\n",
       "                      Protein_name Dataset         index  \\\n",
       "9673  GCF_004311345.1__phage11__99  anubis  anubis__1644   \n",
       "\n",
       "                                                    seq     prophage_id  \\\n",
       "9673  MTANYPASILPPNATAVERAIDRASAAALERLPVYLIRWVKDPDSC...  prophage_11944   \n",
       "\n",
       "             1         2  ...      1271      1272      1273      1274  \\\n",
       "9673 -0.001691 -0.067717  ...  0.119783  0.073706  0.057788  0.004341   \n",
       "\n",
       "          1275      1276      1277      1278      1279      1280  \n",
       "9673  0.007389 -0.081588  0.100995 -0.045545  0.021685  0.013167  \n",
       "\n",
       "[1 rows x 1288 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[all_data[\"index\"] == \"anubis__1644\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Build the graph :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Nodes A : the bacteria (ancestors) - KLtype feature\n",
    "# Nodes B1 : the prophage (phage) - No feature\n",
    "# Nodes B2 : the depo (index_seq) - 1280-d embeddings\n",
    "\n",
    "# Build the Graph Data :\n",
    "graph_data = HeteroData()\n",
    "\n",
    "    # Indexation process (shall add the N phages to predict)\n",
    "indexation_nodes_A = all_data[\"Infected_ancestor\"].unique().tolist()  \n",
    "indexation_nodes_B1 = all_data[\"Phage\"].unique().tolist() + [f\"Dpo_to_predict_{n}\" for n in DF_embeddings[\"index\"].unique().tolist()]\n",
    "indexation_nodes_B2 = DF_embeddings[\"index\"].unique().tolist() \n",
    "\n",
    "ID_nodes_A = {item:index for index, item in enumerate(indexation_nodes_A)}\n",
    "ID_nodes_A_r = {index:item for index, item in enumerate(indexation_nodes_A)}\n",
    "\n",
    "ID_nodes_B1 = {item:index for index, item in enumerate(indexation_nodes_B1)}\n",
    "ID_nodes_B1_r = {index:item for index, item in enumerate(indexation_nodes_B1)}\n",
    "\n",
    "ID_nodes_B2 = {item:index for index, item in enumerate(indexation_nodes_B2)}\n",
    "ID_nodes_B2_r = {index:item for index, item in enumerate(indexation_nodes_B2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Make the node feature file : \n",
    "OHE = OneHotEncoder(sparse=False)\n",
    "one_hot_encoded = OHE.fit_transform(all_data[[\"KL_type_LCA\"]])\n",
    "label_mapping = {label: one_hot_encoded[i] for i, label in enumerate(OHE.categories_[0])}\n",
    "\n",
    "node_feature_A = torch.tensor([label_mapping[all_data[all_data[\"Infected_ancestor\"] == ID_nodes_A_r[i]][\"KL_type_LCA\"].values[0]] for i in range(0,len(ID_nodes_A_r))], dtype=torch.float)\n",
    "node_feature_B1 = torch.zeros((len(ID_nodes_B1), 0), dtype=torch.float)\n",
    "node_feature_B2 = torch.tensor([DF_embeddings[DF_embeddings[\"index\"] == ID_nodes_B2_r[i]].values[0][1:1281].tolist() for i in range(0,len(ID_nodes_B2_r))] , dtype=torch.float)\n",
    "\n",
    "# feed the graph\n",
    "graph_data[\"A\"].x = node_feature_A\n",
    "graph_data[\"B1\"].x = node_feature_B1\n",
    "graph_data[\"B2\"].x = node_feature_B2\n",
    "\n",
    "# Write files : \n",
    "node_feature_A_array = node_feature_A.numpy()\n",
    "node_feature_B1_array = node_feature_B1.numpy()\n",
    "node_feature_B2_array = node_feature_B2.numpy()\n",
    "\n",
    "df_node_feature_A_array = pd.DataFrame(node_feature_A_array)\n",
    "df_node_feature_B1_array = pd.DataFrame(node_feature_B1_array)\n",
    "df_node_feature_B2_array = pd.DataFrame(node_feature_B2_array)\n",
    "\n",
    "df_node_feature_A_array.to_csv(f\"{path_work}/node_features.A.csv\", index=False, header=False)\n",
    "df_node_feature_B1_array.to_csv(f\"{path_work}/node_features.B1.csv\", index=False, header=False)\n",
    "df_node_feature_B2_array.to_csv(f\"{path_work}/node_features.B2.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Make edge file\n",
    "# Node B1 (prophage) - Node A (bacteria) :\n",
    "edge_index_B1_A = []\n",
    "for _, row in all_data.iterrows() :\n",
    "    edge_index_B1_A.append([ID_nodes_B1[row[\"Phage\"]], ID_nodes_A[row[\"Infected_ancestor\"]]])\n",
    "edge_index_B1_A = torch.tensor(edge_index_B1_A , dtype=torch.long)\n",
    "\n",
    "# Node A (bacteria) - Node B1 (prophage) :\n",
    "edge_index_A_B1 = []\n",
    "for _, row in all_data.iterrows() :\n",
    "    edge_index_A_B1.append([ID_nodes_A[row[\"Infected_ancestor\"]] , ID_nodes_B1[row[\"Phage\"]]])\n",
    "edge_index_A_B1 = torch.tensor(edge_index_A_B1 , dtype=torch.long)\n",
    "\n",
    "# Node B2 (depolymerase) - Node B1 (prophage) :\n",
    "edge_index_B2_B1 = []\n",
    "for phage in all_data.Phage.unique() :\n",
    "    all_data_phage = all_data[all_data[\"Phage\"] == phage]\n",
    "    for _, row in all_data_phage.iterrows() :\n",
    "        edge_index_B2_B1.append([ID_nodes_B2[row[\"index\"]], ID_nodes_B1[row[\"Phage\"]]])\n",
    "# Add in there the edges between the fake prophages and the each Dpos :\n",
    "for prophage , index in ID_nodes_B1.items() :\n",
    "    if prophage.count(\"Dpo_to_predict_\") > 0 : \n",
    "        id_dpo = prophage.split(\"Dpo_to_predict_\")[1]\n",
    "        edge_index_B2_B1.append([ID_nodes_B2[id_dpo], index])\n",
    "edge_index_B2_B1 = torch.tensor(edge_index_B2_B1 , dtype=torch.long)\n",
    "\n",
    "# feed the graph\n",
    "graph_data['B1', 'infects', 'A'].edge_index = edge_index_B1_A.t().contiguous()\n",
    "graph_data['B2', 'expressed', 'B1'].edge_index = edge_index_B2_B1.t().contiguous()\n",
    "# That one is optional  \n",
    "graph_data['A', 'harbors', 'B1'].edge_index = edge_index_A_B1.t().contiguous()\n",
    "\n",
    "# Write files : \n",
    "edge_index_B1_A_array = edge_index_B1_A.numpy()\n",
    "edge_index_A_B1_array = edge_index_A_B1.numpy()\n",
    "edge_index_B2_B1_array = edge_index_B2_B1.numpy()\n",
    "\n",
    "df_edge_index_B1_A_array = pd.DataFrame(edge_index_B1_A_array)\n",
    "df_edge_index_A_B1_array = pd.DataFrame(edge_index_A_B1_array)\n",
    "df_edge_index_B2_B1_array = pd.DataFrame(edge_index_B2_B1_array)\n",
    "\n",
    "df_edge_index_B1_A_array.to_csv(f\"{path_work}/edge_index_B1_A_array.csv\", index=False, header=False)\n",
    "df_edge_index_A_B1_array.to_csv(f\"{path_work}/edge_index_A_B1_array.csv\", index=False, header=False)\n",
    "df_edge_index_B2_B1_array.to_csv(f\"{path_work}/edge_index_B2_B1_array.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "    # Make the Y file : \n",
    "graph_data['B1', 'infects', 'A'].y = torch.ones(len(graph_data['B1', 'infects', 'A'].edge_index[0]))\n",
    "graph_data['B2', 'expressed', 'B1'].y = torch.ones(len(graph_data['B2', 'expressed', 'B1'].edge_index[0]))\n",
    "# That one is optional  \n",
    "graph_data['A', 'harbors', 'B1'].y = torch.ones(len(graph_data['A', 'harbors', 'B1'].edge_index[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Work on the GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Data instance : \n",
    "\n",
    "#graph_data\n",
    "#torch.save(graph_data , f'{path_work}/graph_file.1107.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mA\u001b[0m={ x=[4530, 127] },\n",
       "  \u001b[1mB1\u001b[0m={ x=[11339, 0] },\n",
       "  \u001b[1mB2\u001b[0m={ x=[3608, 1280] },\n",
       "  \u001b[1m(B1, infects, A)\u001b[0m={\n",
       "    edge_index=[2, 9677],\n",
       "    y=[9677]\n",
       "  },\n",
       "  \u001b[1m(B2, expressed, B1)\u001b[0m={\n",
       "    edge_index=[2, 13285],\n",
       "    y=[13285]\n",
       "  },\n",
       "  \u001b[1m(A, harbors, B1)\u001b[0m={\n",
       "    edge_index=[2, 9677],\n",
       "    y=[9677]\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot the graph : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HeteroData' has no attribute 'nodes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_644183/2598760266.py\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{path_work}/graph_representation.svg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mvisualize_hetero_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_work\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_644183/2598760266.py\u001b[0m in \u001b[0;36mvisualize_hetero_graph\u001b[0;34m(data, path_work)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiDiGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mnode_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnode_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/torch_geometric/data/hetero_data.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_dict$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         raise AttributeError(f\"'{self.__class__.__name__}' has no \"\n\u001b[0m\u001b[1;32m    138\u001b[0m                              f\"attribute '{key}'\")\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HeteroData' has no attribute 'nodes'"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_graph(G, color):\n",
    "    G = G.to_homogeneous()\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False,\n",
    "                     node_color=color, cmap=\"Set2\")\n",
    "    plt.show()\n",
    "    #plt.save(f\"{path_work}/graph_representation.svg\" , dpi = 800)\n",
    "    \n",
    "G = to_networkx(data_sub, to_undirected=False)\n",
    "visualize_graph(G, color=data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(-1, hidden_channels, add_self_loops=False)\n",
    "        self.conv2 = GCNConv(-1, out_channels, add_self_loops=False)\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def forward(self, x, edge_index):\n",
    "        edge_feat_B1 = x[\"B1\"][edge_index[(\"B1\", \"infects\", \"A\")][0]]\n",
    "        edge_feat_A = x[\"A\"][edge_index[(\"B1\", \"infects\", \"A\")][1]]\n",
    "        return (edge_feat_B1 * edge_feat_A).sum(dim=-1)\n",
    "    \n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.gnn_B2_B1 = GNN(hidden_channels, out_channels)\n",
    "        self.gnn_B1_A = GNN(hidden_channels, out_channels)\n",
    "        self.classifier = Classifier()\n",
    "    def forward(self, graph_data):\n",
    "        # Propagate B2 features to B1\n",
    "        x_B2 = graph_data['B2'].x\n",
    "        edge_index_B2_B1 = graph_data[('B2', 'expressed', 'B1')].edge_index\n",
    "        x_B1_from_B2 = self.gnn_B2_B1(x_B2, edge_index_B2_B1)\n",
    "        # Propagate new B1 features to A\n",
    "        x_B1 = x_B1_from_B2  \n",
    "        edge_index_B1_A = graph_data[('B1', 'infects', 'A')].edge_index\n",
    "        x_A_from_B1 = self.gnn_B1_A(x_B1, edge_index_B1_A)\n",
    "        # Classification based on new features\n",
    "        x = {'B1': x_B1_from_B2, 'A': x_A_from_B1}\n",
    "        pred = self.classifier(x['B1'], x['A'])\n",
    "        return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Template (functional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(-1, hidden_channels, add_self_loops=False)\n",
    "        self.conv2 = GCNConv(-1, out_channels, add_self_loops=False)\n",
    "        #self.conv2 = GCNConv(-1, hidden_channels, add_self_loops=False)\n",
    "        #self.conv3 = GCNConv(-1, out_channels, add_self_loops=False)\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        #x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self, A_dim, B1_dim):\n",
    "        super().__init__()\n",
    "        self.pad = torch.nn.ConstantPad1d((0, A_dim - B1_dim), 0)\n",
    "    def forward(self, x, edge_index):\n",
    "        edge_feat_B1 = x[\"B1\"][edge_index[(\"B1\", \"infects\", \"A\")][0]]\n",
    "        edge_feat_A = x[\"A\"][edge_index[(\"B1\", \"infects\", \"A\")][1]]\n",
    "        # Padding B1 features to match A features dimension\n",
    "        edge_feat_B1 = self.pad(edge_feat_B1)\n",
    "        return (edge_feat_B1 * edge_feat_A).sum(dim=-1)\n",
    "    \n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.gnn_B2_B1 = GNN(hidden_channels, out_channels)\n",
    "        self.gnn_B1_A = GNN(hidden_channels, out_channels)\n",
    "        self.classifier = Classifier()\n",
    "    def forward(self, graph_data):\n",
    "        # Propagate B2 features to B1\n",
    "        x_B2 = graph_data['B2'].x\n",
    "        edge_index_B2_B1 = graph_data[('B2', 'expressed', 'B1')].edge_index\n",
    "        x_B1_from_B2 = self.gnn_B2_B1(x_B2, edge_index_B2_B1)\n",
    "        # Propagate new B1 features to A\n",
    "        x_B1 = x_B1_from_B2  \n",
    "        edge_index_B1_A = graph_data[('B1', 'infects', 'A')].edge_index\n",
    "        x_A_from_B1 = self.gnn_B1_A(x_B1, edge_index_B1_A)\n",
    "        # Concatenate the updated A features with the initial A features\n",
    "        x_A_initial = graph_data['A'].x.clone() # Keeping a copy of initial features\n",
    "        x_A = torch.cat((x_A_initial, x_A_from_B1), dim=-1)\n",
    "        # Classification based on new features\n",
    "        x = {'B1': x_B1_from_B2, 'A': x_A}\n",
    "        pred = self.classifier(x, edge_index_B1_A)\n",
    "        return pred\n",
    "\n",
    "model = Model(hidden_channels= 580 , out_channels = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "> Training : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12661/3247577671.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m transform = T.RandomLinkSplit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mnum_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnum_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#disjoint_train_ratio=...,  # TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mneg_sampling_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'T' is not defined"
     ]
    }
   ],
   "source": [
    "transform = T.RandomLinkSplit(\n",
    "    num_val=0.1, \n",
    "    num_test=0.2, \n",
    "    #disjoint_train_ratio=...,  \n",
    "    neg_sampling_ratio=1.0,  \n",
    "    add_negative_train_samples=True, \n",
    "    edge_types=(\"B1\", \"infects\", \"A\"),\n",
    "    rev_edge_types=(\"A\", \"harbors\", \"B1\"), \n",
    ")\n",
    "\n",
    "train_data, val_data, test_data = transform(graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = LinkNeighborLoader(\n",
    "    data=train_data,  \n",
    "    num_neighbors= [-1],  \n",
    "    edge_label_index=((\"B1\", \"infects\", \"A\"), train_data[\"B1\", \"infects\", \"A\"].edge_label_index),\n",
    "    edge_label=train_data[\"B1\", \"infects\", \"A\"].edge_label,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = LinkNeighborLoader(\n",
    "    data=val_data,  \n",
    "    num_neighbors= [-1],  \n",
    "    edge_label_index=((\"B1\", \"infects\", \"A\"), val_data[\"B1\", \"infects\", \"A\"].edge_label_index),\n",
    "    edge_label=val_data[\"B1\", \"infects\", \"A\"].edge_label,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = LinkNeighborLoader(\n",
    "    data=test_data,  \n",
    "    num_neighbors= [-1],  \n",
    "    edge_label_index=((\"B1\", \"infects\", \"A\"), test_data[\"B1\", \"infects\", \"A\"].edge_label_index),\n",
    "    edge_label=test_data[\"B1\", \"infects\", \"A\"].edge_label,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "'NeighborSampler' requires either 'pyg-lib' or 'torch-sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_644183/3298059406.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msampled_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/torch_geometric/loader/base.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/torch_geometric/loader/link_loader.py\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mEdgeSamplerInput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         out = self.link_sampler.sample_from_edges(\n\u001b[0m\u001b[1;32m    183\u001b[0m             input_data, neg_sampling=self.neg_sampling)\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/torch_geometric/sampler/neighbor_sampler.py\u001b[0m in \u001b[0;36msample_from_edges\u001b[0;34m(self, inputs, neg_sampling)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mneg_sampling\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNegativeSampling\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     ) -> Union[SamplerOutput, HeteroSamplerOutput]:\n\u001b[0;32m--> 182\u001b[0;31m         return edge_sample(inputs, self._sample, self.num_nodes, self.disjoint,\n\u001b[0m\u001b[1;32m    183\u001b[0m                            self.node_time, neg_sampling)\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/torch_geometric/sampler/neighbor_sampler.py\u001b[0m in \u001b[0;36medge_sample\u001b[0;34m(inputs, sample_fn, num_nodes, disjoint, node_time, neg_sampling)\u001b[0m\n\u001b[1;32m    479\u001b[0m                 }\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_time_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;31m# Enhance `out` by label information ##################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/torch_geometric/sampler/neighbor_sampler.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, seed, seed_time, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                 raise ImportError(f\"'{self.__class__.__name__}' requires \"\n\u001b[0m\u001b[1;32m    260\u001b[0m                                   f\"either 'pyg-lib' or 'torch-sparse'\")\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: 'NeighborSampler' requires either 'pyg-lib' or 'torch-sparse'"
     ]
    }
   ],
   "source": [
    "sampled_data = next(iter(train_loader))\n",
    "print(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_graph(G, color):\n",
    "    G = G.to_homogeneous()\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False,\n",
    "                     node_color=color, cmap=\"Set2\")\n",
    "    plt.show()\n",
    "    #plt.save(f\"{path_work}/graph_representation.svg\" , dpi = 800)\n",
    "    \n",
    "G = to_networkx(data_sub, to_undirected=False)\n",
    "visualize_graph(G, color=data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define seed edges:\n",
    "edge_label_index = train_data[\"B1\", \"infects\", \"A\"].edge_label_index\n",
    "edge_label = train_data[\"B1\", \"infects\", \"A\"].edge_label\n",
    "\n",
    "train_loader = LinkNeighborLoader(\n",
    "    data=...,  # TODO\n",
    "    num_neighbors=...,  # TODO\n",
    "    neg_sampling_ratio=...,  # TODO\n",
    "    edge_label_index=((\"B1\", \"infects\", \"A\"), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# setup Dataloader for batch processing\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # get the inputs\n",
    "        inputs, labels = data.x, data.y\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # collect all labels and predictions for metrics calculation\n",
    "        all_labels += labels.tolist()\n",
    "        all_preds += torch.sigmoid(outputs).detach().tolist()\n",
    "\n",
    "    # calculate metrics\n",
    "    if epoch % 10 == 0:\n",
    "        preds_bin = [1 if x >= 0.5 else 0 for x in all_preds]\n",
    "        precision = precision_score(all_labels, preds_bin)\n",
    "        f1 = f1_score(all_labels, preds_bin)\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "        print(f'Epoch: {epoch}, Loss: {loss.item()}, Precision: {precision}, F1-score: {f1}, AUC: {auc}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model and the optimizer\n",
    "model = Model(hidden_channels=580, out_channels=100).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        edge_index = data[('B1', 'infects', 'A')].edge_index\n",
    "        pos_out = model(data)\n",
    "        # Negative sampling\n",
    "        total_edge = edge_index.size(1)\n",
    "        edge_index, _ = negative_sampling(edge_index, num_nodes=data['A'].x.size(0), num_neg_samples=total_edge)\n",
    "        neg_out = model(data)\n",
    "        out = torch.cat([pos_out, neg_out])\n",
    "        # Generate labels for the loss function\n",
    "        y = torch.cat([torch.ones(total_edge, device=device), torch.zeros(total_edge, device=device)])\n",
    "        loss = loss_function(out, y)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(data)\n",
    "        y_pred.append(pred.cpu().numpy())\n",
    "        y_true.append(data.y.cpu().numpy())\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    return roc_auc_score(y_true, y_pred)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100): \n",
    "    loss = train()\n",
    "    auc = test(test_loader)\n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss:.4f}, AUC: {auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data)\n",
    "        val_loss = criterion(pred[data.val_mask], data.y[data.val_mask])\n",
    "        total_loss += val_loss.item()\n",
    "        _, pred_class = pred.max(dim=1)\n",
    "        all_preds.extend(pred_class.cpu().numpy())\n",
    "        all_labels.extend(data.y[data.val_mask].cpu().numpy())\n",
    "    # Calculate the metrics\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    return total_loss / len(loader), f1, precision, accuracy, auc\n",
    "\n",
    "def main():\n",
    "    hidden_channels = 580 \n",
    "    out_channels = 100 \n",
    "    model = GNN(hidden_channels, out_channels).to(device)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    for epoch in range(100): \n",
    "        train_loss = train(model, train_loader, optimizer, criterion)\n",
    "        if epoch % 10 == 0:\n",
    "            test_loss, f1, precision, accuracy, auc = evaluate(model, test_loader, criterion)\n",
    "            print(f'Epoch: {epoch}, Train Loss: {train_loss}, Test Loss: {test_loss}, F1 Score: {f1}, Precision: {precision}, Accuracy: {accuracy}, AUC: {auc}')\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), f\"{path_work}/GCNConv.model.1307.pt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test\n",
    "train_idx, test_idx = train_test_split(range(len(graph_data['B1', 'infects', 'A'].y)), test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model, optimizer and loss function\n",
    "model = Model(hidden_channels=64)\n",
    "model = to_hetero(model, graph_data, aggr='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "for epoch in range(100):  # adjust as needed\n",
    "    optimizer.zero_grad()\n",
    "    out = model(train_data)\n",
    "    pos_score = model.classifier(out, train_data.edge_index)\n",
    "    neg_score = model.classifier(out, generate_negative_samples(train_data.edge_index, train_data.num_nodes, train_data.edge_index.size(1)))\n",
    "    loss = criterion(torch.cat([pos_score, neg_score]), torch.cat([torch.ones(pos_score.size()), torch.zeros(neg_score.size())]))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Train loss: {loss.item()}\")\n",
    "\n",
    "out = model(test_data)\n",
    "pred = torch.sigmoid(model.classifier(out, test_data.edge_index)).numpy()\n",
    "acc = accuracy_score(test_data.y.numpy(), pred > 0.5)\n",
    "prec = precision_score(test_data.y.numpy(), pred > 0.5)\n",
    "rec = recall_score(test_data.y.numpy(), pred > 0.5)\n",
    "f1 = f1_score(test_data.y.numpy(), pred > 0.5)\n",
    "auroc = roc_auc_score(test_data.y.numpy(), pred)\n",
    "\n",
    "print(f\"Accuracy: {acc}, Precision: {prec}, Recall: {rec}, F1-score: {f1}, AUROC: {auroc}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Train / Eval : \n",
    "def generate_negative_samples(edge_index, num_nodes, num_neg_samples):\n",
    "    all_possible_edges = torch.tensor(list(product(range(num_nodes['A']), range(num_nodes['B1']))), dtype=torch.long).t()\n",
    "    positive_edges = set((i.item(), j.item()) for i, j in edge_index.t().tolist())\n",
    "    all_possible_edges = [edge for edge in all_possible_edges if edge not in positive_edges]\n",
    "    negative_edges = random.sample(all_possible_edges, min(num_neg_samples, len(all_possible_edges)))\n",
    "    return torch.tensor(negative_edges, dtype=torch.long).t()\n",
    "\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        pos_batch = batch.edge_index\n",
    "        pos_score = model.classifier(out, pos_batch)\n",
    "        neg_batch = generate_negative_samples(batch.edge_index, batch.num_nodes, pos_batch.size(1))\n",
    "        neg_score = model.classifier(out, neg_batch)\n",
    "        all_score = torch.cat([pos_score, neg_score], dim=0)\n",
    "        all_target = torch.cat([torch.ones(pos_score.size(0)), torch.zeros(neg_score.size(0))], dim=0).to(device)\n",
    "        loss = criterion(all_score, all_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    for batch in loader:\n",
    "        out = model(batch)\n",
    "        pred = torch.sigmoid(model.classifier(out, batch.edge_index)).numpy()\n",
    "        all_preds.append(pred)\n",
    "        all_labels.append(batch.y.numpy())\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    return accuracy_score(all_labels, all_preds > 0.5), precision_score(all_labels, all_preds > 0.5), recall_score(all_labels, all_preds > 0.5), f1_score(all_labels, all_preds > 0.5), roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "# Split the dataset into 5 folds for cross-validation\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Initialize the model, optimizer and loss function\n",
    "model = Model(hidden_channels=64)\n",
    "model = to_hetero(model, data, aggr='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "#criterion = F.binary_cross_entropy_with_logits()\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Cross-validation\n",
    "for fold, (train_idx, test_idx) in enumerate(kfold.split(graph_data.node_data['A'], graph_data.y)):\n",
    "    train_data = graph_data[train_idx]\n",
    "    test_data = graph_data[test_idx]\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=32)\n",
    "    for epoch in range(100):  # adjust as needed\n",
    "        train_loss = train(model, train_loader, optimizer, criterion)\n",
    "        print(f\"Fold: {fold+1}, Epoch: {epoch+1}, Train loss: {train_loss}\")\n",
    "    acc, prec, rec, f1, auroc = test(model, test_loader)\n",
    "    print(f\"Fold: {fold+1}, Accuracy: {acc}, Precision: {prec}, Recall: {rec}, F1-score: {f1}, AUROC: {auroc}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "for fold, (train_idx, test_idx) in enumerate(kfold.split(graph_data.node_items['A'], graph_data.y)):\n",
    "    train_data = graph_data[train_idx]\n",
    "    test_data = graph_data[test_idx]\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=32)\n",
    "    for epoch in range(100):  # adjust as needed\n",
    "        train_loss = train(model, train_loader, optimizer, criterion)\n",
    "        print(f\"Fold: {fold+1}, Epoch: {epoch+1}, Train loss: {train_loss}\")\n",
    "    acc, prec, rec, f1, auroc = test(model, test_loader)\n",
    "    print(f\"Fold: {fold+1}, Accuracy: {acc}, Precision: {prec}, Recall: {rec}, F1-score: {f1}, AUROC: {auroc}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Save model after training\n",
    "torch.save(model.state_dict(), f\"{path_work}/train_nn/MultiDomain.LSTM.model\")\n",
    "\n",
    "import json\n",
    "with open(f\"{path_work}/train_nn/MultiDomain.LSTM.model.out\" , \"w\") as outfile :\n",
    "    outfile.write(json.dumps(history))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "sklearn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
