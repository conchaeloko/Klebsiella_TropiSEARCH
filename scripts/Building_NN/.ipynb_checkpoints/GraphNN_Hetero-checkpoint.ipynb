{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder , label_binarize , OneHotEncoder\n",
    "from sklearn.metrics import average_precision_score\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Load the Dataframes :\n",
    "#path_work = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023\"\n",
    "path_work = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "\n",
    "    # Open the DF\n",
    "DF_info = pd.read_csv(f\"{path_work}/DF_Dpo.final.2705.tsv\", sep = \"\\t\" ,  header = 0 )\n",
    "    # Open the embeddings\n",
    "DF_embeddings = pd.read_csv(f\"{path_work}/Dpo.2705.embeddings.ultimate.csv\", sep = \",\", header= None )\n",
    "DF_embeddings.rename(columns={0: 'index'}, inplace=True)\n",
    "\n",
    "    # Filter the DF :\n",
    "DF_info_filtered = DF_info[~DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "DF_info_ToReLabel = DF_info[DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "all_data = pd.merge(DF_info_filtered , DF_embeddings , on = \"index\")\n",
    "\n",
    "# Mind the over representation of outbreaks :\n",
    "all_data = all_data.drop_duplicates(subset = [\"Infected_ancestor\",\"index\",\"prophage_id\"] , keep = \"first\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Build the Graph Data :\n",
    "\n",
    "    # Indexation process (shall add the N phages to predict)\n",
    "indexation = all_data[\"Infected_ancestor\"].unique().tolist() + all_data[\"Phage\"].unique().tolist() + all_data[\"index\"].unique().tolist() + [f\"Dpo_to_predict_{n}\" for n in DF_info[\"index\"].unique().tolist()]\n",
    "dico_ID = {item:index for index, item in enumerate(indexation)}\n",
    "\n",
    "    # Make edge file \n",
    "edge_index = []\n",
    "# Node A (bacteria) - Node B1 (prophage) :\n",
    "for _, row in all_data.iterrows() :\n",
    "    edge_index.append([dico_ID[row[\"Phage\"]], dico_ID[row[\"Infected_ancestor\"]]])\n",
    "# Node B1 - Node B2 (depolymerase) :\n",
    "for phage in all_data.Phage.unique() :\n",
    "    all_data_phage = all_data[all_data[\"Phage\"] == phage]\n",
    "    for _, row in all_data_phage.iterrows() :\n",
    "        edge_index.append([dico_ID[row[\"index\"]], dico_ID[row[\"Phage\"]]])\n",
    "# Transform into tensor : \n",
    "edge_index_tensor = torch.tensor(edge_index , dtype=torch.long)\n",
    "# Write file : \n",
    "numpy_array = edge_index_tensor.numpy()\n",
    "df = pd.DataFrame(numpy_array)\n",
    "df.to_csv(f\"{path_work}/edge_index.csv\", index=False, header=False)\n",
    "\n",
    "    # Make the node feature file : \n",
    "LE  = LabelEncoder()\n",
    "di = LE.fit_transform(all_data[\"KL_type_LCA\"])\n",
    "label_mapping = dict(zip(LE.classes_, LE.transform(LE.classes_)))\n",
    "\n",
    "node_feature = []\n",
    "for index, item in tqdm(enumerate(indexation)) :\n",
    "    features = [index]\n",
    "    if item in all_data[\"Infected_ancestor\"].unique() : \n",
    "        KL_type = all_data[all_data[\"Infected_ancestor\"] == item][\"KL_type_LCA\"].values[0]\n",
    "        features = features + [label_mapping[KL_type]] + [-1]*1280\n",
    "    elif item in all_data[\"Phage\"].unique() : \n",
    "        features = features + [-1]*1281\n",
    "    elif item in all_data[\"index\"].unique() : \n",
    "        features = features + [-1] + DF_embeddings[DF_embeddings[\"index\"] == item].values[0][1:1281].tolist()\n",
    "    elif item in [f\"Dpo_to_predict_{n}\" for n in DF_info[\"index\"].unique().tolist()] : \n",
    "        features = features + [-1]*1281\n",
    "    node_feature.append(features)\n",
    "    \n",
    "# Transform into tensor : \n",
    "node_feature_tensor = torch.tensor(node_feature , dtype=torch.float)\n",
    "# Write file : \n",
    "numpy_array = node_feature_tensor.numpy()\n",
    "df = pd.DataFrame(numpy_array)\n",
    "df.to_csv(f\"{path_work}/node_features.csv\", index=False, header=False)\n",
    "\n",
    "    # Make the Y file : \n",
    "y_file = [1] * len(edge_index)\n",
    "# Transform into tensor : \n",
    "y_tensor = torch.tensor(y_file , dtype=torch.float)\n",
    "# Write file : \n",
    "numpy_array = y_tensor.numpy()\n",
    "df = pd.DataFrame(numpy_array)\n",
    "df.to_csv(f\"{path_work}/y_file.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Data instance : \n",
    "\n",
    "edge_index = edge_index_tensor\n",
    "x = node_feature_tensor\n",
    "y = y_tensor\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index.t().contiguous(), y=y)\n",
    "\n",
    "# print out the data instance\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Data instance : \n",
    "\n",
    "class MultiDomainDataset(Dataset):\n",
    "    def __init__(self, final_all_data):\n",
    "        self.embeddings = torch.tensor(final_all_data[[int(i) for i in range(1, 1281)]].values, dtype=torch.float)\n",
    "        self.ancestor = torch.tensor(final_all_data[[f\"OHE_ancestor_{i}\" for i in range(labeled_ancestor_OHE.shape[1])]].values, dtype=torch.float)\n",
    "        self.prophage_instance = torch.tensor(final_all_data[[f\"OHE_phage_{i}\" for i in range(labeled_phage_OHE.shape[1])]].values, dtype=torch.float)\n",
    "        self.labels = torch.tensor(final_all_data[\"KL_type_LCA\"].values, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item_domain1 = self.embeddings[idx]\n",
    "        item_domain2 = self.ancestor[idx]\n",
    "        item_domain3 = self.prophage_instance[idx]\n",
    "        item_domain4 = self.labels[idx]\n",
    "        return item_domain1, item_domain2, item_domain3 , item_domain4\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_all_data, final_all_data[\"KL_type_LCA\"], test_size=0.25, random_state=42)\n",
    "\n",
    "train_singledata = MultiDomainDataset(X_train)\n",
    "test_singledata = MultiDomainDataset(X_test)\n",
    "\n",
    "train_loader = DataLoader(train_singledata, batch_size=12, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_singledata, batch_size=12, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 1280]) torch.Size([12, 2179]) torch.Size([12, 1625]) torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "for item_domain1, item_domain2, item_domain3, item_domain4 in train_loader:\n",
    "    print(item_domain1.shape, item_domain2.shape, item_domain3.shape, item_domain4.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0284,  0.0405, -0.0099,  ..., -0.0841,  0.0540,  0.1031]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor(69))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_singledata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# The model : \n",
    "\n",
    "class MultiBranchModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiBranchModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1280, hidden_size=512, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.lstm_branch = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512*2, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=256)\n",
    "        )\n",
    "        self.fnn_branch1 = nn.Sequential(\n",
    "            nn.Linear(2179, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fnn_branch2 = nn.Sequential(\n",
    "            nn.Linear(1625, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pre_classifier = nn.Sequential(\n",
    "            nn.Linear(256*3, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Linear(256, 127)  # as we concatenate the output from all three branches\n",
    "    def forward(self, embeddings, ancestor, prophage):\n",
    "        # unsqueeze embeddings to add sequence length dimension\n",
    "        embeddings = embeddings.unsqueeze(1)  # assuming embeddings have shape (batch_size, embedding_dim)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        lstm_out = lstm_out.squeeze(1)  # squeeze out the sequence length dimension\n",
    "        lstm_out = self.lstm_branch(lstm_out)  # Apply the remaining layers\n",
    "        fnn_out1 = self.fnn_branch1(ancestor)\n",
    "        fnn_out2 = self.fnn_branch2(prophage)\n",
    "        out = torch.cat((lstm_out, fnn_out1, fnn_out2), dim=1)\n",
    "        out = self.pre_classifier(out)\n",
    "        out = self.classifier(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Train / Eval : \n",
    "\n",
    "# Adding the weights : \n",
    "class_samples = np.bincount(all_data.KL_type_LCA)\n",
    "class_weights = 1. / torch.tensor(np.sqrt(class_samples), dtype=torch.float)\n",
    "# define loss function\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available\n",
    "\n",
    "model = MultiBranchModel()\n",
    "model = model.to(device)  # Move model to device\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Increase initial learning rate\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)  # Decrease learning rate every 30 epochs by 0.1\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_accuracy\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_accuracy\": [],\n",
    "    \"val_auprc\": {},  # Store AUPRC for each class\n",
    "    \"val_auroc\": {},  # Store AUROC for each class\n",
    "    \"confusion_matrix\": {},  # Store confusion matrix for each class\n",
    "}\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Switch model to training mode\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for i, (embeddings, ancestor, prophage, labels) in enumerate(train_loader):\n",
    "        embeddings = embeddings.to(device)\n",
    "        ancestor = ancestor.to(device)\n",
    "        prophage = prophage.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(embeddings, ancestor, prophage)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    epoch_loss /= len(train_loader)\n",
    "    history[\"train_loss\"].append(epoch_loss)\n",
    "    scheduler.step()\n",
    "\n",
    "    # Switch model to evaluation mode\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_predictions = []\n",
    "    val_true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for embeddings, ancestor, prophage, labels in test_loader:\n",
    "            embeddings = embeddings.to(device)\n",
    "            ancestor = ancestor.to(device)\n",
    "            prophage = prophage.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(embeddings, ancestor, prophage)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            val_predictions.append(outputs.cpu().numpy())\n",
    "            val_true_labels.append(labels.cpu().numpy())\n",
    "    val_loss /= len(val_loader)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    val_predictions = np.concatenate(val_predictions, axis=0)\n",
    "    val_true_labels = np.concatenate(val_true_labels, axis=0)\n",
    "    #auprc = average_precision_score(val_true_labels, val_predictions, average=\"macro\")  # Use macro-average to handle multiclass/multilabel\n",
    "    #history[\"val_auprc\"].append(auprc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Save model after training\n",
    "torch.save(model.state_dict(), f\"{path_work}/train_nn/MultiDomain.LSTM.model\")\n",
    "\n",
    "import json\n",
    "with open(f\"{path_work}/train_nn/MultiDomain.LSTM.model.out\" , \"w\") as outfile :\n",
    "    outfile.write(json.dumps(history))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "sklearn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
