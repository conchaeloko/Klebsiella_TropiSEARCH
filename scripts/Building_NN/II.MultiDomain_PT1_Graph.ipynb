{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal here is to correct the label before the finetuning of ESM2\n",
    "***\n",
    "## I. Generate the DF_info and the DF_embeddings \n",
    "## II. Tryout multiple algoritms : \n",
    "> NN : Graph Neural Networks  <br>\n",
    "***\n",
    "https://theaisummer.com/gnn-architectures/\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder , label_binarize , OneHotEncoder\n",
    "from sklearn.metrics import average_precision_score\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Open the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_work = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023\"\n",
    "path_work = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "\n",
    "# Open the DF\n",
    "DF_info = pd.read_csv(f\"{path_work}/DF_Dpo.final.2705.tsv\", sep = \"\\t\" ,  header = 0 )\n",
    "# Open the embeddings\n",
    "DF_embeddings = pd.read_csv(f\"{path_work}/Dpo.2705.embeddings.ultimate.csv\", sep = \",\", header= None )\n",
    "DF_embeddings.rename(columns={0: 'index'}, inplace=True)\n",
    "\n",
    "# Filter the DF :\n",
    "DF_info_filtered = DF_info[~DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "DF_info_ToReLabel = DF_info[DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "all_data = pd.merge(DF_info_filtered , DF_embeddings , on = \"index\")\n",
    "\n",
    "# Mind the over representation of outbreaks :\n",
    "all_data = all_data.drop_duplicates(subset = [\"Infected_ancestor\",\"index\",\"prophage_id\"] , keep = \"first\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1271</th>\n",
       "      <th>1272</th>\n",
       "      <th>1273</th>\n",
       "      <th>1274</th>\n",
       "      <th>1275</th>\n",
       "      <th>1276</th>\n",
       "      <th>1277</th>\n",
       "      <th>1278</th>\n",
       "      <th>1279</th>\n",
       "      <th>1280</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ppt__2930</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-0.017329</td>\n",
       "      <td>0.012884</td>\n",
       "      <td>0.037123</td>\n",
       "      <td>-0.123747</td>\n",
       "      <td>0.004186</td>\n",
       "      <td>-0.061367</td>\n",
       "      <td>-0.056718</td>\n",
       "      <td>-0.037215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098806</td>\n",
       "      <td>0.012989</td>\n",
       "      <td>-0.001155</td>\n",
       "      <td>0.139749</td>\n",
       "      <td>-0.030987</td>\n",
       "      <td>0.059306</td>\n",
       "      <td>0.107041</td>\n",
       "      <td>-0.041463</td>\n",
       "      <td>-0.085581</td>\n",
       "      <td>0.114973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ppt__3300</td>\n",
       "      <td>0.004044</td>\n",
       "      <td>0.040011</td>\n",
       "      <td>-0.001234</td>\n",
       "      <td>-0.095745</td>\n",
       "      <td>-0.058056</td>\n",
       "      <td>-0.002394</td>\n",
       "      <td>0.007648</td>\n",
       "      <td>-0.059740</td>\n",
       "      <td>0.060850</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020369</td>\n",
       "      <td>0.016287</td>\n",
       "      <td>0.062586</td>\n",
       "      <td>-0.024336</td>\n",
       "      <td>0.019276</td>\n",
       "      <td>0.069623</td>\n",
       "      <td>0.035261</td>\n",
       "      <td>-0.118962</td>\n",
       "      <td>0.035672</td>\n",
       "      <td>0.085582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ppt__1182</td>\n",
       "      <td>0.018767</td>\n",
       "      <td>0.068116</td>\n",
       "      <td>-0.009109</td>\n",
       "      <td>-0.012598</td>\n",
       "      <td>-0.107001</td>\n",
       "      <td>0.011569</td>\n",
       "      <td>-0.030943</td>\n",
       "      <td>-0.045359</td>\n",
       "      <td>0.048923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014524</td>\n",
       "      <td>-0.024645</td>\n",
       "      <td>0.071878</td>\n",
       "      <td>0.018206</td>\n",
       "      <td>0.042790</td>\n",
       "      <td>0.088410</td>\n",
       "      <td>0.031970</td>\n",
       "      <td>-0.124592</td>\n",
       "      <td>0.070040</td>\n",
       "      <td>0.065348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ppt__3540</td>\n",
       "      <td>-0.028261</td>\n",
       "      <td>-0.047253</td>\n",
       "      <td>-0.027340</td>\n",
       "      <td>-0.052824</td>\n",
       "      <td>-0.089644</td>\n",
       "      <td>-0.023079</td>\n",
       "      <td>0.094861</td>\n",
       "      <td>0.026104</td>\n",
       "      <td>0.024001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051728</td>\n",
       "      <td>0.005634</td>\n",
       "      <td>-0.077874</td>\n",
       "      <td>0.030336</td>\n",
       "      <td>-0.037648</td>\n",
       "      <td>0.050625</td>\n",
       "      <td>0.046142</td>\n",
       "      <td>-0.158841</td>\n",
       "      <td>-0.007670</td>\n",
       "      <td>0.034556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ppt__942</td>\n",
       "      <td>0.014863</td>\n",
       "      <td>0.028030</td>\n",
       "      <td>0.014927</td>\n",
       "      <td>-0.025997</td>\n",
       "      <td>-0.096138</td>\n",
       "      <td>0.016290</td>\n",
       "      <td>0.015008</td>\n",
       "      <td>-0.066254</td>\n",
       "      <td>0.077959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008521</td>\n",
       "      <td>-0.019820</td>\n",
       "      <td>0.123201</td>\n",
       "      <td>-0.040306</td>\n",
       "      <td>0.030893</td>\n",
       "      <td>0.051362</td>\n",
       "      <td>0.047316</td>\n",
       "      <td>-0.102698</td>\n",
       "      <td>0.044830</td>\n",
       "      <td>0.084530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3603</th>\n",
       "      <td>anubis__304</td>\n",
       "      <td>0.006264</td>\n",
       "      <td>0.006471</td>\n",
       "      <td>-0.031665</td>\n",
       "      <td>0.078502</td>\n",
       "      <td>-0.131247</td>\n",
       "      <td>0.077167</td>\n",
       "      <td>0.043005</td>\n",
       "      <td>-0.183636</td>\n",
       "      <td>-0.022181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044299</td>\n",
       "      <td>-0.061847</td>\n",
       "      <td>0.017696</td>\n",
       "      <td>0.054798</td>\n",
       "      <td>-0.035830</td>\n",
       "      <td>-0.030202</td>\n",
       "      <td>0.039051</td>\n",
       "      <td>-0.127020</td>\n",
       "      <td>-0.113630</td>\n",
       "      <td>0.211258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3604</th>\n",
       "      <td>anubis__1273</td>\n",
       "      <td>-0.019114</td>\n",
       "      <td>0.063302</td>\n",
       "      <td>0.006635</td>\n",
       "      <td>-0.060343</td>\n",
       "      <td>-0.034054</td>\n",
       "      <td>-0.003895</td>\n",
       "      <td>0.033920</td>\n",
       "      <td>-0.080352</td>\n",
       "      <td>0.073579</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004504</td>\n",
       "      <td>-0.007906</td>\n",
       "      <td>0.075141</td>\n",
       "      <td>-0.052423</td>\n",
       "      <td>0.027127</td>\n",
       "      <td>0.073984</td>\n",
       "      <td>0.030664</td>\n",
       "      <td>-0.096409</td>\n",
       "      <td>0.011906</td>\n",
       "      <td>0.124885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3605</th>\n",
       "      <td>anubis__1311</td>\n",
       "      <td>0.051261</td>\n",
       "      <td>0.067942</td>\n",
       "      <td>0.005061</td>\n",
       "      <td>-0.019131</td>\n",
       "      <td>-0.060296</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.037515</td>\n",
       "      <td>-0.033887</td>\n",
       "      <td>0.091774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044678</td>\n",
       "      <td>0.052609</td>\n",
       "      <td>0.112994</td>\n",
       "      <td>-0.000592</td>\n",
       "      <td>0.027122</td>\n",
       "      <td>0.086020</td>\n",
       "      <td>0.013660</td>\n",
       "      <td>-0.055491</td>\n",
       "      <td>0.021665</td>\n",
       "      <td>0.049301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3606</th>\n",
       "      <td>anubis__1525</td>\n",
       "      <td>-0.010655</td>\n",
       "      <td>0.083864</td>\n",
       "      <td>0.009084</td>\n",
       "      <td>-0.042220</td>\n",
       "      <td>-0.066479</td>\n",
       "      <td>0.008724</td>\n",
       "      <td>0.010109</td>\n",
       "      <td>-0.078033</td>\n",
       "      <td>0.065285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020752</td>\n",
       "      <td>0.024543</td>\n",
       "      <td>0.071302</td>\n",
       "      <td>0.035980</td>\n",
       "      <td>0.012171</td>\n",
       "      <td>0.054399</td>\n",
       "      <td>0.032167</td>\n",
       "      <td>-0.151018</td>\n",
       "      <td>0.042541</td>\n",
       "      <td>0.035221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3607</th>\n",
       "      <td>anubis__794</td>\n",
       "      <td>0.003410</td>\n",
       "      <td>0.035987</td>\n",
       "      <td>-0.006781</td>\n",
       "      <td>-0.006114</td>\n",
       "      <td>-0.096747</td>\n",
       "      <td>0.055312</td>\n",
       "      <td>0.020362</td>\n",
       "      <td>-0.106669</td>\n",
       "      <td>0.007063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040264</td>\n",
       "      <td>-0.011636</td>\n",
       "      <td>0.056235</td>\n",
       "      <td>0.041319</td>\n",
       "      <td>-0.008228</td>\n",
       "      <td>0.036652</td>\n",
       "      <td>0.048385</td>\n",
       "      <td>-0.104287</td>\n",
       "      <td>0.016014</td>\n",
       "      <td>0.068212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3608 rows × 1281 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             index         1         2         3         4         5  \\\n",
       "0        ppt__2930 -0.000061 -0.017329  0.012884  0.037123 -0.123747   \n",
       "1        ppt__3300  0.004044  0.040011 -0.001234 -0.095745 -0.058056   \n",
       "2        ppt__1182  0.018767  0.068116 -0.009109 -0.012598 -0.107001   \n",
       "3        ppt__3540 -0.028261 -0.047253 -0.027340 -0.052824 -0.089644   \n",
       "4         ppt__942  0.014863  0.028030  0.014927 -0.025997 -0.096138   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "3603   anubis__304  0.006264  0.006471 -0.031665  0.078502 -0.131247   \n",
       "3604  anubis__1273 -0.019114  0.063302  0.006635 -0.060343 -0.034054   \n",
       "3605  anubis__1311  0.051261  0.067942  0.005061 -0.019131 -0.060296   \n",
       "3606  anubis__1525 -0.010655  0.083864  0.009084 -0.042220 -0.066479   \n",
       "3607   anubis__794  0.003410  0.035987 -0.006781 -0.006114 -0.096747   \n",
       "\n",
       "             6         7         8         9  ...      1271      1272  \\\n",
       "0     0.004186 -0.061367 -0.056718 -0.037215  ...  0.098806  0.012989   \n",
       "1    -0.002394  0.007648 -0.059740  0.060850  ... -0.020369  0.016287   \n",
       "2     0.011569 -0.030943 -0.045359  0.048923  ...  0.014524 -0.024645   \n",
       "3    -0.023079  0.094861  0.026104  0.024001  ...  0.051728  0.005634   \n",
       "4     0.016290  0.015008 -0.066254  0.077959  ...  0.008521 -0.019820   \n",
       "...        ...       ...       ...       ...  ...       ...       ...   \n",
       "3603  0.077167  0.043005 -0.183636 -0.022181  ...  0.044299 -0.061847   \n",
       "3604 -0.003895  0.033920 -0.080352  0.073579  ... -0.004504 -0.007906   \n",
       "3605  0.000984  0.037515 -0.033887  0.091774  ...  0.044678  0.052609   \n",
       "3606  0.008724  0.010109 -0.078033  0.065285  ...  0.020752  0.024543   \n",
       "3607  0.055312  0.020362 -0.106669  0.007063  ...  0.040264 -0.011636   \n",
       "\n",
       "          1273      1274      1275      1276      1277      1278      1279  \\\n",
       "0    -0.001155  0.139749 -0.030987  0.059306  0.107041 -0.041463 -0.085581   \n",
       "1     0.062586 -0.024336  0.019276  0.069623  0.035261 -0.118962  0.035672   \n",
       "2     0.071878  0.018206  0.042790  0.088410  0.031970 -0.124592  0.070040   \n",
       "3    -0.077874  0.030336 -0.037648  0.050625  0.046142 -0.158841 -0.007670   \n",
       "4     0.123201 -0.040306  0.030893  0.051362  0.047316 -0.102698  0.044830   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3603  0.017696  0.054798 -0.035830 -0.030202  0.039051 -0.127020 -0.113630   \n",
       "3604  0.075141 -0.052423  0.027127  0.073984  0.030664 -0.096409  0.011906   \n",
       "3605  0.112994 -0.000592  0.027122  0.086020  0.013660 -0.055491  0.021665   \n",
       "3606  0.071302  0.035980  0.012171  0.054399  0.032167 -0.151018  0.042541   \n",
       "3607  0.056235  0.041319 -0.008228  0.036652  0.048385 -0.104287  0.016014   \n",
       "\n",
       "          1280  \n",
       "0     0.114973  \n",
       "1     0.085582  \n",
       "2     0.065348  \n",
       "3     0.034556  \n",
       "4     0.084530  \n",
       "...        ...  \n",
       "3603  0.211258  \n",
       "3604  0.124885  \n",
       "3605  0.049301  \n",
       "3606  0.035221  \n",
       "3607  0.068212  \n",
       "\n",
       "[3608 rows x 1281 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1280"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DF_embeddings[DF_embeddings[\"index\"] == \"ppt__942\"].values[0][1:1281])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Build the Graph Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Indexation process (shall add the N phages to predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexation = all_data[\"Infected_ancestor\"].unique().tolist() + all_data[\"Phage\"].unique().tolist() + all_data[\"index\"].unique().tolist() + [f\"Dpo_to_predict_{n}\" for n in DF_info[\"index\"].unique().tolist()]\n",
    "\n",
    "dico_ID = {item:index for index, item in enumerate(indexation)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Make edge file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = []\n",
    "\n",
    "# Node A (bacteria) - Node B1 (prophage) :\n",
    "for _, row in all_data.iterrows() :\n",
    "    edge_index.append([dico_ID[row[\"Phage\"]], dico_ID[row[\"Infected_ancestor\"]]])\n",
    "    \n",
    "# Node B1 - Node B2 (depolymerase) :\n",
    "for phage in all_data.Phage.unique() :\n",
    "    all_data_phage = all_data[all_data[\"Phage\"] == phage]\n",
    "    for _, row in all_data_phage.iterrows() :\n",
    "        edge_index.append([dico_ID[row[\"index\"]], dico_ID[row[\"Phage\"]]])\n",
    "\n",
    "# Transform into tensor : \n",
    "edge_index_tensor = torch.tensor(edge_index , dtype=torch.long)\n",
    "\n",
    "# Write file : \n",
    "numpy_array = edge_index_tensor.numpy()\n",
    "df = pd.DataFrame(numpy_array)\n",
    "df.to_csv(f\"{path_work}/edge_index.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Make the node feature file : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'KL1': 0,\n",
       " 'KL10': 1,\n",
       " 'KL101': 2,\n",
       " 'KL102': 3,\n",
       " 'KL103': 4,\n",
       " 'KL104': 5,\n",
       " 'KL105': 6,\n",
       " 'KL106': 7,\n",
       " 'KL107': 8,\n",
       " 'KL108': 9,\n",
       " 'KL109': 10,\n",
       " 'KL11': 11,\n",
       " 'KL110': 12,\n",
       " 'KL111': 13,\n",
       " 'KL112': 14,\n",
       " 'KL113': 15,\n",
       " 'KL114': 16,\n",
       " 'KL115': 17,\n",
       " 'KL116': 18,\n",
       " 'KL117': 19,\n",
       " 'KL118': 20,\n",
       " 'KL119': 21,\n",
       " 'KL12': 22,\n",
       " 'KL121': 23,\n",
       " 'KL122': 24,\n",
       " 'KL123': 25,\n",
       " 'KL124': 26,\n",
       " 'KL125': 27,\n",
       " 'KL126': 28,\n",
       " 'KL127': 29,\n",
       " 'KL128': 30,\n",
       " 'KL13': 31,\n",
       " 'KL130': 32,\n",
       " 'KL131': 33,\n",
       " 'KL132': 34,\n",
       " 'KL134': 35,\n",
       " 'KL136': 36,\n",
       " 'KL137': 37,\n",
       " 'KL139': 38,\n",
       " 'KL14': 39,\n",
       " 'KL140': 40,\n",
       " 'KL141': 41,\n",
       " 'KL142': 42,\n",
       " 'KL143': 43,\n",
       " 'KL144': 44,\n",
       " 'KL145': 45,\n",
       " 'KL146': 46,\n",
       " 'KL147': 47,\n",
       " 'KL148': 48,\n",
       " 'KL149': 49,\n",
       " 'KL15': 50,\n",
       " 'KL150': 51,\n",
       " 'KL151': 52,\n",
       " 'KL152': 53,\n",
       " 'KL153': 54,\n",
       " 'KL154': 55,\n",
       " 'KL155': 56,\n",
       " 'KL157': 57,\n",
       " 'KL158': 58,\n",
       " 'KL159': 59,\n",
       " 'KL16': 60,\n",
       " 'KL162': 61,\n",
       " 'KL163': 62,\n",
       " 'KL164': 63,\n",
       " 'KL166': 64,\n",
       " 'KL169': 65,\n",
       " 'KL17': 66,\n",
       " 'KL170': 67,\n",
       " 'KL18': 68,\n",
       " 'KL19': 69,\n",
       " 'KL2': 70,\n",
       " 'KL20': 71,\n",
       " 'KL21': 72,\n",
       " 'KL22': 73,\n",
       " 'KL23': 74,\n",
       " 'KL24': 75,\n",
       " 'KL25': 76,\n",
       " 'KL26': 77,\n",
       " 'KL27': 78,\n",
       " 'KL28': 79,\n",
       " 'KL29': 80,\n",
       " 'KL3': 81,\n",
       " 'KL30': 82,\n",
       " 'KL31': 83,\n",
       " 'KL33': 84,\n",
       " 'KL34': 85,\n",
       " 'KL35': 86,\n",
       " 'KL36': 87,\n",
       " 'KL37': 88,\n",
       " 'KL38': 89,\n",
       " 'KL39': 90,\n",
       " 'KL4': 91,\n",
       " 'KL40': 92,\n",
       " 'KL41': 93,\n",
       " 'KL42': 94,\n",
       " 'KL43': 95,\n",
       " 'KL45': 96,\n",
       " 'KL46': 97,\n",
       " 'KL47': 98,\n",
       " 'KL48': 99,\n",
       " 'KL49': 100,\n",
       " 'KL5': 101,\n",
       " 'KL51': 102,\n",
       " 'KL52': 103,\n",
       " 'KL53': 104,\n",
       " 'KL54': 105,\n",
       " 'KL55': 106,\n",
       " 'KL56': 107,\n",
       " 'KL57': 108,\n",
       " 'KL58': 109,\n",
       " 'KL59': 110,\n",
       " 'KL6': 111,\n",
       " 'KL60': 112,\n",
       " 'KL61': 113,\n",
       " 'KL62': 114,\n",
       " 'KL63': 115,\n",
       " 'KL64': 116,\n",
       " 'KL66': 117,\n",
       " 'KL67': 118,\n",
       " 'KL7': 119,\n",
       " 'KL70': 120,\n",
       " 'KL71': 121,\n",
       " 'KL74': 122,\n",
       " 'KL8': 123,\n",
       " 'KL81': 124,\n",
       " 'KL82': 125,\n",
       " 'KL9': 126}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LE  = LabelEncoder()\n",
    "di = LE.fit_transform(all_data[\"KL_type_LCA\"])\n",
    "label_mapping = dict(zip(LE.classes_, LE.transform(LE.classes_)))\n",
    "label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26766it [02:01, 219.57it/s]\n"
     ]
    }
   ],
   "source": [
    "node_feature = []\n",
    "\n",
    "for index, item in tqdm(enumerate(indexation)) :\n",
    "    features = [index]\n",
    "    if item in all_data[\"Infected_ancestor\"].unique() : \n",
    "        KL_type = all_data[all_data[\"Infected_ancestor\"] == item][\"KL_type_LCA\"].values[0]\n",
    "        features = features + [label_mapping[KL_type]] + [-1]*1280\n",
    "    elif item in all_data[\"Phage\"].unique() : \n",
    "        features = features + [-1]*1281\n",
    "    elif item in all_data[\"index\"].unique() : \n",
    "        features = features + [-1] + DF_embeddings[DF_embeddings[\"index\"] == item].values[0][1:1281].tolist()\n",
    "    elif item in [f\"Dpo_to_predict_{n}\" for n in DF_info[\"index\"].unique().tolist()] : \n",
    "        features = features + [-1]*1281\n",
    "    node_feature.append(features)\n",
    "    \n",
    "# Transform into tensor : \n",
    "node_feature_tensor = torch.tensor(node_feature , dtype=torch.float)\n",
    "\n",
    "# Write file : \n",
    "numpy_array = node_feature_tensor.numpy()\n",
    "df = pd.DataFrame(numpy_array)\n",
    "df.to_csv(f\"{path_work}/node_features.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Make the Y file : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_file = [1] * len(edge_index)\n",
    "\n",
    "# Transform into tensor : \n",
    "y_tensor = torch.tensor(y_file , dtype=torch.float)\n",
    "\n",
    "# Write file : \n",
    "numpy_array = y_tensor.numpy()\n",
    "df = pd.DataFrame(numpy_array)\n",
    "df.to_csv(f\"{path_work}/y_file.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Create the Data instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[26766, 1282], edge_index=[2, 19354], y=[19354])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data , DataLoader\n",
    "\n",
    "edge_index = edge_index_tensor\n",
    "x = node_feature_tensor\n",
    "y = y_tensor\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index.t().contiguous(), y=y)\n",
    "\n",
    "# print out the data instance\n",
    "print(data)\n",
    "\n",
    "data.validate(raise_on_error=True)\n",
    "data.is_undirected()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_88422/2055416614.py\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdata_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_networkx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_undirected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/torch_geometric/data/data.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;31m# can accept key: Union[str, TensorAttr] in __getitem__.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_store\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/torch_geometric/data/storage.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_graph(G, color):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False,\n",
    "                     node_color=color, cmap=\"Set2\")\n",
    "    plt.show()\n",
    "    \n",
    "data_sub = data[:50]\n",
    "    \n",
    "G = to_networkx(data_sub, to_undirected=False)\n",
    "visualize_graph(G, color=data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Transductive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "class Classifier(torch.nn.Module):\n",
    "    def forward(self, x, edge_index):\n",
    "        edge_feat_start = x[edge_index[0]]\n",
    "        edge_feat_end = x[edge_index[1]]\n",
    "        return (edge_feat_start * edge_feat_end).sum(dim=-1)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.gnn = GNN(in_channels, hidden_channels)\n",
    "        self.classifier = Classifier()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.gnn(data.x, data.edge_index)\n",
    "        pred = self.classifier(x, data.edge_index)\n",
    "        return pred\n",
    "\n",
    "model = Model(in_channels=1280, hidden_channels=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, ReLU\n",
    "from torch_geometric.nn import Sequential, GCNConv\n",
    "\n",
    "model = Sequential('x, edge_index', [\n",
    "    (GCNConv(in_channels, 64), 'x, edge_index -> x'),\n",
    "    ReLU(inplace=True),\n",
    "    (GCNConv(64, 64), 'x, edge_index -> x'),\n",
    "    ReLU(inplace=True),\n",
    "    Linear(64, out_channels),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Without negative loss sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a DataLoader and loss function\n",
    "data_loader = DataLoader(data_list, batch_size=32)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(100):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        loss = criterion(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With negative loss sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "# Define a DataLoader and loss function\n",
    "data_loader = DataLoader(data_list, batch_size=32)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# Define an optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        # Compute embeddings for nodes in batch\n",
    "        node_embeddings = model(batch)\n",
    "        # Compute positive score\n",
    "        pos_edge_index = batch.edge_index\n",
    "        pos_out = model.classifier(node_embeddings[pos_edge_index[0]], node_embeddings[pos_edge_index[1]])\n",
    "        # Generate negative edges and compute negative score\n",
    "        num_nodes = node_embeddings.size(0)\n",
    "        neg_edge_index = negative_sampling(edge_index=pos_edge_index, num_nodes=num_nodes, num_neg_samples=pos_edge_index.size(1))\n",
    "        neg_out = model.classifier(node_embeddings[neg_edge_index[0]], node_embeddings[neg_edge_index[1]])\n",
    "        # Compute the loss\n",
    "        pos_loss = criterion(pos_out, batch.y)\n",
    "        neg_loss = criterion(neg_out, torch.zeros(neg_out.size()).to(device))\n",
    "        loss = pos_loss + neg_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from sklearn.metrics import f1_score, precision_score, roc_auc_score\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    # Negative sampling for each batch\n",
    "    neg_edge_index = negative_sampling(edge_index, num_nodes=data.num_nodes)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Get the model's predictions for positive and negative samples\n",
    "    pos_out = model(data.x, edge_index)\n",
    "    neg_out = model(data.x, neg_edge_index)\n",
    "\n",
    "    pos_loss = F.binary_cross_entropy_with_logits(pos_out, torch.ones(pos_out.shape))\n",
    "    neg_loss = F.binary_cross_entropy_with_logits(neg_out, torch.zeros(neg_out.shape))\n",
    "\n",
    "    loss = pos_loss + neg_loss\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    # Returning the loss values for potential logging\n",
    "    return pos_loss.item(), neg_loss.item()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    # Negative sampling for the full dataset\n",
    "    neg_edge_index = negative_sampling(edge_index, num_nodes=data.num_nodes)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get the model's predictions for positive and negative samples\n",
    "        pos_out = model(data.x, edge_index)\n",
    "        neg_out = model(data.x, neg_edge_index)\n",
    "\n",
    "        pos_target = torch.ones(pos_out.shape[0])\n",
    "        neg_target = torch.zeros(neg_out.shape[0])\n",
    "\n",
    "        # Concatenate the predictions and their respective targets\n",
    "        predictions = torch.cat([pos_out, neg_out]).numpy()\n",
    "        targets = torch.cat([pos_target, neg_target]).numpy()\n",
    "\n",
    "        # Calculate the metrics\n",
    "        f1 = f1_score(targets, predictions.round())\n",
    "        precision = precision_score(targets, predictions.round())\n",
    "        auroc = roc_auc_score(targets, predictions)\n",
    "\n",
    "    return f1, precision, auroc\n",
    "\n",
    "# Validation strategy: split the edges and their respective y into train and validation sets.\n",
    "edge_index_train, edge_index_val, y_train, y_val = train_test_split(edge_index.T, y, test_size=0.2)\n",
    "edge_index_train = edge_index_train.T\n",
    "edge_index_val = edge_index_val.T\n",
    "\n",
    "# To monitor overfitting, you can compare the performance on the validation set.\n",
    "best_val_auroc = 0\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    pos_loss, neg_loss = train()\n",
    "    val_f1, val_precision, val_auroc = test(edge_index_val, y_val)\n",
    "\n",
    "    # Save the model if it has the best validation performance so far\n",
    "    if val_auroc > best_val_auroc:\n",
    "        best_val_auroc = val_auroc\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Pos loss: {pos_loss}, Neg loss: {neg_loss}, Val F1: {val_f1}, Val Precision: {val_precision}, Val AUROC: {val_auroc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The GNN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GCN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import NegativeSamplingLoss\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        # create edge embeddings by taking the dot product of the node embeddings\n",
    "        start, end = edge_index\n",
    "        edge_embeddings = torch.einsum(\"ef,ef->e\", x[start], x[end])\n",
    "\n",
    "        return edge_embeddings\n",
    "\n",
    "# Create an instance of the model\n",
    "model = Net(num_node_features=data.num_node_features, hidden_channels=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GAT layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=8, dropout=0.6)\n",
    "        self.conv2 = GATConv(8*hidden_channels, hidden_channels, heads=8, concat=False, dropout=0.6)\n",
    "        self.lin1 = torch.nn.Linear(2*hidden_channels, hidden_channels)\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        edge_attr = data.edge_attr\n",
    "\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        # Get the embeddings of the start and end nodes of each edge\n",
    "        start, end = edge_index[:, edge_attr==1]\n",
    "        edge_embeddings = torch.cat([x[start], x[end]], dim=1)\n",
    "\n",
    "        # Pass through a couple fully connected layers\n",
    "        edge_embeddings = F.elu(self.lin1(edge_embeddings))\n",
    "        edge_scores = torch.sigmoid(self.lin2(edge_embeddings))\n",
    "\n",
    "        return edge_scores.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Method Somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
    "        # Define a 2-layer GNN computation graph.\n",
    "        # Use a *single* `ReLU` non-linearity in-between.\n",
    "        # TODO:\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Our final classifier applies the dot-product between source and destination\n",
    "# node embeddings to derive edge-level predictions:\n",
    "class Classifier(torch.nn.Module):\n",
    "    def forward(self, x_user: Tensor, x_movie: Tensor, edge_label_index: Tensor) -> Tensor:\n",
    "        # Convert node embeddings to edge-level representations:\n",
    "        edge_feat_user = x_user[edge_label_index[0]]\n",
    "        edge_feat_movie = x_movie[edge_label_index[1]]\n",
    "\n",
    "        # Apply dot-product to get a prediction per supervision edge:\n",
    "        return (edge_feat_user * edge_feat_movie).sum(dim=-1)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        # Since the dataset does not come with rich features, we also learn two\n",
    "        # embedding matrices for users and movies:\n",
    "        self.movie_lin = torch.nn.Linear(20, hidden_channels)\n",
    "        self.user_emb = torch.nn.Embedding(data[\"user\"].num_nodes, hidden_channels)\n",
    "        self.movie_emb = torch.nn.Embedding(data[\"movie\"].num_nodes, hidden_channels)\n",
    "\n",
    "        # Instantiate homogeneous GNN:\n",
    "        self.gnn = GNN(hidden_channels)\n",
    "\n",
    "        # Convert GNN model into a heterogeneous variant:\n",
    "        self.gnn = to_hetero(self.gnn, metadata=data.metadata())\n",
    "\n",
    "        self.classifier = Classifier()\n",
    "\n",
    "    def forward(self, data: HeteroData) -> Tensor:\n",
    "        x_dict = {\n",
    "          \"user\": self.user_emb(data[\"user\"].node_id),\n",
    "          \"movie\": self.movie_lin(data[\"movie\"].x) + self.movie_emb(data[\"movie\"].node_id),\n",
    "        } \n",
    "\n",
    "        # `x_dict` holds feature matrices of all node types\n",
    "        # `edge_index_dict` holds all edge indices of all edge types\n",
    "        x_dict = self.gnn(x_dict, data.edge_index_dict)\n",
    "\n",
    "        pred = self.classifier(\n",
    "            x_dict[\"user\"],\n",
    "            x_dict[\"movie\"],\n",
    "            data[\"user\", \"rates\", \"movie\"].edge_label_index,\n",
    "        )\n",
    "\n",
    "        return pred\n",
    "\n",
    "        \n",
    "model = Model(hidden_channels=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool\n",
    "from torch_geometric.nn import NegativeSamplingLoss\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.loss_fn = NegativeSamplingLoss()\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=2, dropout=0.6)\n",
    "        self.conv2 = GATConv(2*hidden_channels, hidden_channels, heads=2, dropout=0.6)\n",
    "        self.conv3 = GATConv(2*hidden_channels, hidden_channels, heads=2, concat=False, dropout=0.6)\n",
    "        self.loss_fn = NegativeSamplingLoss()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "import numpy as np\n",
    "\n",
    "# Initialize your model\n",
    "model = GAT(num_node_features, hidden_channels)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = BCEWithLogitsLoss()\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in data_loader:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        # Compute scores for positive pairs\n",
    "        pos_out = torch.sum(out[batch.edge_index[0]] * out[batch.edge_index[1]], dim=-1)\n",
    "        # Negative sampling: for each src node in edge_index, we sample one\n",
    "        # negative target node and add it to the edge_index tensor.\n",
    "        neg_edge_index = negative_sampling(batch.edge_index, num_nodes=batch.num_nodes)\n",
    "        neg_out = torch.sum(out[neg_edge_index[0]] * out[neg_edge_index[1]], dim=-1)\n",
    "        # The final output tensor should have this form: [pos1, neg1, pos2, neg2, pos3, neg3, ...]\n",
    "        # where pos{i} is the score for the i-th positive pair, and neg{i} is the score for the i-th negative pair.\n",
    "        total_out = torch.stack([pos_out, neg_out], dim=1).view(-1)\n",
    "        # The target tensor should have this form: [1, 0, 1, 0, 1, 0, ...]\n",
    "        total_target = torch.tensor(np.repeat([1, 0], pos_out.shape[0]), dtype=torch.float)\n",
    "        # Compute loss and backpropagate\n",
    "        loss = criterion(total_out, total_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch: {:03d}, Loss: {:.4f}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### The training : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "# Prepare the graph data\n",
    "x = ...  # Node features tensor (shape: [num_nodes, num_features])\n",
    "edge_index = ...  # Edge indices tensor (shape: [2, num_edges])\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Create positive and negative edge samples\n",
    "pos_edge_index = data.edge_index\n",
    "neg_edge_index = negative_sampling(edge_index, num_nodes=data.num_nodes, num_neg_samples=data.num_edges)\n",
    "\n",
    "# Train the model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    node_embeddings = model(data.x, data.edge_index)\n",
    "\n",
    "    pos_edge_preds = (node_embeddings[pos_edge_index[0]] * node_embeddings[pos_edge_index[1]]).sum(dim=1)\n",
    "    neg_edge_preds = (node_embeddings[neg_edge_index[0]] * node_embeddings[neg_edge_index[1]]).sum(dim=1)\n",
    "\n",
    "    edge_preds = torch.cat([pos_edge_preds, neg_edge_preds], dim=0)\n",
    "    edge_labels = torch.cat([torch.ones(pos_edge_index.size(1)), torch.zeros(neg_edge_index.size(1))], dim=0)\n",
    "\n",
    "    loss = criterion(edge_preds, edge_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Predict edge probabilities\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    node_embeddings = model(data.x, data.edge_index)\n",
    "    edge_probs = torch.sigmoid((node_embeddings[edge_index[0]] * node_embeddings[edge_index[1]]).sum(dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GAT(in_channels, hidden_channels).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "train_dataset = ...\n",
    "val_dataset = ...\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.binary_cross_entropy(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_dataset)\n",
    "\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in val_loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(data)\n",
    "        loss = F.binary_cross_entropy(pred, data.y)\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(val_dataset)\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_loss = validate()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Loss: {val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Transductive predictions : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def predict():\n",
    "    model.eval()\n",
    "\n",
    "    data = full_dataset.to(device)  # The whole graph\n",
    "    with torch.no_grad():\n",
    "        preds = model(data)  # predictions for all edges\n",
    "\n",
    "    # Get predictions only for the desired edges\n",
    "    desired_edge_preds = preds[data.y == -1] \n",
    "\n",
    "    return desired_edge_preds\n",
    "\n",
    "# After training the model\n",
    "predictions = predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "A1: Inductive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "# Define your model and optimizer\n",
    "model = GCN(num_node_features=1280, num_classes=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Create DataLoader instances for your training and test datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):  # 100 epochs\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = torch.nn.functional.cross_entropy(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "model.eval()\n",
    "for data in test_loader:\n",
    "    with torch.no_grad():\n",
    "        predictions = model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "A2: Transductive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = GCN(num_node_features=1280, num_classes=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(100):  # 100 epochs\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = torch.nn.functional.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(data)\n",
    "unlabeled_predictions = predictions[data.test_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle()\n",
    "train_dataset = dataset[:540]\n",
    "test_dataset = dataset[540:]\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "    # Maybe would be nice to \n",
    "# We also need to make sure to add the reverse edges from movies to users\n",
    "# in order to let a GNN be able to pass messages in both directions.\n",
    "# We can leverage the `T.ToUndirected()` transform for this from PyG:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "sklearn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
