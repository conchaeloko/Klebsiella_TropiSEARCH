{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData, DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, to_hetero , SAGEConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder , label_binarize , OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import random\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Load the Dataframes :\n",
    "path_work = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023\"\n",
    "graph_data = torch.load(f'{path_work}/train_nn/graph_file.1107.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# The model : Linear Classifier\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels , conv=SAGEConv):\n",
    "        super().__init__()\n",
    "        self.conv = conv(-1, hidden_channels, aggr='mean')  # Use 'mean' aggregation\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv(x, edge_index)  # GNN layer (feature transformation)\n",
    "        return x\n",
    "        \n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.lin1 = torch.nn.Linear(2 * hidden_channels, hidden_channels)\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, features_A, features_B1, graph_data):\n",
    "        index_B1 , index_A = graph_data[\"B1\", \"infects\", \"A\"].edge_label_index\n",
    "        z = torch.cat([features_B1[index_B1] ,features_A[index_A]], dim=-1)  # Can you explain why this line gives me an error \n",
    "        z = self.lin1(z).relu()\n",
    "        z = self.lin2(z)\n",
    "        return z.view(-1)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, out_channels , conv=SAGEConv):\n",
    "        super().__init__()\n",
    "        self.gnn_B2_B1 = GNN(out_channels, conv)  # Use GNN instead of conv\n",
    "        self.gnn_B1_A = GNN(out_channels, conv)  # Use GNN instead of conv\n",
    "        self.decode = EdgeDecoder(out_channels)\n",
    "        \n",
    "    def forward(self, graph_data):\n",
    "        # Propagate B2 features to B1 : prop 1 \n",
    "        prop_1_x = graph_data.x_dict[\"B1\"]\n",
    "        prop_1_edge = graph_data.edge_index_dict[('B2','expressed','B1')]\n",
    "        features_B1_updated = self.gnn_B2_B1(prop_1_x, prop_1_edge)  # Added edge_index_B2_B1        \n",
    "        # Propagate new B1 features to A : prop 2 \n",
    "        prop_2_edge = graph_data.edge_index_dict[('B1','infects','A')]  # Fixed the variable name from sampled_data to graph_data\n",
    "        features_A_updated = self.gnn_B1_A(features_B1_updated, prop_2_edge)  # Added edge_index_B1_A\n",
    "        return self.decode(features_A_updated , features_B1_updated, graph_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12661/3247577671.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m transform = T.RandomLinkSplit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mnum_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnum_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#disjoint_train_ratio=...,  # TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mneg_sampling_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'T' is not defined"
     ]
    }
   ],
   "source": [
    "# *****************************************************************************\n",
    "# Pre-process data :\n",
    "transform = T.RandomLinkSplit(\n",
    "    num_val=0.1, \n",
    "    num_test=0.2, \n",
    "    #disjoint_train_ratio=...,  \n",
    "    neg_sampling_ratio=1.0,  \n",
    "    add_negative_train_samples=True, \n",
    "    edge_types=(\"B1\", \"infects\", \"A\"),\n",
    "    rev_edge_types=(\"A\", \"harbors\", \"B1\"), \n",
    ")\n",
    "\n",
    "train_data, val_data, test_data = transform(graph_data)\n",
    "\n",
    "train_loader = LinkNeighborLoader(\n",
    "    data=train_data,  \n",
    "    num_neighbors= [-1],  \n",
    "    edge_label_index=((\"B1\", \"infects\", \"A\"), train_data[\"B1\", \"infects\", \"A\"].edge_label_index),\n",
    "    edge_label=train_data[\"B1\", \"infects\", \"A\"].edge_label,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = LinkNeighborLoader(\n",
    "    data=val_data,  \n",
    "    num_neighbors= [-1],  \n",
    "    edge_label_index=((\"B1\", \"infects\", \"A\"), val_data[\"B1\", \"infects\", \"A\"].edge_label_index),\n",
    "    edge_label=val_data[\"B1\", \"infects\", \"A\"].edge_label,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = LinkNeighborLoader(\n",
    "    data=test_data,  \n",
    "    num_neighbors= [-1],  \n",
    "    edge_label_index=((\"B1\", \"infects\", \"A\"), test_data[\"B1\", \"infects\", \"A\"].edge_label_index),\n",
    "    edge_label=test_data[\"B1\", \"infects\", \"A\"].edge_label,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Training :\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available\n",
    "\n",
    "def train(model, loader, optimizer, criterion, edge_type):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        edge_labels = data[edge_type].edge_label   \n",
    "        loss = criterion(out, edge_labels)  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "    \n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, edge_type):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []  # Collect output probabilities for AUC\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        edge_labels = data[edge_type].edge_label\n",
    "        val_loss = criterion(out, edge_labels)\n",
    "        total_loss += val_loss.item()\n",
    "        probs = torch.sigmoid(out)  # Convert output to probabilities\n",
    "        pred_class = probs.round()  # Round to nearest integer to get class predictions\n",
    "        all_preds.extend(pred_class.cpu().numpy())\n",
    "        all_labels.extend(edge_labels.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())  # Collect output probabilities\n",
    "    # Calculate the metrics\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "    precision = precision_score(all_labels, all_preds, average='binary')\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)  # Use probabilities, not class predictions\n",
    "    return total_loss / len(loader), f1, precision, accuracy, auc\n",
    "\n",
    "def main():\n",
    "    hidden_channels = 60 \n",
    "    model = Model(hidden_channels).to(device)\n",
    "    # Due to lazy initialization, we need to run one model step so the number\n",
    "    # of parameters can be inferred:\n",
    "    eg_gratia_data = next(iter(val_loader))\n",
    "    with torch.no_grad():\n",
    "        model(eg_gratia_data)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    edge_type = (\"B1\", \"infects\", \"A\")  \n",
    "    for epoch in range(100): \n",
    "        train_loss = train(model, train_loader, optimizer, criterion, edge_type)\n",
    "        if epoch % 10 == 0:\n",
    "            test_loss, f1, precision, accuracy, auc = evaluate(model, test_loader, criterion, edge_type)\n",
    "            print(f'Epoch: {epoch}, Train Loss: {train_loss}, Test Loss: {test_loss}, F1 Score: {f1}, Precision: {precision}, Accuracy: {accuracy}, AUC: {auc}')\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), f\"{path_work}/SAGEConv.model.1307.pt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Whole graph at once : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, edge_type):\n",
    "    model.train()\n",
    "    data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    edge_labels = data[edge_type].edge_label\n",
    "    loss = criterion(out, edge_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data, criterion, edge_type):\n",
    "    model.eval()\n",
    "    data = data.to(device)\n",
    "    out = model(data)\n",
    "    edge_labels = data[edge_type].edge_label\n",
    "    val_loss = criterion(out, edge_labels)\n",
    "    probs = torch.sigmoid(out)  \n",
    "    pred_class = probs.round()  \n",
    "    all_preds = pred_class.cpu().numpy()\n",
    "    all_labels = edge_labels.cpu().numpy()\n",
    "    all_probs = probs.cpu().numpy()\n",
    "    # Calculate the metrics\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "    precision = precision_score(all_labels, all_preds, average='binary')\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    return val_loss.item(), f1, precision, accuracy, auc\n",
    "\n",
    "def main():\n",
    "    hidden_channels = 60 \n",
    "    model = Model(hidden_channels).to(device)\n",
    "    model(data)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    edge_type = (\"B1\", \"infects\", \"A\")  \n",
    "    for epoch in range(500): \n",
    "        train_loss = train(model, train_data, optimizer, criterion, edge_type)\n",
    "        if epoch % 10 == 0:\n",
    "            test_loss, f1, precision, accuracy, auc = evaluate(model, test_data, criterion, edge_type)\n",
    "            print(f'Epoch: {epoch}, Train Loss: {train_loss}, Test Loss: {test_loss}, F1 Score: {f1}, Precision: {precision}, Accuracy: {accuracy}, AUC: {auc}')\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), f\"{path_work}/SAGEConv.model.single_batch.1307.pt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#BATCH --job-name=GCNConv__\n",
    "#SBATCH --qos=short\n",
    "#SBATCH --ntasks=1 \n",
    "#SBATCH --cpus-per-task=10 \n",
    "#SBATCH --mem=100gb \n",
    "#SBATCH --time=1-00:00:00 \n",
    "#SBATCH --output=GCNConv__%j.log \n",
    "\n",
    "source /storage/apps/ANACONDA/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate torch_geometric\n",
    "\n",
    "python /home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023/train_nn/script_files/GCNConv_Hetero.dot.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "for fold, (train_idx, test_idx) in enumerate(kfold.split(graph_data.node_items['A'], graph_data.y)):\n",
    "    train_data = graph_data[train_idx]\n",
    "    test_data = graph_data[test_idx]\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=32)\n",
    "    for epoch in range(100):  # adjust as needed\n",
    "        train_loss = train(model, train_loader, optimizer, criterion)\n",
    "        print(f\"Fold: {fold+1}, Epoch: {epoch+1}, Train loss: {train_loss}\")\n",
    "    acc, prec, rec, f1, auroc = test(model, test_loader)\n",
    "    print(f\"Fold: {fold+1}, Accuracy: {acc}, Precision: {prec}, Recall: {rec}, F1-score: {f1}, AUROC: {auroc}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geometric",
   "language": "python",
   "name": "torch_geometric"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
