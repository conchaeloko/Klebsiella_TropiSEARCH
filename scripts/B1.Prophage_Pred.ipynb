{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **II. Prophages**<br>\n",
    "### The goal is to predict the prophages and to identify the strains in which they are present\n",
    "## 1. Prophage prediction \n",
    "## 2. FastANI process\n",
    "## 3. Inspecting FastANI output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 1. Prophage prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The prediction command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophage annotation : \n",
    "#***********************************************************  \n",
    "from os import system, listdir, chdir, mkdir\n",
    "from os.path import isdir\n",
    "import os\n",
    "import random                                   \n",
    "path_klebsiella=\"/home/conchae/prediction_depolymerase_tropism\"\n",
    "path_phageboost=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/phageboost/phageboost_prediction\"\n",
    "\n",
    "good_strains=open(f\"{path_klebsiella}/panacota_pangenome/panacota_pangenome_list.txt\").read().split(\"\\n\")\n",
    "\n",
    "for specie in os.listdir(path_klebsiella):\n",
    "    if specie[0]==\"k\" and os.path.isdir(f\"{path_klebsiella}/{specie}\")== True:\n",
    "        for strain in random.sample(os.listdir(f\"{path_klebsiella}/{specie}/refseq/bacteria\"), len(os.listdir(f\"{path_klebsiella}/{specie}/refseq/bacteria\"))):\n",
    "            if strain in good_strains :\n",
    "                path_fna=f\"{path_klebsiella}/{specie}/refseq/bacteria/{strain}/prokka_annotation_all/{strain}.fna\"\n",
    "                path_prophage=f\"{path_phageboost}/{strain}\"\n",
    "                try :\n",
    "                    mkdir(path_prophage)\n",
    "                except FileExistsError :\n",
    "                    print(\"The output for phageboost already exists for some reason. We shall continue\")\n",
    "                if len(os.listdir(f\"{path_prophage}\")) == 0:\n",
    "                    system(f\"PhageBoost -f {path_fna} -o {path_prophage}  --threads 4\")\n",
    "                    with open(f\"{path_prophage}/process_done\",\"w\") as outfile:\n",
    "                        outfile.write(\"This strain has been studied\")\n",
    "                    \n",
    "#!/bin/bash\n",
    "#BATCH --job-name=PhageBoost_cmd\n",
    "#SBATCH --partition=medium \n",
    "#SBATCH --ntasks=1 \n",
    "#SBATCH --cpus-per-task=40\n",
    "#SBATCH --mem=125gb \n",
    "#SBATCH --time=7-00:00:00 \n",
    "#SBATCH --output=PhageBoost_cmd%j.log \n",
    "\n",
    "\n",
    "source /storage/apps/ANACONDA/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate PhageBoost-env\n",
    "\n",
    "python3 /home/conchae/prediction_depolymerase_tropism/script_files/prophage_prediction/phageboost_script.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Getting the prediction score for each prophage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Writting some info files ...\n",
    "from os import system, listdir, chdir, mkdir\n",
    "from os.path import isdir\n",
    "import os\n",
    "\n",
    "path_phageboost=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/phageboost\"\n",
    "\n",
    "with open(f\"{path_phageboost}/score_distribution.phageboost.csv\",\"w\") as outfile:\n",
    "    for strain in os.listdir(f\"{path_phageboost}/phageboost_prediction\") :\n",
    "        if len(os.listdir(f\"{path_phageboost}/phageboost_prediction/{strain}\")) > 2 :\n",
    "            for file in os.listdir(f\"{path_phageboost}/phageboost_prediction/{strain}\"):\n",
    "                if file[0:6]==\"phages\":\n",
    "                    info_file=open(f\"{path_phageboost}/phageboost_prediction/{strain}/{file}\").read().split(\"\\n\")[2:]\n",
    "                    for index_info, info in enumerate(info_file):\n",
    "                        if info :\n",
    "                            score=info.split(\"\\t\")[5]\n",
    "                            outfile.write(f\"{strain},{score}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2. FastANI computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "> The actual command "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******************************************************************************************************************************************\n",
    "# The fastANI command :\n",
    "# *******************************************************************************************************************************************\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "path_phageboot_info=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_decipher/phageboost/phageboost_info\"\n",
    "path_phageboost_pred=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_decipher/phageboost/phageboost_prediction\"\n",
    "path_fastANI_2=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022\"\n",
    "path_ktype=\"/home/conchae/prediction_depolymerase_tropism\"\n",
    "\n",
    "\n",
    "    # But first, the strain_ktype dictionary :\n",
    "strain_ktype={}\n",
    "good_strain=open(f\"{path_ktype}/results_kleborate_count.tsv\").read().split(\"\\n\")\n",
    "for index_strain, info in enumerate(good_strain):\n",
    "    if info:\n",
    "        strain=info.split(\"\\t\")[0].strip()\n",
    "        ktype=info.split(\"\\t\")[2].strip()\n",
    "        strain_ktype[strain]=ktype\n",
    "\n",
    "with open(f\"{path_phageboot_info}/results_phageboost.70.20102022.tsv\",\"w\") as outfile1 :\n",
    "    outfile1.write(f\"Prophage_name\\tProphage_length\\tN_genes\\tScore\\tK_type\\n\")\n",
    "    for strain in tqdm(os.listdir(path_phageboost_pred)):\n",
    "        # Opening the resume file of phageboost prediction :\n",
    "        for file in os.listdir(f\"{path_phageboost_pred}/{strain}\"):\n",
    "            if file[0:6]==\"phages\":\n",
    "                try :\n",
    "                    resume= pd.read_csv(f\"{path_phageboost_pred}/{strain}/{file}\", skiprows=1, sep=\"\\t\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Seems like there is no prophage for \")\n",
    "                #Scanning the file for phage with a score > 0.70\n",
    "                for index_info, info in resume.iterrows():\n",
    "                    if float(info[\"score\"])>= 0.70 :\n",
    "                        # Getting the prophage info :\n",
    "                        prophage_id= info[\"attributes\"].split(\"phage_id=\")[1]\n",
    "                        prophage_len= int(info[\"start\"]) -int(info[\"end\"])\n",
    "                        n_genes= info[\"attributes\"].split(\"n_genes=\")[1].split(\";\")[0]\n",
    "                        for file2 in os.listdir(f\"{path_phageboost_pred}/{strain}\"):\n",
    "                            if file2.count(prophage_id)>0:\n",
    "                                seq=open(f\"{path_phageboost_pred}/{strain}/{file2}\").read().split(\"\\n\")[1]\n",
    "                                if os.path.isfile(f\"{path_fastANI_2}/{strain}__{prophage_id}.fasta\")==False:\n",
    "                                    with open(f\"{path_fastANI_2}/{strain}__{prophage_id}.fasta\",\"w\") as outfile :\n",
    "                                        outfile.write(f\">{strain}__{prophage_id}\\n{seq}\")\n",
    "                        outfile1.write(f\"{strain}__{prophage_id}\\t{str(prophage_len)}\\t{n_genes}\\t{info['score']}\\t{strain_ktype[strain]}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#*****************************************************************************************************************************************\n",
    "#!/bin/bash\n",
    "#BATCH --job-name=writting_phb\n",
    "#SBATCH --partition=small \n",
    "#SBATCH --ntasks=1 \n",
    "#SBATCH --cpus-per-task=5\n",
    "#SBATCH --mem=15gb \n",
    "#SBATCH --time=0-05:00:00 \n",
    "#SBATCH --output=writting_phb%j.log \n",
    "\n",
    "source /storage/apps/ANACONDA/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate ScaleAP\n",
    "\n",
    "python3 /home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/script_files/py_files/prepare_fastani.20102022.py\n",
    "\n",
    "#*****************************************************************************************************************************************        \n",
    "# Step 2 :\n",
    "# Writting the path file :\n",
    "\n",
    "with open(f\"{path_phageboot_info}/fastANI_list.20102022.tsv\",\"w\") as outfile :\n",
    "    for file in tqdm(os.listdir(path_fastANI_2)):\n",
    "        outfile.write(f\"{path_fastANI_2}/{file}\\n\")\n",
    "        \n",
    "        \n",
    "#*****************************************************************************************************************************************    \n",
    "# Step 3 :        \n",
    "# fatANI commands : \n",
    "\n",
    "#!/bin/bash\n",
    "#BATCH --job-name=fatANI_phb\n",
    "#SBATCH --partition=medium \n",
    "#SBATCH --ntasks=1 \n",
    "#SBATCH --cpus-per-task=40\n",
    "#SBATCH --mem=200gb \n",
    "#SBATCH --time=4-00:00:00 \n",
    "#SBATCH --output=fatANI_phb%j.log \n",
    "\n",
    "module restore la_base\n",
    "source /storage/apps/ANACONDA/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate fastani\n",
    "\n",
    "fastANI  --ql /home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_decipher/phageboost/phageboost_info/fastANI_list.20102022.tsv --rl /home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_decipher/phageboost/phageboost_info/fastANI_list.20102022.tsv -o /home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_out_20102022  --matrix  -t 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3. Inspecting FastANI output \n",
    "\n",
    "> First round inspection : Get the pairs of prophages with a ANI score>0.99 and coverage > 80%\n",
    "\n",
    "Output format. In all above use cases, OUTPUT_FILE will contain tab delimited row(s) with query genome, reference genome, ANI value, count of bidirectional fragment mappings, and total query fragments. Alignment fraction (wrt. the query genome) is simply the ratio of mappings and total fragments.\n",
    "(https://github.com/ParBLiSS/FastANI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******************************************************************************************************************************************\n",
    "# Inspecting the fastANI outputs :\n",
    "# *******************************************************************************************************************************************\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "path_fastani=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022_out\"\n",
    "path_ktype=\"/home/conchae/prediction_depolymerase_tropism\"\n",
    "\n",
    "# Generating the dico with the k_type info for each strain\n",
    "strain_ktype={}\n",
    "good_strain=open(f\"{path_ktype}/results_kleborate_count.tsv\").read().split(\"\\n\")\n",
    "for index_strain, info in enumerate(good_strain):\n",
    "    if info:\n",
    "        strain=info.split(\"\\t\")[0].strip()\n",
    "        ktype=info.split(\"\\t\")[2].strip()\n",
    "        strain_ktype[strain]=ktype\n",
    "    \n",
    "fastani_names = [\"Query\",\"Reference_genome\",\"ANI\",\"fragments\",\"total_fragments\"]\n",
    "fastani_df = pd.read_csv(f\"{path_fastani}/fastANI_out_20102022\",sep=\"\\t\", names = fastani_names, nrows=10000)\n",
    "\n",
    "families = []\n",
    "fastani_dict = fastani_df.to_dict('records')\n",
    "for row in tqdm(fastani_dict) :\n",
    "    if float(row[\"ANI\"]) >=99 and float(row[\"fragments\"])/float(row[\"total_fragments\"])>=0.80:\n",
    "        l_query = len(open(f\"{row['Query']}\").read().split(\"\\n\")[1]) \n",
    "        l_refer = len(open(f\"{row['Reference_genome']}\").read().split(\"\\n\")[1])\n",
    "        if l_query /l_refer >= 0.8 and l_query /l_refer <= 1.25 : \n",
    "                prophage_1 = row[\"Query\"].split(\"/\")[-1]\n",
    "                prophage_2 = row[\"Reference_genome\"].split(\"/\")[-1]\n",
    "                pair = [prophage_1, prophage_2]\n",
    "                for cluster in families :\n",
    "                    if prophage_1 in cluster or prophage_2 in cluster: \n",
    "                        cluster.add(prophage_1)\n",
    "                        cluster.add(prophage_2)\n",
    "                        break\n",
    "                else :\n",
    "                    cluster = set()\n",
    "                    cluster.add(prophage_1)\n",
    "                    cluster.add(prophage_2)\n",
    "                    families.append(cluster)\n",
    "\n",
    "with open(f\"{path_fastani}/clusters_99_80.info.tsv\",'w') as outfile :\n",
    "    with open(f\"{path_fastani}/clusters_99_80.tsv\",'w') as outfile_cluster :\n",
    "        outfile.write(\"Family_index\\tMember\\n\")\n",
    "        outfile_cluster.write(\"Family_index\\tMembers\\n\")\n",
    "        for index_c, cluster in enumerate(families) :\n",
    "            outfile_cluster.write(f\"{index_c}\\t\")\n",
    "            cluster_c_l = []\n",
    "            for member in cluster :\n",
    "                outfile.write(f\"family {index_c}\\t{member}\\n\")\n",
    "                cluster_c_l.append(member)\n",
    "            outfile_cluster.write(\",\".join(cluster_c_l))\n",
    "            outfile_cluster.write(\"\\n\")\n",
    "\n",
    "# *******************************************************************************************************************************************                    \n",
    "#!/bin/bash\n",
    "#BATCH --job-name=post_ANI2_\n",
    "#SBATCH --partition=short \n",
    "#SBATCH --ntasks=1 \n",
    "#SBATCH --cpus-per-task=10\n",
    "#SBATCH --mem=10gb \n",
    "#SBATCH --time=1-00:00:00 \n",
    "#SBATCH --output=post_ANI2_%j.log \n",
    "\n",
    "source /storage/apps/ANACONDA/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate ScaleAP\n",
    "\n",
    "python3 /home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/script_files/py_files/post_fastani.20102022.py\n",
    "# *******************************************************************************************************************************************                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******************************************************************************************************************************************\n",
    "# Inspecting the fastANI outputs : Approach 2 (20/04/2023)\n",
    "# *******************************************************************************************************************************************\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "path_fastani=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022_out\"\n",
    "path_ktype=\"/home/conchae/prediction_depolymerase_tropism\"\n",
    "\n",
    "# Generating the dico with the k_type info for each strain\n",
    "strain_ktype={}\n",
    "good_strain=open(f\"{path_ktype}/results_kleborate_count.tsv\").read().split(\"\\n\")\n",
    "for index_strain, info in enumerate(good_strain):\n",
    "    if info:\n",
    "        strain=info.split(\"\\t\")[0].strip()\n",
    "        ktype=info.split(\"\\t\")[2].strip()\n",
    "        strain_ktype[strain]=ktype\n",
    "    \n",
    "fastani_names = [\"Query\",\"Reference_genome\",\"ANI\",\"fragments\",\"total_fragments\"]\n",
    "fastani_df = pd.read_csv(f\"{path_fastani}/fastANI_out_20102022\",sep=\"\\t\", names = fastani_names , nrows = 1000)\n",
    "fastani_df = fastani_df[fastani_df[\"ANI\"] >= 90]\n",
    "\n",
    "families = []\n",
    "fastani_dict = fastani_df.to_dict('records')\n",
    "for row in tqdm(fastani_dict) :\n",
    "    if float(row[\"ANI\"]) >=99 and float(row[\"fragments\"])/float(row[\"total_fragments\"])>=0.80:\n",
    "        l_query = len(open(f\"{row['Query']}\").read().split(\"\\n\")[1]) \n",
    "        l_refer = len(open(f\"{row['Reference_genome']}\").read().split(\"\\n\")[1])\n",
    "        case_1 = (l_query > l_refer and l_query /l_refer >= 0.8)\n",
    "        case_2 = (l_refer > l_query and l_refer /l_query >= 0.8)\n",
    "        #if l_query /l_refer >= 0.8 and l_query /l_refer <= 1.25 : \n",
    "        if case_1 or case_2 :\n",
    "        #if l_query /l_refer >= 0.8 and l_query /l_refer <= 1.25 : \n",
    "            prophage_1 = row[\"Query\"].split(\"/\")[-1]\n",
    "            prophage_2 = row[\"Reference_genome\"].split(\"/\")[-1]\n",
    "            pair = {prophage_1, prophage_2}\n",
    "            for cluster in families :\n",
    "                if cluster.isdisjoint(pair) == False :\n",
    "                    cluster.update(pair)\n",
    "                    break\n",
    "            else :\n",
    "                families.append(pair)\n",
    "\n",
    "\n",
    "with open(f\"{path_fastani}/clusters_99_80.info.2004.v2.tsv\",'w') as outfile :\n",
    "    with open(f\"{path_fastani}/clusters_99_80.2004.v2.tsv\",'w') as outfile_cluster :\n",
    "        outfile.write(\"Family_index\\tMember\\n\")\n",
    "        outfile_cluster.write(\"Family_index\\tMembers\\n\")\n",
    "        for index_c, cluster in enumerate(families) :\n",
    "            outfile_cluster.write(f\"{index_c}\\t\")\n",
    "            cluster_c_l = []\n",
    "            for member in cluster :\n",
    "                outfile.write(f\"family {index_c}\\t{member}\\n\")\n",
    "                cluster_c_l.append(member)\n",
    "            outfile_cluster.write(\",\".join(cluster_c_l))\n",
    "            outfile_cluster.write(\"\\n\")\n",
    "\n",
    "# *******************************************************************************************************************************************                    \n",
    "#!/bin/bash\n",
    "#BATCH --job-name=post_ANI2_\n",
    "#SBATCH --qos=short \n",
    "#SBATCH --ntasks=1 \n",
    "#SBATCH --cpus-per-task=10\n",
    "#SBATCH --mem=10gb \n",
    "#SBATCH --time=1-00:00:00 \n",
    "#SBATCH --output=post_ANI2_%j.log \n",
    "\n",
    "source /storage/apps/ANACONDA/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate ScaleAP\n",
    "\n",
    "python3 /home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/script_files/py_files/cluster.I.2004.py\n",
    "# *******************************************************************************************************************************************              \n",
    "# Check the integrety of the DF :\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "path_fastani=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022_out\"\n",
    "path_phages = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022\"\n",
    "path_ktype=\"/home/conchae/prediction_depolymerase_tropism\"\n",
    "\n",
    "families = pd.read_csv(f\"{path_fastani}/clusters_99_80.2004.v2.tsv\", header = 0, sep='\\t')\n",
    "families_set = [set(fam.split(\",\")) for fam in families[\"Members\"]]\n",
    "\n",
    "n_iteration = 7\n",
    "clean_families = []\n",
    "tmp_families = families_set.copy()\n",
    "# fill up the tmp clusters\n",
    "for n in range(n_iteration):\n",
    "    for index_set, cluster in tqdm(enumerate(tmp_families)) :\n",
    "        for index_2, cluster_2 in enumerate(families_set):\n",
    "            if cluster.isdisjoint(cluster_2) == False :\n",
    "                cluster.update(cluster_2)\n",
    "                continue\n",
    "            else :\n",
    "                continue\n",
    "\n",
    "# Gather the clean clusters :\n",
    "for index, cluster in enumerate(tmp_families) :\n",
    "    if cluster not in clean_families :\n",
    "        clean_families.append(cluster)\n",
    "# *******************************************************************************************************************************************************************\n",
    "# Write the final files :\n",
    "with open(f\"{path_fastani}/clusters_99_80.clean.2004.v2.tsv\",\"w\") as outfile :\n",
    "    outfile.write(\"Family_index\\tMembers\\n\")\n",
    "    phages = set()\n",
    "    L = len(clean_families)\n",
    "    loners = []\n",
    "    for index_f, family in tqdm(enumerate(clean_families_2)) :\n",
    "        cluster_list = \",\".join(list(family))\n",
    "        outfile.write(f\"Family_{index_f}\\t{cluster_list}\\n\")\n",
    "    for phage in tqdm(os.listdir(path_phages)):\n",
    "        for index_f, family in enumerate(clean_families_2):\n",
    "            if phage in family :\n",
    "                break\n",
    "        else :\n",
    "            loners.append(phage)\n",
    "    for index, phage in enumerate(loners) :        \n",
    "        outfile.write(f\"Loner_{str(L+index)}\\t{phage}\\n\")    \n",
    "\n",
    "# *******************************************************************************************************************************************************************\n",
    "# Check the integrity of the files :\n",
    "cluster = pd.read_csv(f\"{path_fastani}/clusters_99_80.clean.v2.tsv\", header = 0, sep=\"\\t\")\n",
    "phages = []\n",
    "cluster_dict = cluster.to_dict(\"records\")\n",
    "\n",
    "for row in tqdm(cluster_dict) :\n",
    "    for member in row[\"Members\"].split(\",\") :\n",
    "        phages.append(member)\n",
    "        \n",
    "loners_df = cluster[cluster[\"Family_index\"]==\"Loner\"]\n",
    "fammmm_df = cluster[cluster[\"Family_index\"]!=\"Loner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "path_fastani = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022_out\"\n",
    "path_phages = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022\"\n",
    "path_ktype=\"/home/conchae/prediction_depolymerase_tropism\"\n",
    "\n",
    "cluster = pd.read_csv(f\"{path_fastani}/clusters_99_80.clean.2004.v2.tsv\", header = 0, sep=\"\\t\")\n",
    "\n",
    "\n",
    "with open(f\"{path_fastani}/clusters_99_80.extra_clean.2004.v2.tsv\",\"w\") as outfile :\n",
    "    outfile.write(f\"prophage_id\\tprophage\\n\")\n",
    "    for index,row in tqdm(cluster.iterrows()) :\n",
    "        for member in row[\"Members\"].split(\",\") :\n",
    "            outfile.write(f\"prophage_{index}\\t{member}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fix the families "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the integrety of the DF :\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "path_fastani=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022_out\"\n",
    "path_phages = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022\"\n",
    "path_ktype=\"/home/conchae/prediction_depolymerase_tropism\"\n",
    "\n",
    "families = pd.read_csv(f\"{path_fastani}/clusters_99_80.tsv\", header = 0, sep='\\t')\n",
    "families_set = [set(fam.split(\",\")) for fam in families[\"Members\"]]\n",
    "\n",
    "n_iteration = 10\n",
    "clean_families = []\n",
    "tmp_families = families_set.copy()\n",
    "# fill up the tmp clusters\n",
    "for n in range(n_iteration):\n",
    "    for index_set, cluster in tqdm(enumerate(tmp_families)) :\n",
    "        for index_2, cluster_2 in enumerate(families_set):\n",
    "            if cluster.isdisjoint(cluster_2) == False :\n",
    "                cluster.update(cluster_2)\n",
    "                continue\n",
    "            else :\n",
    "                continue\n",
    "    print(f\"Iteration number {n}\")\n",
    "\n",
    "# Gather the clean clusters :\n",
    "for index, cluster in enumerate(tmp_families) :\n",
    "    if cluster not in clean_families :\n",
    "        clean_families.append(cluster)\n",
    "        \n",
    "        \n",
    "        \n",
    "clean_families = []\n",
    "for index_set, cluster in tqdm(enumerate(families_set)) :\n",
    "    clean_cluster = cluster.copy()\n",
    "    #print(clean_cluster)\n",
    "    for index_2, cluster_2 in enumerate(families_set):\n",
    "        if clean_cluster.isdisjoint(cluster_2) == False :\n",
    "            clean_cluster.update(cluster_2)\n",
    "            continue\n",
    "        else :\n",
    "            continue\n",
    "    #print(clean_cluster)\n",
    "    if clean_cluster not in clean_families :\n",
    "        clean_families.append(clean_cluster)\n",
    "        \n",
    "\n",
    "# Repeat the iteration : \n",
    "clean_families_2 = []\n",
    "for index_set, cluster in tqdm(enumerate(clean_families)) :\n",
    "    clean_cluster = cluster.copy()\n",
    "    for index_2, cluster_2 in enumerate(clean_families):\n",
    "        if clean_cluster.isdisjoint(cluster_2) == False :\n",
    "            clean_cluster.update(cluster_2)\n",
    "            continue\n",
    "        else :\n",
    "            continue\n",
    "    if clean_cluster not in clean_families_2 :\n",
    "        clean_families_2.append(clean_cluster)\n",
    "\n",
    "# *******************************************************************************************************************************************************************\n",
    "n_iteration = 10\n",
    "clean_families = []\n",
    "for n in range(n_iteration):\n",
    "    tmp_families = []\n",
    "    for index_set, cluster in tqdm(enumerate(clean_families)) :\n",
    "        clean_cluster = cluster.copy()\n",
    "        #print(clean_cluster)\n",
    "        for index_2, cluster_2 in enumerate(families_set):\n",
    "            if clean_cluster.isdisjoint(cluster_2) == False :\n",
    "                clean_cluster.update(cluster_2)\n",
    "                continue\n",
    "            else :\n",
    "                continue\n",
    "        #print(clean_cluster)\n",
    "        if clean_cluster not in clean_families :\n",
    "            clean_families.append(clean_cluster)\n",
    "    \n",
    "    \n",
    "\n",
    "# *******************************************************************************************************************************************************************\n",
    "with open(f\"{path_fastani}/clusters_99_80.clean.tsv\",\"w\") as outfile :\n",
    "    outfile.write(\"Family_index\\tMembers\\n\")\n",
    "    phages = set()\n",
    "    L = len(clean_families)\n",
    "    loners = []\n",
    "    for index_f, family in tqdm(enumerate(clean_families_2)) :\n",
    "        cluster_list = \",\".join(list(family))\n",
    "        outfile.write(f\"Family_{index_f}\\t{cluster_list}\\n\")\n",
    "    for phage in tqdm(os.listdir(path_phages)):\n",
    "        for index_f, family in enumerate(clean_families_2):\n",
    "            if phage in family :\n",
    "                break\n",
    "        else :\n",
    "            loners.append(phage)\n",
    "    for index, phage in enumerate(loners) :        \n",
    "        outfile.write(f\"Loner_{str(L+index)}\\t{phage}\\n\")\n",
    "# *******************************************************************************************************************************************************************\n",
    "#!/bin/bash\n",
    "#BATCH --job-name=post_ANI2_\n",
    "#SBATCH --qos=short \n",
    "#SBATCH --ntasks=1 \n",
    "#SBATCH --cpus-per-task=10\n",
    "#SBATCH --mem=10gb \n",
    "#SBATCH --time=1-00:00:00 \n",
    "#SBATCH --output=post_ANI2_%j.log \n",
    "\n",
    "source /storage/apps/ANACONDA/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate ScaleAP\n",
    "\n",
    "python3 /home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/script_files/py_files/cluster.II.old.py\n",
    "#*******************************************************************************************************************************************************************\n",
    "\n",
    "            \n",
    "# Check the integrity of the files :\n",
    "cluster = pd.read_csv(f\"{path_fastani}/clusters_99_80.clean.tsv\", header = 0, sep=\"\\t\")\n",
    "phages = []\n",
    "cluster_dict = cluster.to_dict(\"records\")\n",
    "\n",
    "for row in tqdm(cluster_dict) :\n",
    "    for member in row[\"Members\"].split(\",\") :\n",
    "        phages.append(member)\n",
    "        \n",
    "loners_df = cluster[cluster[\"Family_index\"]==\"Loner\"]\n",
    "fammmm_df = cluster[cluster[\"Family_index\"]!=\"Loner\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Generate the matrix \n",
    "### With prophages as row, bacterial strains as column. \"0\" ; \"1\" for presence absence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "path_fastani=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022_out\"\n",
    "path_phages = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022\"\n",
    "path_ktype=\"/home/conchae/prediction_depolymerase_tropism\"\n",
    "\n",
    "cluster_df = pd.read_csv(f\"{path_fastani}/clusters_99_80.clean.2004.v2.tsv\", header = 0, sep=\"\\t\")\n",
    "\n",
    "dico_strains = {}\n",
    "for prophage, row in cluster_df.iterrows():\n",
    "    strains = [seq.split(\"__\")[0] for seq in row[\"Members\"].split(\",\")]\n",
    "    for _,strain in enumerate(strains) :\n",
    "        if strain not in dico_strains :\n",
    "            tmp_set = set()\n",
    "            tmp_set.add(f\"prophage_{prophage}\")\n",
    "            dico_strains[strain] = tmp_set\n",
    "        else :\n",
    "            dico_strains[strain].add(f\"prophage_{prophage}\")\n",
    "            \n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(width = 200)\n",
    "\n",
    "pp.pprint(dico_strains)            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique prophages (column names)\n",
    "prophages = set()\n",
    "for key in dico_strains:\n",
    "    prophages.update(dico_strains[key])\n",
    "    \n",
    "    \n",
    "# Create a binary matrix\n",
    "prophages = sorted(list(prophages))\n",
    "binary_matrix = []\n",
    "for prophage in prophages:\n",
    "    row = [0] * len(dico_strains)\n",
    "    for i, key in enumerate(dico_strains):\n",
    "        if prophage in dico_strains[key]:\n",
    "            row[i] = 1\n",
    "    binary_matrix.append(row)\n",
    "\n",
    "# Create a DataFrame with the binary matrix\n",
    "df_strain = pd.DataFrame(binary_matrix, index=prophages, columns=dico_strains.keys())\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Add the protein information (100% sequence identity only) :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The working one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "path_fastani=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022_out\"\n",
    "path_phages = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022\"\n",
    "path_ktype=\"/home/conchae/prediction_depolymerase_tropism\"\n",
    "\n",
    "path_db = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/15122022_session\"\n",
    "\n",
    "cluster_df = pd.read_csv(f\"{path_fastani}/clusters_99_80.clean.2004.v2.tsv\", header = 0, sep=\"\\t\")\n",
    "desirable_df = pd.read_csv(f\"{path_db}/Results_III_DataFrame.v3.final.desired_ipr.csv\", sep=\"\\t\", header= 0)\n",
    "sub_df = desirable_df.drop_duplicates(subset = [\"protein_name\", \"index_seq\"])\n",
    "\n",
    "depolymerase_dico = {}\n",
    "prophage_done = {}\n",
    "for _,row in tqdm(sub_df.iterrows()) :\n",
    "    prophage_name =  \"__\".join(row[\"protein_name\"].split(\"__\")[0:2])\n",
    "    if prophage_name not in prophage_done :\n",
    "        for prophage_indice, cluster in cluster_df.iterrows() :\n",
    "            if prophage_name in [member.split(\".fasta\")[0] for member in cluster[\"Members\"].split(\",\")] :\n",
    "                #print(\"Here he is ! \")\n",
    "                prophage_fam = f\"prophage_{prophage_indice}\"\n",
    "                depo_name = f\"DepoSeq_{row['index_seq']}\"\n",
    "                print(prophage_fam , depo_name)\n",
    "                if depo_name not in depolymerase_dico :\n",
    "                    tmp_set = set()\n",
    "                    tmp_set.add(prophage_fam)\n",
    "                    depolymerase_dico[depo_name] = tmp_set\n",
    "                else :\n",
    "                    depolymerase_dico[depo_name].add(prophage_fam)\n",
    "                break\n",
    "    continue\n",
    "        \n",
    "for key in depolymerase_dico:\n",
    "    depolymerase_dico[key] = list(depolymerase_dico[key])\n",
    "\n",
    "with open(f\"{path_db}/depo_prophageFAM.dico.json\", \"w\") as outfile:\n",
    "    json.dump(prophage_dico, outfile)\n",
    "    \n",
    "# ********************************************************************************************\n",
    "#!/bin/bash\n",
    "#BATCH --job-name=write_dico__\n",
    "#SBATCH --qos=short\n",
    "#SBATCH --ntasks=1 \n",
    "#SBATCH --cpus-per-task=5 \n",
    "#SBATCH --mem=5gb \n",
    "#SBATCH --time=1-00:00:00 \n",
    "#SBATCH --output=write_dico__%j.log \n",
    "\n",
    "source /storage/apps/ANACONDA/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate ScaleAP\n",
    "\n",
    "python /home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/15122022_session/script_files/part_III/generate_df.pt1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#path_db = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/15122022_session\"\n",
    "\n",
    "#results_df = pd.read_csv(f\"{path_db}/Results_III_DataFrame.v3.final.csv\", sep=\"\\t\", header= 0)\n",
    "\n",
    "#undesirable = [\"IPR023346\",\"IPR002152\",\"IPR007781\",\"IPR000974\"]\n",
    "#undesired_id = set()\n",
    "#for ipr in undesirable :\n",
    "#    undesired_df = results_df[results_df[\"IPR_entry\"].str.split(\".\").str[0] == ipr]\n",
    "#    for _, row in undesired_df.iterrows() :\n",
    "#        undesired_id.add(row[\"index_seq\"])\n",
    "\n",
    "#desirable_df = results_df[~results_df[\"index_seq\"].isin(undesired_id)]\n",
    "#desirable_df.to_csv(f\"{path_db}/Results_III_DataFrame.v3.final.desired_ipr.csv\", sep=\"\\t\",header =  [\"protein_name\",\"KL_type_ancestor\",\"ancestor_id\",\"IPR_entry\",\"score\",\"qstart\",\"qstop\",\"sequence\",\"index_seq\"], index = False )\n",
    "\n",
    "#****\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "path_fastani=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022_out\"\n",
    "path_phages = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022\"\n",
    "path_ktype=\"/home/conchae/prediction_depolymerase_tropism\"\n",
    "\n",
    "path_db = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/15122022_session\"\n",
    "\n",
    "cluster_df = pd.read_csv(f\"{path_fastani}/clusters_99_80.clean.2004.v2.tsv\", header = 0, sep=\"\\t\")\n",
    "desirable_df = pd.read_csv(f\"{path_db}/Results_III_DataFrame.v3.final.desired_ipr.csv\", sep=\"\\t\", header= 0)\n",
    "sub_df = desirable_df.drop_duplicates(subset = [\"protein_name\", \"index_seq\"])\n",
    "\n",
    "prophage_dico = {}\n",
    "for _,row in tqdm(sub_df.iterrows()) :\n",
    "    for prophage, cluster in cluster_df.iterrows() :\n",
    "        if \"__\".join(row[\"protein_name\"].split(\"__\")[0:2]) in [member.split(\".fasta\")[0] for member in cluster[\"Members\"].split(\",\")] :\n",
    "            prophage_name = f\"prophage_{prophage}\"\n",
    "            if prophage_name not in prophage_dico :\n",
    "                tmp_set = set()\n",
    "                tmp_set.add(f\"DepoSeq_{row['index_seq']}\")\n",
    "                prophage_dico[prophage_name] = tmp_set\n",
    "            else :\n",
    "                prophage_dico[prophage_name].add(f\"DepoSeq_{row['index_seq']}\")\n",
    "            break\n",
    "        else :\n",
    "            continue\n",
    "\n",
    "for key in prophage_dico:\n",
    "    prophage_dico[key] = list(prophage_dico[key])\n",
    "\n",
    "with open(f\"{path_db}/prophage_with_RBP.dico.json\", \"w\") as outfile:\n",
    "    json.dump(prophage_dico, outfile)\n",
    "    \n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(width = 200)\n",
    "\n",
    "pp.pprint(prophage_dico)  \n",
    "\n",
    "# ********************************************************************************************\n",
    "#!/bin/bash\n",
    "#BATCH --job-name=write_dico__\n",
    "#SBATCH --qos=short\n",
    "#SBATCH --ntasks=1 \n",
    "#SBATCH --cpus-per-task=5 \n",
    "#SBATCH --mem=5gb \n",
    "#SBATCH --time=1-00:00:00 \n",
    "#SBATCH --output=phrogs_PPT__%j.log \n",
    "\n",
    "source /storage/apps/ANACONDA/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate ScaleAP\n",
    "\n",
    "python /home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/script_files/py_files/write_dico.rbp_pro.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Add the information from the mini batch : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "path_fastani=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022_out\"\n",
    "path_phages = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022\"\n",
    "path_ktype=\"/home/conchae/prediction_depolymerase_tropism\"\n",
    "\n",
    "path_db = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/15122022_session\"\n",
    "\n",
    "cluster_df = pd.read_csv(f\"{path_fastani}/clusters_99_80.clean.2004.v2.tsv\", header = 0, sep=\"\\t\")\n",
    "desirable_df = pd.read_csv(f\"{path_db}/Results_III_DataFrame.v3.final.desired_ipr.csv\", sep=\"\\t\", header= 0)\n",
    "sub_df = desirable_df.drop_duplicates(subset = [\"protein_name\", \"index_seq\"])\n",
    "\n",
    "#depolymerase_dico = json.load(open(f\"{path_db}/depo_prophageFAM.dico.json\"))\n",
    "minibatch_dico = json.load(open(f\"{path_db}/minibatchdepo_prophageFAM.dico.json\"))\n",
    "\n",
    "dico_strains = {}\n",
    "for prophage, row in cluster_df.iterrows():\n",
    "    strains = [seq.split(\"__\")[0] for seq in row[\"Members\"].split(\",\")]\n",
    "    for _,strain in enumerate(strains) :\n",
    "        if strain not in dico_strains :\n",
    "            tmp_set = set()\n",
    "            tmp_set.add(f\"prophage_{prophage}\")\n",
    "            dico_strains[strain] = tmp_set\n",
    "        else :\n",
    "            dico_strains[strain].add(f\"prophage_{prophage}\")\n",
    "            \n",
    "            \n",
    "prophages = set()\n",
    "for key in dico_strains:\n",
    "    prophages.update(dico_strains[key])\n",
    "\n",
    "# Create a binary matrix\n",
    "prophages = sorted(list(prophages))\n",
    "binary_matrix = []\n",
    "for prophage in prophages:\n",
    "    row = [0] * len(dico_strains)\n",
    "    for i, key in enumerate(dico_strains):\n",
    "        if prophage in dico_strains[key]:\n",
    "            row[i] = 1\n",
    "    binary_matrix.append(row)\n",
    "\n",
    "# Create a DataFrame with the binary matrix\n",
    "df_strain = pd.DataFrame(binary_matrix, index=prophages, columns=dico_strains.keys())\n",
    "\n",
    "# ***********************************************************************************************************\n",
    "# regular depo matrix :\n",
    "# Create a binary matrix\n",
    "prophages = sorted(list(prophages))\n",
    "binary_matrix = []\n",
    "for prophage in prophages:\n",
    "    row = [0] * len(depolymerase_dico)\n",
    "    for i, key in enumerate(depolymerase_dico):\n",
    "        if prophage in depolymerase_dico[key]:\n",
    "            row[i] = 1\n",
    "    binary_matrix.append(row)\n",
    "# Create a DataFrame with the binary matrix\n",
    "df_depo = pd.DataFrame(binary_matrix, index=prophages, columns=depolymerase_dico.keys())\n",
    "\n",
    "# ***********************************************************************************************************\n",
    "# Minibatch matrix : \n",
    "# Create a binary matrix\n",
    "prophages = sorted(list(prophages))\n",
    "binary_matrix = []\n",
    "for prophage in prophages:\n",
    "    row = [0] * len(minibatch_dico)\n",
    "    for i, key in enumerate(minibatch_dico):\n",
    "        if prophage in minibatch_dico[key]:\n",
    "            row[i] = 1\n",
    "    binary_matrix.append(row)\n",
    "# Create a DataFrame with the binary matrix\n",
    "df_minibatch = pd.DataFrame(binary_matrix, index=prophages, columns=minibatch_dico.keys())\n",
    "final_df = pd.concat([df_strain, df_minibatch], axis = 1)\n",
    "\n",
    "\n",
    "# The final dataframe :\n",
    "final_df = pd.concat([df_strain, df_depo, df_minibatch], axis = 1)\n",
    "final_df.to_csv(f\"{path_db}/prophages.strain_depo.matrix.csv\", header = 0 , sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#BATCH --job-name=write_dico__\n",
    "#SBATCH --qos=short\n",
    "#SBATCH --ntasks=1 \n",
    "#SBATCH --cpus-per-task=5 \n",
    "#SBATCH --mem=5gb \n",
    "#SBATCH --time=1-00:00:00 \n",
    "#SBATCH --output=write_dico__%j.log \n",
    "\n",
    "source /storage/apps/ANACONDA/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate ScaleAP\n",
    "\n",
    "python /home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/15122022_session/script_files/part_III/write_Rafa_df.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Save the dico "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/15122022_session/script_files/part_III\"\n",
    "\n",
    "import pandas as pd \n",
    "import os \n",
    "\n",
    "file_out = [line for line in open(f\"{path_out}/write_dico__26469.log\").read().split(\"\\n\") if line if line[0]==\"p\"]\n",
    "to_df = []\n",
    "for line in file_out :\n",
    "    to_df.append(line.split())\n",
    "    \n",
    "\n",
    "file_out_df = pd.DataFrame(to_df, columns = [\"prophage\",\"depo\"])\n",
    "file_out_df.to_csv(f\"{path_out}/depo_prophageFAM.dico.saved.json\", sep = \"\\t\", index=False)\n",
    "\n",
    "\n",
    "file_out_df = pd.read_csv(f\"{path_out}/depo_prophageFAM.dico.saved.json\", sep = \"\\t\", names = [\"prophage\",\"depo\"], )\n",
    "\n",
    "depolymerase_dico = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "> Create directory phageboost with the new prophage name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre step : \n",
    "# Create a tmp with all the candidates :\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "path_phageboost_pred=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_decipher/phageboost/phageboost_prediction\"\n",
    "path_fasta=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022\"\n",
    "path_fastani=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_out_20102022\"\n",
    "path_decipher = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_decipher/phageboost/phageboost_70_20102022\"\n",
    "path_ktype=\"/home/conchae/prediction_depolymerase_tropism\"\n",
    "\n",
    "\n",
    "# Generating the dico with the k_type info for each strain\n",
    "strain_ktype={}\n",
    "good_strain=open(f\"{path_ktype}/results_kleborate_count.tsv\").read().split(\"\\n\")\n",
    "for index_strain, info in enumerate(good_strain):\n",
    "    if info:\n",
    "        strain=info.split(\"\\t\")[0].strip()\n",
    "        ktype=info.split(\"\\t\")[1].strip()\n",
    "        strain_ktype[strain]=ktype\n",
    "        \n",
    "\n",
    "for phage in tqdm(os.listdir(path_fasta)):       \n",
    "    strain=phage.split(\"\\t\")[0].split(\"__\")[0]\n",
    "    prophage_id=phage.split(\"\\t\")[0].split(\"__\")[1].split(\".fasta\")[0]\n",
    "    prophage=phage.split(\"\\t\")[0].split(\".fasta\")[0]\n",
    "    print(strain,prophage_id,prophage)\n",
    "    try :\n",
    "        os.mkdir(f\"{path_decipher}/{strain}\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    if os.path.isfile(f\"{path_decipher}/{strain}/{prophage}\")== False :\n",
    "        copy_fasta = f\"cp {path_fasta}/{prophage}.fasta {path_decipher}/{strain}/{prophage}.fasta\"\n",
    "        copy_fasta_process = subprocess.Popen(copy_fasta, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "        press_out, press_err = copy_fasta_process.communicate()\n",
    "    genecall=pd.read_csv(f\"{path_phageboost_pred}/{strain}/genecalls_{strain.split('.')[0]}.gff3\", sep=\"\\t\")\n",
    "    for file in os.listdir(f\"{path_phageboost_pred}/{strain}\"):\n",
    "        if file[0:6]==\"phages\":\n",
    "            resume= pd.read_csv(f\"{path_phageboost_pred}/{strain}/{file}\", skiprows=1, sep=\"\\t\")\n",
    "            break\n",
    "    print(resume)\n",
    "    attributes = resume[\"attributes\"].to_list()\n",
    "    for index_att, attri in enumerate(attributes):\n",
    "        if attri.split(\"phage_id=\")[1] == prophage_id :\n",
    "            attribute_line = attri\n",
    "            break\n",
    "    frag_id = resume[resume[\"attributes\"] == attribute_line][\"#seqid\"].values[0]\n",
    "    start_genome ,stop_genome =resume[resume[\"attributes\"] == attribute_line][\"start\"].values[0], resume[resume[\"attributes\"] == attribute_line][\"end\"].values[0]\n",
    "    print(frag_id ,start_genome, stop_genome)\n",
    "    #break\n",
    "    genecall_frag = genecall[genecall[\"contig\"] == frag_id]\n",
    "    genecall_frag_dict = genecall_frag.to_dict('records')\n",
    "    with open(f\"{path_decipher}/{strain}/{prophage_id}.multi.candidates.faa\", \"w\") as outfile_faa :\n",
    "        with open(f\"{path_decipher}/{strain}/{prophage_id}.multi.candidates.ffn\", \"w\") as outfile_ffn :\n",
    "            for line in genecall_frag_dict : \n",
    "                if line[\"start\"] in range(start_genome, stop_genome) :\n",
    "                    nt_seq, aa_seq, prot_id =line[\"DNAseq\"] , line[\"AAseq\"],  line[\"id\"]\n",
    "                    print(len(aa_seq), \"Protein_id : \", prot_id)\n",
    "                    if len(aa_seq) > 200 :\n",
    "                        outfile_faa.write(f\">{strain}__{prophage_id}__{prot_id}\\n{aa_seq}\\n\")\n",
    "                        outfile_ffn.write(f\">{strain}__{prophage_id}__{prot_id}\\n{nt_seq}\\n\")\n",
    "                        print(f\">{strain}__{prophage_id}__{prot_id}\\n{aa_seq}\\n\", f\">{strain}__{prophage_id}__{prot_id}\\n{nt_seq}\\n\")\n",
    "                \n",
    "    \n",
    "# *******************************************************************************************************************************************************************\n",
    "#!/bin/bash\n",
    "#BATCH --job-name=candidates\n",
    "#SBATCH --partition=short \n",
    "#SBATCH --ntasks=1 \n",
    "#SBATCH --cpus-per-task=10\n",
    "#SBATCH --mem=10gb \n",
    "#SBATCH --time=1-00:00:00 \n",
    "#SBATCH --output=candidates%j.log \n",
    "\n",
    "module restore la_base\n",
    "source /storage/apps/ANACONDA/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate ScaleAP\n",
    "\n",
    "python /home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/script_files/py_files/post_fastani_pt3.py\n",
    "# *******************************************************************************************************************************************************************\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "path_phageboost_pred=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_decipher/phageboost/phageboost_prediction\"\n",
    "path_fasta=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022\"\n",
    "path_fastani=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_out_20102022\"\n",
    "path_decipher = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_decipher/phageboost/phageboost_70_20102022\"\n",
    "path_ktype=\"/home/conchae/prediction_depolymerase_tropism\"\n",
    "\n",
    "\n",
    "for strain in os.listdir(path_decipher):\n",
    "    try :\n",
    "        os.mkdir (f\"{path_decipher}/{strain}/hmmer_out\")\n",
    "        os.mkdir (f\"{path_decipher}/{strain}/tmp\")\n",
    "    except FileExistsError :\n",
    "        pass\n",
    "    hmmer_out=f\"{path_decipher}/{strain}/hmmer_out\"\n",
    "    tmp=f\"{path_decipher}/{strain}/tmp\"\n",
    "    for file in os.listdir(f\"{path_decipher}/{strain}\"):\n",
    "        if file[-14:]==\"candidates.faa\":\n",
    "            candidates= open(f\"{path_decipher}/{strain}/{file}\").read().split(\">\")\n",
    "            prophage_name=file.split(\".\")[0]\n",
    "            try :\n",
    "                os.mkdir (f\"{tmp}/{prophage_name}\")\n",
    "                os.mkdir (f\"{hmmer_out}/{prophage_name}\")\n",
    "            except FileExistsError :\n",
    "                pass\n",
    "            path_out=f\"{hmmer_out}/{prophage_name}\"\n",
    "            for index_seq, seq_faa in enumerate(candidates) :\n",
    "                if seq_faa :\n",
    "                    seq_name=seq_faa.split(\"\\n\")[0]\n",
    "                    if os.path.isfile(f\"{tmp}/{prophage_name}/{seq_name}.fasta\")== False :\n",
    "                        with open(f\"{tmp}/{prophage_name}/{seq_name}.fasta\",\"w\") as outfile :\n",
    "                            outfile.write(f\">{seq_faa}\")\n",
    "\n",
    "# *******************************************************************************************************************************************\n",
    "#!/bin/bash\n",
    "#BATCH --job-name=tmp_file\n",
    "#SBATCH --partition=short \n",
    "#SBATCH --ntasks=1 \n",
    "#SBATCH --cpus-per-task=5\n",
    "#SBATCH --mem=10gb \n",
    "#SBATCH --time=1-00:00:00 \n",
    "#SBATCH --output=tmp_file%j.log \n",
    "\n",
    "module restore la_base                                \n",
    "source /storage/apps/ANACONDA/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate ScaleAP\n",
    "                                                      \n",
    "python /home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/script_files/py_files/create_tmp.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML_work_v2]",
   "language": "python",
   "name": "conda-env-ML_work_v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
