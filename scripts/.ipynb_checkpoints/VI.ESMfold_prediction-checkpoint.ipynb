{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOAL : generate ESMfold prediction for the selected proteins after V3\n",
    "### I. Generate the sequence file\n",
    "### II. The embedding script\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the sequence idex file :\n",
    "import os \n",
    "from Bio import SeqIO\n",
    "import pandas as pd \n",
    "\n",
    "path_millard = \"/home/conchae/ML_depolymerase/get_candidates/millard\"\n",
    "\n",
    "good = [\"tail\",\"EPS\",\"receptor\",\"baseplate\",\"collar\",\"lipase\",\"depolymerase\",\"cps\",\"lyase\"] \n",
    "bad = [\"endolysin\"]\n",
    "data = {}\n",
    "for rec in SeqIO.parse(f\"{path_millard}/millard_depo.v3.fasta\" , \"fasta\") :\n",
    "    if rec.seq not in data :\n",
    "        for des in good :\n",
    "            if rec.description.count(des) > 0:\n",
    "                data[rec.seq] = rec.description\n",
    "                break\n",
    "    else :\n",
    "        pass\n",
    "    \n",
    "with open(f\"{path_millard}/df_sequences.esmfold.v3.fasta\" ,\"w\") as outfile :\n",
    "    for seq in data :\n",
    "        if data[seq].count(\"tail length tape measure protein\") < 1 and data[seq].count(\"lysozyme\") < 1:\n",
    "            outfile.write(f\">{data[seq]}\\n{seq}\\n\")\n",
    "    \n",
    "    for index_seq, seq in enumerate(list(data.keys())) :\n",
    "        for prot in data[seq] :\n",
    "            outfile.write(f\"{index_seq}\\t{prot}\\t{seq}\\n\")    \n",
    "    \n",
    "with open(f\"{path_millard}/df_sequences.index.v2.csv\" ,\"w\") as outfile :\n",
    "    for index_seq, seq in enumerate(list(data.keys())) :\n",
    "        for prot in data[seq] :\n",
    "            outfile.write(f\"{index_seq}\\t{prot}\\t{seq}\\n\")\n",
    "        \n",
    "df = pd.read_csv(f\"{path_millard}/df_sequences.index.v2.csv\", sep=\"\\t\", names = [\"index\",\"id\",\"sequence\"])       \n",
    "df = df.drop_duplicates(subset=[\"index\"], keep=\"first\")\n",
    "df.to_csv(f\"{path_millard}/df_sequences.index.clean.v2.csv\", sep=\"\\t\", columns = [\"index\",\"sequence\"], index=False)\n",
    "\n",
    "\n",
    "df = pd.read_csv(f\"{path_millard}/df_sequences.index.clean.v2.csv\", sep=\"\\t\")\n",
    "with open(f\"{path_millard}/millard_depo.indexed.v2.fasta\" , \"w\") as outfile :\n",
    "    dico_interest = df.to_dict(\"records\")\n",
    "    for row in dico_interest :\n",
    "        outfile.write(f\">{row['index']}\\n{row['sequence']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "II. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from multiprocessing.pool import ThreadPool\n",
    "#from multiprocessing import Pool\n",
    "import os \n",
    "import time\n",
    "\n",
    "from transformers import AutoTokenizer, EsmForProteinFolding\n",
    "import torch\n",
    "\n",
    "from transformers.models.esm.openfold_utils.protein import to_pdb, Protein as OFProtein\n",
    "from transformers.models.esm.openfold_utils.feats import atom14_to_atom37\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esmfold_v1\")\n",
    "model = EsmForProteinFolding.from_pretrained(\"facebook/esmfold_v1\")\n",
    "# model = EsmForProteinFolding.from_pretrained(\"facebook/esmfold_v1\", low_cpu_mem_usage=True)\n",
    "path_proteins = \"/home/conchae/ML_depolymerase/get_candidates/millard\"\n",
    "\n",
    "# done = [prot.split(\".pdb\")[0] for prot in os.listdir(f\"{path_proteins}/millard_pdb\")]\n",
    "\n",
    "\n",
    "def tokenize_fasta(sequences, fasta_csv) :\n",
    "    \"\"\"\n",
    "    The function takes as an input a either a multifasta file or a dataframe with two columns.\n",
    "    If the input is a dataframe, the shape would consist of two columns with :\n",
    "    - 'id', which corresponds to the protein name\n",
    "    - 'sequence', which corresponds to the aa sequence\n",
    "    The function returns a list of tuples (a,b) with a as the id and b as the tokenized inputs\n",
    "    \n",
    "    \"\"\"\n",
    "    if fasta_csv == \"csv\" :\n",
    "        dico_seq = sequences.to_dict('records')\n",
    "    elif fasta_csv == \"fasta\" :\n",
    "        from Bio import SeqIO\n",
    "        dico_seq = {record.id : str(record.seq) for record in SeqIO.parse(sequences, \"fasta\") if record.id not in done}\n",
    "    tokenized_sequences = []\n",
    "    for idd in dico_seq :\n",
    "        tokenized_input = tokenizer(dico_seq[idd], return_tensors=\"pt\", add_special_tokens=False)['input_ids']\n",
    "        a = (idd , tokenized_input)\n",
    "        tokenized_sequences.append(a)\n",
    "    return tokenized_sequences\n",
    "\n",
    "def convert_outputs_to_pdb(outputs):\n",
    "    final_atom_positions = atom14_to_atom37(outputs[\"positions\"][-1], outputs)\n",
    "    outputs = {k: v.to(\"cpu\").numpy() for k, v in outputs.items()}\n",
    "    final_atom_positions = final_atom_positions.cpu().numpy()\n",
    "    final_atom_mask = outputs[\"atom37_atom_exists\"]\n",
    "    pdbs = []\n",
    "    for i in range(outputs[\"aatype\"].shape[0]):\n",
    "        aa = outputs[\"aatype\"][i]\n",
    "        pred_pos = final_atom_positions[i]\n",
    "        mask = final_atom_mask[i]\n",
    "        resid = outputs[\"residue_index\"][i] + 1\n",
    "        pred = OFProtein(\n",
    "            aatype=aa,\n",
    "            atom_positions=pred_pos,\n",
    "            atom_mask=mask,\n",
    "            residue_index=resid,\n",
    "            b_factors=outputs[\"plddt\"][i],\n",
    "            chain_index=outputs[\"chain_index\"][i] if \"chain_index\" in outputs else None,\n",
    "        )\n",
    "        pdbs.append(to_pdb(pred))\n",
    "    return pdbs\n",
    "\n",
    "\n",
    "def esmfold_prediction(proteins) :\n",
    "    \"\"\"\n",
    "    The function takes as an input :\n",
    "    - 'tokenized_sequences', the output of the function tokenize_fasta\n",
    "    - 'path_out', the path of the directory when the pdb files are to be written\n",
    "    The function generates the pdb files in the path_out\n",
    "    \n",
    "    # Changed for the map process\n",
    "    \n",
    "    \"\"\"\n",
    "    path_out = f\"{path_proteins}/millard_pdb\"\n",
    "    #try :\n",
    "    #    os.mkdir(f\"{path_out}\")\n",
    "    #except Exception as e :\n",
    "        #print(e)\n",
    "    #    pass\n",
    "    with open(f\"{path_proteins}/control_esm_fold.single\",\"a+\") as outfile_control :\n",
    "        for protein in proteins :\n",
    "            outfile_control.write(f\"prediction of {protein[0]}.\\nTensor : {protein[1]}__ {time.ctime(time.time())}\\n\")\n",
    "            pdb_files = []\n",
    "            output = \"\"\n",
    "            with torch.no_grad():\n",
    "                output = model(protein[1])\n",
    "            outfile_control.write(f\"model done with {protein[0]}__{time.ctime(time.time())}.\\n\")\n",
    "            pdb_txt = convert_outputs_to_pdb(output)\n",
    "            with open(f\"{path_out}/{protein[0]}.pdb\" ,\"w\") as outfile :\n",
    "                outfile.write(pdb_txt[0])\n",
    "\n",
    "                \n",
    "# The final function :           \n",
    "def esmfold_prediction_parralel(protein) :\n",
    "    \"\"\"\n",
    "    The function takes as an input :\n",
    "    - 'tokenized_sequences', the output of the function tokenize_fasta\n",
    "    - 'path_out', the path of the directory when the pdb files are to be written\n",
    "    The function generates the pdb files in the path_out\n",
    "    \n",
    "    # Changed for the map process\n",
    "    \n",
    "    \"\"\"\n",
    "    path_out = f\"{path_proteins}/millard_pdb\"\n",
    "    #try :\n",
    "    #    os.mkdir(f\"{path_out}\")\n",
    "    #except Exception as e :\n",
    "        #print(e)\n",
    "    #    pass\n",
    "    with open(f\"{path_proteins}/control_esm_fold.parra\",\"a+\") as outfile_control :\n",
    "        outfile_control.write(f\"prediction of {protein[0]}.\\nTensor : {protein[1]}__ {time.ctime(time.time())}\\n\")\n",
    "        pdb_files = []\n",
    "        output = \"\"\n",
    "        with torch.no_grad():\n",
    "            output = model(protein[1])\n",
    "        outfile_control.write(f\"model done with {protein[0]}__{time.ctime(time.time())}.\\n\")\n",
    "        pdb_txt = convert_outputs_to_pdb(output)\n",
    "        with open(f\"{path_out}/{protein[0]}.pdb\" ,\"w\") as outfile :\n",
    "            outfile.write(pdb_txt[0])\n",
    "                \n",
    "                \n",
    "# The predictions\n",
    "#__try__ = tokenize_fasta(f\"{path_proteins}/millard_depo.v3.fasta\" , \"fasta\")\n",
    "# esmfold_prediction(__try__)\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    with ThreadPool(60) as p:\n",
    "#        p.map(esmfold_prediction, __try__)\n",
    "if __name__ == '__main__':\n",
    "    with open(f\"{path_proteins}/control_esm_fold.parrallel\",\"a+\") as outfile :\n",
    "        outfile.write(f\"Main is strating.\\nWe are dealing with {len(__try__)} seqeunces.\\n\")\n",
    "    with ThreadPool(5) as p:\n",
    "        p.map(esmfold_prediction_parralel, __try__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#BATCH --job-name=ESM_fold_millard_\n",
    "#SBATCH --qos=long-mem\n",
    "#SBATCH --ntasks=1 \n",
    "#SBATCH --cpus-per-task=60 \n",
    "#SBATCH --mem=200gb \n",
    "#SBATCH --time=10-00:00:00 \n",
    "#SBATCH --output=ESM_fold__%j.log \n",
    "\n",
    "source /storage/apps/ANACONDA/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate embeddings\n",
    "\n",
    "python /home/conchae/ML_depolymerase/scripts/esmfold_pred.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsync -avzhe ssh conchae@garnatxa.srv.cpd:/home/conchae/ML_depolymerase/get_candidates/millard/millard_depo.indexed.v3.fasta /media/concha-eloko/Linux/depolymerase_building\n",
    "rsync -avzhe ssh conchae@garnatxa.srv.cpd:/home/conchae/ML_depolymerase/get_candidates/millard/embeddings.proteins.v3.csv /media/concha-eloko/Linux/depolymerase_building\n",
    "rsync -avzhe ssh conchae@garnatxa.srv.cpd:/home/conchae/ML_depolymerase/get_candidates/millard/df_sequences.index.v3.csv /media/concha-eloko/Linux/depolymerase_building\n",
    "rsync -avzhe ssh conchae@garnatxa.srv.cpd:/home/conchae/ML_depolymerase/get_candidates/millard/proteinID_annotation.v3.json /media/concha-eloko/Linux/depolymerase_building\n",
    "    \n",
    "rsync -avzhe ssh conchae@garnatxa.srv.cpd:/home/conchae/ML_depolymerase/get_candidates/millard/df_sequences.esmfold.v3.fasta /media/concha-eloko/Linux/depolymerase_building\n",
    "rsync -avzhe ssh conchae@garnatxa.srv.cpd:/home/conchae/ML_depolymerase/get_candidates/millard/sword_out /media/concha-eloko/Linux/depolymerase_building\n",
    "rsync -avzhe ssh conchae@garnatxa.srv.cpd:/home/conchae/ML_depolymerase/get_candidates/millard/millard_pdb /media/concha-eloko/Linux/depolymerase_building\n",
    "rsync -avzhe ssh conchae@garnatxa.srv.cpd:/home/conchae/ML_depolymerase/get_candidates/millard/DBsuite_depo3_outputs /media/concha-eloko/Linux/depolymerase_building\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time\n",
    "\n",
    "\n",
    "path_proteins = \"/home/conchae/ML_depolymerase/get_candidates/millard\"\n",
    "\n",
    "done = [prot.split(\".pdb\")[0] for prot in os.listdir(f\"{path_proteins}/millard_pdb\")]\n",
    "\n",
    "for file in tqdm(os.listdir(f\"{path_proteins}/DBsuite_depo3\")) :\n",
    "    for prot in done :\n",
    "        if file.count(prot) > 0 :\n",
    "            os.system(f\"cp {path_proteins}/DBsuite_depo3/{file} {path_proteins}/DBsuite_depo3_outputs\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The threadpool version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool\n",
    "import os \n",
    "import time\n",
    "\n",
    "from transformers import AutoTokenizer, EsmForProteinFolding\n",
    "import torch\n",
    "\n",
    "from transformers.models.esm.openfold_utils.protein import to_pdb, Protein as OFProtein\n",
    "from transformers.models.esm.openfold_utils.feats import atom14_to_atom37\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esmfold_v1\")\n",
    "model = EsmForProteinFolding.from_pretrained(\"facebook/esmfold_v1\")\n",
    "# model = EsmForProteinFolding.from_pretrained(\"facebook/esmfold_v1\", low_cpu_mem_usage=True)\n",
    "path_proteins = \"/home/conchae/ML_depolymerase/get_candidates/millard\"\n",
    "\n",
    "done = [prot.split(\".pdb\")[0] for prot in os.listdir(f\"{path_proteins}/millard_pdb\")]\n",
    "\n",
    "\n",
    "def tokenize_fasta(sequences, fasta_csv) :\n",
    "    \"\"\"\n",
    "    The function takes as an input a either a multifasta file or a dataframe with two columns.\n",
    "    If the input is a dataframe, the shape would consist of two columns with :\n",
    "    - 'id', which corresponds to the protein name\n",
    "    - 'sequence', which corresponds to the aa sequence\n",
    "    The function returns a list of tuples (a,b) with a as the id and b as the tokenized inputs\n",
    "    \n",
    "    \"\"\"\n",
    "    if fasta_csv == \"csv\" :\n",
    "        dico_seq = sequences.to_dict('records')\n",
    "    elif fasta_csv == \"fasta\" :\n",
    "        from Bio import SeqIO\n",
    "        dico_seq = {record.id : str(record.seq) for record in SeqIO.parse(sequences, \"fasta\") if record.id not in done}\n",
    "    tokenized_sequences = []\n",
    "    for idd in dico_seq :\n",
    "        tokenized_input = tokenizer(dico_seq[idd], return_tensors=\"pt\", add_special_tokens=False)['input_ids']\n",
    "        a = (idd , tokenized_input)\n",
    "        tokenized_sequences.append(a)\n",
    "    return tokenized_sequences\n",
    "\n",
    "def convert_outputs_to_pdb(outputs):\n",
    "    final_atom_positions = atom14_to_atom37(outputs[\"positions\"][-1], outputs)\n",
    "    outputs = {k: v.to(\"cpu\").numpy() for k, v in outputs.items()}\n",
    "    final_atom_positions = final_atom_positions.cpu().numpy()\n",
    "    final_atom_mask = outputs[\"atom37_atom_exists\"]\n",
    "    pdbs = []\n",
    "    for i in range(outputs[\"aatype\"].shape[0]):\n",
    "        aa = outputs[\"aatype\"][i]\n",
    "        pred_pos = final_atom_positions[i]\n",
    "        mask = final_atom_mask[i]\n",
    "        resid = outputs[\"residue_index\"][i] + 1\n",
    "        pred = OFProtein(\n",
    "            aatype=aa,\n",
    "            atom_positions=pred_pos,\n",
    "            atom_mask=mask,\n",
    "            residue_index=resid,\n",
    "            b_factors=outputs[\"plddt\"][i],\n",
    "            chain_index=outputs[\"chain_index\"][i] if \"chain_index\" in outputs else None,\n",
    "        )\n",
    "        pdbs.append(to_pdb(pred))\n",
    "    return pdbs\n",
    "\n",
    "\n",
    "def esmfold_prediction(proteins) :\n",
    "    \"\"\"\n",
    "    The function takes as an input :\n",
    "    - 'tokenized_sequences', the output of the function tokenize_fasta\n",
    "    - 'path_out', the path of the directory when the pdb files are to be written\n",
    "    The function generates the pdb files in the path_out\n",
    "    \n",
    "    # Changed for the map process\n",
    "    \n",
    "    \"\"\"\n",
    "    path_out = f\"{path_proteins}/millard_pdb\"\n",
    "    #try :\n",
    "    #    os.mkdir(f\"{path_out}\")\n",
    "    #except Exception as e :\n",
    "        #print(e)\n",
    "    #    pass\n",
    "    with open(f\"{path_proteins}/control_esm_fold.single\",\"a+\") as outfile_control :\n",
    "        outfile_control.write(f\"prediction of {protein[0]}.\\nTensor : {protein[1]}__ {time.ctime(time.time())}\\n\")\n",
    "        pdb_files = []\n",
    "        output = \"\"\n",
    "        with torch.no_grad():\n",
    "            output = model(protein[1])\n",
    "        outfile_control.write(f\"model done with {protein[0]}__{time.ctime(time.time())}.\\n\")\n",
    "        pdb_txt = convert_outputs_to_pdb(output)\n",
    "        with open(f\"{path_out}/{protein[0]}.pdb\" ,\"w\") as outfile :\n",
    "            outfile.write(pdb_txt[0])\n",
    "\n",
    "                \n",
    "                \n",
    "def esmfold_prediction_parralel(protein) :\n",
    "    \"\"\"\n",
    "    The function takes as an input :\n",
    "    - 'tokenized_sequences', the output of the function tokenize_fasta\n",
    "    - 'path_out', the path of the directory when the pdb files are to be written\n",
    "    The function generates the pdb files in the path_out\n",
    "    \n",
    "    # Changed for the map process\n",
    "    \n",
    "    \"\"\"\n",
    "    path_out = f\"{path_proteins}/millard_pdb\"\n",
    "    #try :\n",
    "    #    os.mkdir(f\"{path_out}\")\n",
    "    #except Exception as e :\n",
    "        #print(e)\n",
    "    #    pass\n",
    "    with open(f\"{path_proteins}/control_esm_fold.parra\",\"a+\") as outfile_control :\n",
    "        outfile_control.write(f\"prediction of {protein[0]}.\\nTensor : {protein[1]}__ {time.ctime(time.time())}\\n\")\n",
    "        pdb_files = []\n",
    "        output = \"\"\n",
    "        with torch.no_grad():\n",
    "            output = model(protein[1])\n",
    "        outfile_control.write(f\"model done with {protein[0]}__{time.ctime(time.time())}.\\n\")\n",
    "        pdb_txt = convert_outputs_to_pdb(output)\n",
    "        with open(f\"{path_out}/{protein[0]}.pdb\" ,\"w\") as outfile :\n",
    "            outfile.write(pdb_txt[0])\n",
    "                \n",
    "                \n",
    "# The predictions\n",
    "#__try__ = tokenize_fasta(f\"{path_proteins}/millard_depo.v3.fasta\" , \"fasta\")\n",
    "# esmfold_prediction(__try__)\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    with ThreadPool(60) as p:\n",
    "#        p.map(esmfold_prediction, __try__)\n",
    "if __name__ == '__main__':\n",
    "    with open(f\"{path_proteins}/control_esm_fold.parrallel\",\"a+\") as outfile :\n",
    "        outfile.write(f\"Main is strating.\\nWe are dealing with {len(__try__)} seqeunces.\\n\")\n",
    "    with ThreadPool(5) as p:\n",
    "        p.map(esmfold_prediction_parralel, __try__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
