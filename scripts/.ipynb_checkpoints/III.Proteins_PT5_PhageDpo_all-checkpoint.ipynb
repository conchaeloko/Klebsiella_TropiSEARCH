{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decipher the proteins with B helix that were missed by our methods\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "from collections import Counter, defaultdict\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "path_fasta = f\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/15122022_session/part_III_ptA/input_db/all_prophage_proteins.db.fasta\"\n",
    "path_current = f\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023\"\n",
    "path_model = f\"/home/conchae/PhageDepo_pdb/script_files/esm2_t30_150M_UR50D-finetuned-depolymerase/checkpoint-198\"\n",
    "path_work = f\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023\"\n",
    "path_task = f\"{path_work}/Rafa_task\"\n",
    "path_labels = f\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_labeling/phageboost/info\"\n",
    "\n",
    "\n",
    "df_labels = pd.read_csv(f\"{path_labels}/prophage_data.clusters_80.phageboost_70.2504.tsv\", sep = \"\\t\" , skiprows=1)\n",
    "df_labels.columns = [\"Prophage_name\",\"KL_type\",\"Infected_ancestor\",\"n_clades\",\"siblings\",\"n_ancestors\",\"n_KL_swaps\",\"old_KL_types\",\"all_old_KL_types\"]\n",
    "\n",
    "df_current = pd.read_csv(f\"{path_current}/DF_Dpo.final.1005.tsv\", sep = \"\\t\", header = 0)\n",
    "fasta_seqs = SeqIO.parse(path_fasta , \"fasta\")\n",
    "\n",
    "dico_seq = defaultdict(list)\n",
    "for record in fasta_seqs:\n",
    "    tmp_prot_name = record.id\n",
    "    sequence = str(record.seq)\n",
    "    dico_seq[sequence].append(tmp_prot_name)\n",
    "        \n",
    "seq_set = set(df_current[\"seq\"])\n",
    "\n",
    "# Load the model : \n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_model)\n",
    "model = AutoModelForTokenClassification.from_pretrained(path_model)\n",
    "\n",
    "def model_out(sequence) :\n",
    "    input_ids = tokenizer.encode(sequence, return_tensors='pt', truncation= True, max_length = 1024)\n",
    "    outputs = model(input_ids)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    labels = model.config.id2label\n",
    "    tokens = []\n",
    "    for token_id, token_probs in zip(input_ids[0], probs[0]):\n",
    "        top_label_id = token_probs.argmax().item()\n",
    "        tokens.append(int(labels[top_label_id].split(\"_\")[1]))\n",
    "    return tokens\n",
    "\n",
    "def longest_run_of_ones(tokens):\n",
    "    str_lst = ''.join(map(str, tokens))\n",
    "    runs = list(map(len, str_lst.split('0')))\n",
    "    longest_run = max(runs)\n",
    "    start_pos = runs.index(longest_run)\n",
    "    end_pos = start_pos + longest_run - 1\n",
    "    return longest_run, start_pos, end_pos\n",
    "\n",
    "\n",
    "def beta_helix_assess(sequence):\n",
    "    tokens = model_out(sequence)\n",
    "    longest_run, start_pos, end_pos = longest_run_of_ones(tokens)\n",
    "    if int(longest_run) > 180 :\n",
    "        if sequence not in seq_set:\n",
    "            protein_names = dico_seq[sequence]\n",
    "            with open(f\"{path_work}/Dpo_from_the_dead.tsv\" , \"a+\") as outfile:\n",
    "                for protein_name in protein_names:\n",
    "                    outfile.write(f\"{protein_name}\\t{start_pos}\\t{end_pos}\\t{sequence}\\n\")\n",
    "\n",
    "                    \n",
    "                    \n",
    "if __name__ == '__main__':\n",
    "    results = map(beta_helix_assess, list(dico_seq.keys()))\n",
    "    # If you want to force computation and get a list of results:\n",
    "    results = list(results)\n",
    "    \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    with ThreadPool(20) as p:\n",
    "        p.map(beta_helix_assess, list(dico_seq.keys()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#BATCH --job-name=Anubis__\n",
    "#SBATCH --qos=short \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=30\n",
    "#SBATCH --mem=100gb \n",
    "#SBATCH --time=1-00:00:00 \n",
    "#SBATCH --output=Anubis__%j.log \n",
    "\n",
    "source /storage/apps/ANACONDA/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate embeddings\n",
    "\n",
    "python /home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023/script_files/anubis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "\n",
    "path_fasta = f\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/15122022_session/part_III_ptA/input_db/all_prophage_proteins.db.fasta\"\n",
    "path_current = f\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023\"\n",
    "path_model = f\"/home/conchae/PhageDepo_pdb/script_files/esm2_t30_150M_UR50D-finetuned-depolymerase/checkpoint-198\"\n",
    "path_work = f\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023\"\n",
    "\n",
    "df_anubis = pd.read_csv(f\"{path_work}/Dpo_from_the_dead.tsv\", sep = \"\\t\", names = [\"prot_name\", \"start\", \"end\",\"sequence\"])\n",
    "\n",
    "df_seq = df_anubis.drop_duplicates(subset = [\"sequence\"], keep = \"first\")\n",
    "df_seq.to_csv(f\"{path_work}/Anubis_Dpo.index.csv\" , sep = \"\\t\", index = False)\n",
    "\n",
    "df_seq = pd.read_csv(f\"{path_work}/Anubis_Dpo.index.csv\" , sep = \"\\t\", header = 0)\n",
    "\n",
    "with open(f\"{path_work}/Anubis_Dpo.fasta\", \"w\") as outfile :\n",
    "    n = 0\n",
    "    for _,row in df_seq.iterrows() :\n",
    "        outfile.write(f\">{n}\\n{row['sequence']}\\n\")\n",
    "        n += 1 \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsync -avzhe ssh \\\n",
    "conchae@garnatxa.srv.cpd:/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023/Anubis_Dpo.fasta \\\n",
    "/media/concha-eloko/Linux/PPT_clean/ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
