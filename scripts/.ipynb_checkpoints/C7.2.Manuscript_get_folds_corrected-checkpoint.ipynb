{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c82d34d-eacd-49a1-9cf5-6b9b41fa8f6c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Initio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8060e571-30ce-4e62-9233-320aa2ae2f78",
   "metadata": {},
   "source": [
    "> Anubis : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d14da-7954-41c9-bc8e-a22a5e204a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# local :\n",
    "path_project = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "\n",
    "info_col = [\"prot_name\",\"start\",\"end\",\"domain_seq\",\"full_seq\",\"dpo_index\"]\n",
    "\n",
    "df_prophage_label = pd.read_csv(f\"{path_project}/prophage_data.clusters_80.phageboost_70.2504.tsv\" , sep = \"\\t\", skiprows=1)\n",
    "df_prophage_label.columns = [\"Prophage_name\",\"KL_type\",\"Infected_ancestor\",\"n_clades\",\"siblings\",\"n_ancestors\",\"n_KL_swaps\",\"old_KL_types\",\"all_old_KL_types\"]\n",
    "\n",
    "df_anubis_return_emb = pd.read_csv(f\"{path_project}/embeddings/anubis_return.esm2.embedding.csv\" , sep = \",\", header = None)\n",
    "df_anubis_return_info = pd.read_csv(f\"{path_project}/Anubis_return.predictions.0709.big.annotated.tsv\" , sep = \"\\t\", names = info_col)\n",
    "anubis_sequences = df_anubis_return_info[\"full_seq\"].unique().tolist()\n",
    "len(anubis_sequences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708fcf2a-2936-414c-b8f4-1ad183ae3496",
   "metadata": {},
   "source": [
    "> ppt :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756cbe46-a1e1-49b0-a749-497ec1d1dfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsync -avzhe ssh \\\n",
    "conchae@garnatxa.srv.cpd:/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/15122022_session/Results_III_DataFrame.v3.csv \\\n",
    "/media/concha-eloko/Linux/PPT_clean \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9846e0ec-13fa-49c1-8f15-1bc43e11a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "\n",
    "path_project = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "\n",
    "labels_results = [\"protein_name\",\"KL_type_ancestor\",\"ancestor_id\",\"IPR_entry\",\"score\",\"qstart\",\"qstop\",\"sequence\"]\n",
    "results_df = pd.read_csv(f\"{path_project}/Results_III_DataFrame.v3.csv\", sep=\"\\t\", names= labels_results)\n",
    "ppt_seq_uniq = results_df[\"sequence\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718a1ce1-dcc6-4829-a138-9cef8123443a",
   "metadata": {},
   "source": [
    "> Minibatch : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b492631e-3f6d-4523-b10c-be8393931620",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsync -avzhe ssh \\\n",
    "conchae@garnatxa.srv.cpd:/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/Bea_hits.seq_unique.fasta \\\n",
    "/media/concha-eloko/Linux/PPT_clean \n",
    "\n",
    "rsync -avzhe ssh \\\n",
    "conchae@garnatxa.srv.cpd:/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/Minibatch_hits.75bits.tsv \\\n",
    "/media/concha-eloko/Linux/PPT_clean \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae1d835-44d9-4f8d-93bf-14d0fd8e71c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "\n",
    "path_project = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "path_work = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "\n",
    "#minibatch_seq_uniq = [item.split(\"\\n\")[1] for item in open(f\"{path_project}/Bea_hits.seq_unique.fasta\").read().split(\">\")[1:]]\n",
    "DF_info_mini = pd.read_csv(f\"{path_work}/Minibatch_hits.75bits.tsv\", sep = \"\\t\" ,  names = [\"protein\", \"bitscore\",\"sequence\"])\n",
    "minibatch_seq_uniq = set(seq for seq in DF_info_mini[\"sequence\"] if len(seq)<1500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffce525-6cc1-44bf-8c5c-6144737431eb",
   "metadata": {},
   "source": [
    "> Final DF : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6e36c9-a16f-4936-84be-4ffa27bc1a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_work = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "\n",
    "DF_info = pd.read_csv(f\"{path_work}/TropiGATv2.final_df.tsv\", sep = \"\\t\" ,  header = 0)\n",
    "DF_lvl_0 = DF_info[~DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4f9d50-c0af-4283-9643-25e737b9ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_unique = DF_lvl_0[\"seq\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5047282a-7265-4616-8ee2-33cea274d2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "set_anubis = set(anubis_sequences)\n",
    "set_minibatch = set(minibatch_seq_uniq)\n",
    "set_ppt = set(ppt_seq_uniq)\n",
    "set_final = set(final_unique)\n",
    "\n",
    "pred_anubis = set_anubis.intersection(set_final)\n",
    "pred_ppt = set_ppt.intersection(set_final)\n",
    "pred_minibatch = set_minibatch.intersection(set_final)\n",
    "\n",
    "\n",
    "# Create the Venn diagram\n",
    "plt.figure(figsize=(8, 8))\n",
    "venn3([pred_anubis, pred_ppt, pred_minibatch], ('Anubis', 'PPT', \"Minibatch\"))\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797a01f7-6b42-46df-8c02-00d463165818",
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_methods = pred_ppt.copy()\n",
    "classic_methods.update(pred_minibatch)\n",
    "\n",
    "anubis_exclu = pred_anubis.difference(classic_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540440cb-d648-4fd9-b2f3-ba9eb92f9678",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classic_gold = classic_methods.difference(pred_anubis)\n",
    "len(classic_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda56781-eaa9-4eb5-99f7-07de385d7682",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_classic = pred_ppt.intersection(pred_minibatch)\n",
    "classic_gold = common_classic.difference(pred_anubis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92c1673-a788-4249-b6d5-cc7d5f97d441",
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_gold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7a4fd6-6abe-4d82-bf3d-25826753087e",
   "metadata": {},
   "source": [
    "# Get the fold proportions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0272b1ac-8c17-4ae7-8db2-851b96f7436f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "\n",
    "path_work = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "\n",
    "DF_info = pd.read_csv(f\"{path_work}/TropiGATv2.final_df_v2.filtered.tsv\", sep = \"\\t\" ,  header = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eab834-6490-4739-9a30-20db473cad75",
   "metadata": {},
   "source": [
    "***\n",
    "# Check PPT :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cbfc73-11e7-46a7-bdeb-567ddae79249",
   "metadata": {},
   "source": [
    "> Open DF :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161b1c6b-3ddb-4b51-b14b-58b405c6e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "path_project = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "\n",
    "# PPT data : \n",
    "#labels_results = [\"protein_name\",\"KL_type_ancestor\",\"ancestor_id\",\"IPR_entry\",\"score\",\"qstart\",\"qstop\",\"sequence\"]\n",
    "results_III_df = pd.read_csv(f\"{path_project}/PPT_results_DF.tsv\", sep=\"\\t\", header = 0)\n",
    "#index_ppt_df = pd.read_csv(f\"{path_project}/Results_III_sequences.v3.csv\", sep = \"\\t\", names = [\"index\", \"seq\"])\n",
    "#results_III_df[\"index\"] = results_III_df[\"sequence\"].progress_apply(lambda x : \"ppt__\" + str(index_ppt_df[index_ppt_df[\"seq\"] == x][\"index\"].values[0]))\n",
    "#results_III_df.to_csv(f\"{path_project}/PPT_results_DF.tsv\", sep = \"\\t\", index = False, header = True)\n",
    "\n",
    "# TropiGAT data : \n",
    "DF_info = pd.read_csv(f\"{path_project}/TropiGATv2.final_df.tsv\", sep = \"\\t\" ,  header = 0)\n",
    "DF_lvl_0 = DF_info[~DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "\n",
    "final_unique = DF_lvl_0[\"seq\"].unique().tolist()\n",
    "all_final_unique = DF_info[\"seq\"].unique().tolist()\n",
    "\n",
    "# Make the relevant sets :\n",
    "set_ppt = set(ppt_seq_uniq)\n",
    "set_final = set(all_final_unique)\n",
    "pred_ppt = set_ppt.intersection(set_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd86e6e-d17d-4881-81e4-f101000a9f0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DF_info[\"index\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a61d8ce-1c4d-4766-9d67-7425b5813bca",
   "metadata": {},
   "source": [
    "> Get the missing indices :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f723ca-44b1-4c47-8ade-5af329ee196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "\n",
    "path_project = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "path_info = f\"/media/concha-eloko/Linux/depolymerase_building/depolymerase_fold.csv\"\n",
    "dir_out = f\"{path_project}/ficheros_28032023/seekfold_PPT\"\n",
    "\n",
    "info_df = pd.read_csv(path_info , sep = \"\\t\", header = 0)\n",
    "\n",
    "outputs = [f\"{dir_out}/{file}\" for file in os.listdir(dir_out) if file[-3:]==\"out\"]\n",
    "header_seekfold = [\"query\",\"target\",\"pident\",\"alnlen\",\"gapopen\",\"qstart\",\"qend\",\"tstart\",\"tend\",\"bits\",\"prob\"]\n",
    "\n",
    "dico_folds_ppt = {\"jelly-roll\" : [],\n",
    "              \"alpha/alpha toroid\" : [],\n",
    "              \"right-handed beta-helix\" : [] ,\n",
    "              \"TIM beta/alpha-barrel\" : [],\n",
    "              \"6-bladed beta-propeller\" : [] ,\n",
    "              \"Flavodoxin-like\" : [] ,\n",
    "              \"Alpha/Beta hydrolase fold\" : [] ,\n",
    "              \"Other\" : [],\n",
    "             }\n",
    "\n",
    "depos_indices = []\n",
    "unique_sequences = set()\n",
    "n = 0 \n",
    "\n",
    "# Scan the results for each pdb files :\n",
    "for results in tqdm(outputs) :\n",
    "    results_df = pd.read_csv(f\"{results}\", sep = \"\\t\" , names = header_seekfold)\n",
    "    for _,row in results_df.iterrows() :\n",
    "        fold = info_df[info_df[\"ID\"] == row[\"target\"]][\"Folds\"].values[0]\n",
    "        if fold not in [\"jelly-roll\", \"Other\"] :\n",
    "            if row[\"prob\"] >= 0.5 :\n",
    "                if row[\"query\"] not in dico_folds_ppt[fold] :\n",
    "                    n += 1\n",
    "                    dico_folds_ppt[fold].append(row[\"query\"])\n",
    "                    depos_indices.append(\"ppt__\" + row[\"query\"].split(\".\")[0])\n",
    "                    break\n",
    "            elif fold == \"right-handed beta-helix\" and row[\"prob\"] >= 0.2 :\n",
    "                if row[\"query\"] not in dico_folds_ppt[fold] :\n",
    "                    n += 1\n",
    "                    dico_folds_ppt[fold].append(row[\"query\"])\n",
    "                    depos_indices.append(\"ppt__\" + row[\"query\"].split(\".\")[0])\n",
    "                    break\n",
    "\n",
    "# Get the indices of the proteins not represented in the TropiGAT DF :\n",
    "missing_ppt_i = []\n",
    "\n",
    "for ppt_i in depos_indices :\n",
    "    if ppt_i not in DF_info[\"index\"].tolist() : \n",
    "        missing_ppt_i.append(ppt_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e6d700-cefe-4fbc-9baa-e9c87d934e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in dico_folds_ppt : \n",
    "    print(f\"The {fold} presented {len(dico_folds_ppt[fold])} depolymerases.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd32ac1-4f22-4b62-804b-36fb5da69649",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(missing_ppt_i) , n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a1e02-9db4-4186-8958-38d6daac7f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2097\n",
    "results_III_df[results_III_df[\"index\"] == f\"ppt__{index}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3db2e51-3b54-4e5a-ae9b-2502bbb71c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_info[DF_info[\"Phage\"] == \" GCF_013415495.1__phage12\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a555b63-1c75-4f00-84de-f8cd56691479",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{path_project}/Missing_PPT.indices.tsv\", \"w\") as outfile : \n",
    "    for _,index in enumerate(missing_ppt_i) :\n",
    "        outfile.write(index + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ce8ec5-3828-4eda-82cf-f5fc9ce8816c",
   "metadata": {},
   "source": [
    "> Check the presence of an effective SWORD2 prediction : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfce3a2-417d-4684-b660-8ce8abae8a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_sword_ppt = \"/media/concha-eloko/Linux/PPT_clean/ficheros_28032023/sword2_DepoSeq_pt2\"\n",
    "\n",
    "bad_swords = []\n",
    "\n",
    "for missing_ppt in tqdm(missing_ppt_i) :\n",
    "    index = missing_ppt.split(\"__\")[1]\n",
    "    if os.path.isdir(f\"{path_sword_ppt}/{index}/{index}_A/Protein_Units\") == True : \n",
    "        pass\n",
    "    else :\n",
    "        bad_swords.append(missing_ppt)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59183770-55f9-47f2-9b19-b5c88a01464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_swords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09526f14-31f5-4b75-b3c4-616f8700f4bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "***\n",
    "# Check Minibatch : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e67b02-ef7e-4c71-a556-7aaf086febca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "\n",
    "path_project = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "\n",
    "#minibatch_seq_uniq = [item.split(\"\\n\")[1] for item in open(f\"{path_project}/Bea_hits.seq_unique.fasta\").read().split(\">\")[1:]]\n",
    "DF_info_mini = pd.read_csv(f\"{path_project}/Minibatch_hits.75bits.tsv\", sep = \"\\t\" ,  names = [\"protein\", \"bitscore\",\"sequence\"])\n",
    "index_df_minibatch = pd.read_csv(f\"{path_project}/minibatch_index.csv\", sep = \"\\t\", names = [\"index\", \"seq\"])\n",
    "minibatch_seq_uniq = set(seq for seq in DF_info_mini[\"sequence\"] if len(seq)<1500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8b8d4c-a4ff-474c-bd20-7dc2d218913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_or_default(seq_value, default_value=\"Not_predicted\"):\n",
    "    filtered_df = index_df_minibatch[index_df_minibatch[\"seq\"] == seq_value]\n",
    "    if not filtered_df.empty:\n",
    "        return \"minibatch__\" + str(filtered_df[\"index\"].values[0])\n",
    "    else:\n",
    "        return default_value\n",
    "\n",
    "#DF_info_mini[\"index\"] = DF_info_mini[\"sequence\"].progress_apply(lambda x: get_index_or_default(x))\n",
    "#DF_info_mini.to_csv(f\"{path_project}/minibatch_results_DF.tsv\", sep = \"\\t\", index = False, header = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bec770-a7b3-4cf9-81dd-8970d7a93c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_info_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6878256b-1647-4f15-be75-3b919930ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "unpredicted_mini = []\n",
    "\n",
    "for seq in tqdm(DF_info_mini[DF_info_mini[\"index\"] == \"Not_predicted\"][\"sequence\"].unique().tolist()) : \n",
    "    if seq not in results_III_df[\"sequence\"].unique().tolist() : \n",
    "        n += 1 \n",
    "        unpredicted_mini.append(seq)\n",
    "# All very long sequences somehow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7f3501-6b25-464b-b512-be021e106d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "\n",
    "path_project = \"/media/concha-eloko/Linux/PPT_clean/ficheros_28032023\"\n",
    "dir_out = f\"{path_project}/seekfold_minibatch\"\n",
    "\n",
    "path_info = f\"/media/concha-eloko/Linux/depolymerase_building/depolymerase_fold.csv\"\n",
    "info_df = pd.read_csv(path_info , sep = \"\\t\", header = 0)\n",
    "\n",
    "\n",
    "outputs = [f\"{dir_out}/{file}\" for file in os.listdir(dir_out) if file[-3:]==\"out\"]\n",
    "header_seekfold = [\"query\",\"target\",\"pident\",\"alnlen\",\"gapopen\",\"qstart\",\"qend\",\"tstart\",\"tend\",\"bits\",\"prob\"]\n",
    "depo_results = {}\n",
    "n = 0\n",
    "\n",
    "dico_folds_minibatch = {\"jelly-roll\" : [],\n",
    "              \"alpha/alpha toroid\" : [],\n",
    "              \"right-handed beta-helix\" : [] ,\n",
    "              \"TIM beta/alpha-barrel\" : [],\n",
    "              \"6-bladed beta-propeller\" : [] ,\n",
    "              \"Flavodoxin-like\" : [] ,\n",
    "              \"Alpha/Beta hydrolase fold\" : [] ,\n",
    "              \"Other\" : [],\n",
    "             }\n",
    "\n",
    "outputs = [f\"{dir_out}/{file}\" for file in os.listdir(dir_out) if file[-3:]==\"out\"]\n",
    "header_seekfold = [\"query\",\"target\",\"pident\",\"alnlen\",\"gapopen\",\"qstart\",\"qend\",\"tstart\",\"tend\",\"bits\",\"prob\"]\n",
    "depo_results_minibatch = {}\n",
    "depo_results_minibatch_list = []\n",
    "\n",
    "n = 0\n",
    "for results in tqdm(outputs) :\n",
    "    results_df = pd.read_csv(f\"{results}\", sep = \"\\t\" , names = header_seekfold)\n",
    "    for _,row in results_df.iterrows() :\n",
    "        fold = info_df[info_df[\"ID\"] == row[\"target\"]][\"Folds\"].values[0]\n",
    "        if fold not in [\"jelly-roll\", \"Other\"] :\n",
    "            if row[\"prob\"] >= 0.5 :\n",
    "                if row[\"query\"] not in dico_folds_minibatch[fold] :\n",
    "                    dico_folds_minibatch[fold].append(row[\"query\"])\n",
    "                    depo_results_minibatch_list.append(\"minibatch__\" + row[\"query\"].split(\".\")[0])\n",
    "                    n += 1\n",
    "                    break\n",
    "            elif fold == \"right-handed beta-helix\" and row[\"prob\"] >= 0.2 :\n",
    "                if row[\"query\"] not in dico_folds_minibatch[fold] :\n",
    "                    dico_folds_minibatch[fold].append(row[\"query\"])\n",
    "                    depo_results_minibatch_list.append(\"minibatch__\" + row[\"query\"].split(\".\")[0])\n",
    "                    n += 1\n",
    "                    break\n",
    "                \n",
    "# Get the indices of the proteins not represented in the TropiGAT DF :\n",
    "missing_mini_i = []\n",
    "\n",
    "for mini_i in depo_results_minibatch_list :\n",
    "    if mini_i not in DF_info[\"index\"].tolist() : \n",
    "        missing_mini_i.append(mini_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb310b9-63db-4cf3-8f1b-16ff9c0fc38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in dico_folds_minibatch : \n",
    "    print(f\"The {fold} presented {len(dico_folds_minibatch[fold])} depolymerases.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33308ad-3369-4ae9-85ce-ff5b444f2348",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(f\"{path_project}/Missing_minibatch.indices.tsv\", \"w\") as outfile : \n",
    "    for _,index in enumerate(missing_mini_i) :\n",
    "        outfile.write(index + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caf7864-1cf3-4235-a58d-748ebd87f896",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_mini_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625cbb29-e2f8-4df6-8305-a3f4b269849d",
   "metadata": {},
   "source": [
    "***\n",
    "# Check Anubis : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bb93ca-33ac-44af-97e1-4b56cdaf64ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "path_project = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "\n",
    "# TropiGAT data : \n",
    "DF_info = pd.read_csv(f\"{path_project}/TropiGATv2.final_df.tsv\", sep = \"\\t\" ,  header = 0)\n",
    "DF_lvl_0 = DF_info[~DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d687dc8-4e57-4e6e-9bfb-a6e7e16b1a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anubis = DF_info[DF_info[\"index\"].str.contains(\"anubis\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e08417-92cb-4334-898c-5b81b91199a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anubis = df_anubis.drop_duplicates(subset = [\"seq\"])\n",
    "df_anubis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ac7b6-4330-4dee-8134-d5b2481bab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_anubis.seq.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b94bde9-34af-4e5d-ab93-70fcb526e749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from DepoScope_functions import Dpo_classifier , find_longest_non_zero_suite_with_n_zeros , predict_sequence, plot_token\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "path_work = \"/media/concha-eloko/Linux/depolymerase_building\"\n",
    "\n",
    "esm2_model_path = f\"{path_work}/esm2_t12_35M_UR50D-finetuned-depolymerase.labels_4/checkpoint-6015\"\n",
    "DpoDetection_path = f\"{path_work}/DepoDetection.T12.4Labels.1908.model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(esm2_model_path)\n",
    "esm2_finetuned = AutoModelForTokenClassification.from_pretrained(esm2_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d87ea9-3bdd-47ed-b617-2f238ee6bb88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_classifier = Dpo_classifier(esm2_finetuned) # Create an instance of Dpo_classifier\n",
    "model_classifier.load_state_dict(torch.load(DpoDetection_path), strict = False) # Load the saved weights ; weird Error with some of the keys \n",
    "model_classifier.eval() # Set the model to evaluation mode for inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa099a0e-26d7-49ce-bc7d-abe87bff6dcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "fold_results = {}\n",
    "\n",
    "for _,row in tqdm(df_anubis.iterrows()) :\n",
    "    protein_seq = row[\"seq\"]\n",
    "    prediction, sequence_outputs = predict_sequence(model_classifier, protein_seq)\n",
    "    if prediction[0] == 1 :\n",
    "        label_count = dict(Counter(sequence_outputs))\n",
    "        fold_results[row[\"index\"]] = label_count\n",
    "    else :\n",
    "        print(\"TF\")\n",
    "        fold_results[row[\"index\"]] = sequence_outputs\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6b8791-a9a9-45ab-b6f2-6535d5870713",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696eb26c-482f-4754-ba15-de4db6f52997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open(f\"{path_project}/Anubis_seqouts.json\", \"w\") as outfile :\n",
    "    json.dump(fold_results, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c4eab2-8815-4d64-a845-f51318ef68a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_anubis = {}\n",
    "failed = []\n",
    "\n",
    "folds_label = {1.0 : \"beta-helix\", 2.0 : \"beta-propeller\", 3.0 : \"triple-helix\"}\n",
    "\n",
    "for protein, pred in fold_results.items() :\n",
    "    if isinstance(pred, dict) == True :\n",
    "        for label in pred :\n",
    "            if label in folds_label :\n",
    "                fold = folds_label[label]\n",
    "                fold_anubis[protein] = fold\n",
    "                break\n",
    "    else :\n",
    "        failed.append(protein)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14a3fff-1c8e-4877-a6c1-029ab184e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c431dbd-1118-401e-a10c-de23590fcc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_folds = [fold for prot,fold in fold_anubis.items()]\n",
    "Counter(predicted_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf1718-dc40-4c37-aab8-9cc0dce6dacb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_work",
   "language": "python",
   "name": "ml_work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
