{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources :<br>\n",
    "https://machinelearningmastery.com/scikit-optimize-for-hyperparameter-tuning-in-machine-learning/<br>\n",
    "https://towardsdatascience.com/bayesian-optimization-with-python-85c66df711ec<br>\n",
    "https://towardsdatascience.com/hyperparameter-optimization-with-scikit-learn-scikit-opt-and-keras-f13367f3e796<br>\n",
    "***\n",
    "# Run a Random Forest for each KL type, after optimizing the clustering preference\n",
    "***\n",
    "### Process in 4 steps :\n",
    "#### I. Clustering the Dpo domains based on the preference\n",
    "#### II. Build the DF with the corresponding Dpo clusters\n",
    "#### III. Run the RF modelization\n",
    "#### IV. Optimazation of the Parameters (RF parameters and preference)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The data :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "\n",
    "# remote :\n",
    "path_project = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023\"\n",
    "path_similarity = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022_out\"\n",
    "path_db = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/15122022_session\"\n",
    "\n",
    "# The embeddings :\n",
    "esm2_depo = pd.read_csv(f\"{path_project}/Dpo.0805.embeddings.ultimate.csv\" , header = None, sep = \",\" , index_col = 0)\n",
    "\n",
    "# Informative DF :\n",
    "family_df = pd.read_csv(f\"{path_similarity}/clusters_99_80.extra_clean.2004.v2.tsv\", sep = \"\\t\", header = 0)\n",
    "df_info = pd.read_csv(f\"{path_db}/DF_Dpo.final.1005.tsv\" , sep = \"\\t\", header =0)\n",
    "prophage_ktype_df = pd.read_csv(f\"{path_project}/DF_optimization/prophage_K_types.ultimate.csv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats import fisher_exact\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def build_dataframe(dico):\n",
    "    prophages = set(prophage for prophages in dico.values() for prophage in prophages)\n",
    "    matrix = [[1 if prophage in dico[dpo] else 0 for dpo in dico.keys()] for prophage in prophages]\n",
    "    df = pd.DataFrame(matrix, index=prophages, columns=dico.keys())\n",
    "    return df\n",
    "\n",
    "def cluster_AFF_prop(pref) :\n",
    "    \"\"\"\n",
    "    In 3 steps :\n",
    "    I. The clustering\n",
    "    II. Generate the Dico\n",
    "    III. Make the DF\n",
    "    \n",
    "    It outputs the DF_preference.cluster in the rep tmp.\n",
    "    \n",
    "    As we plan to run the optimizer in parrallel, some cluster may ahve already been built.\n",
    "    So before clustering, we'll check that the DF has not already been built.\n",
    "    \"\"\"\n",
    "    # I\n",
    "    if os.path.isfile(f\"{path_project}/DF_optimization/tmp/dico_cluster.ultimate.pref_{pref}.json\") == False :\n",
    "        af = AffinityPropagation(damping=0.95, preference=pref, random_state=123, max_iter=1000,verbose=True).fit(esm2_depo)\n",
    "        cluster_centers_indices = af.cluster_centers_indices_\n",
    "        labels = af.labels_\n",
    "        n_clusters_ = len(cluster_centers_indices)\n",
    "        n_clusters_\n",
    "        # II\n",
    "        dico_df = {\"Dpo\" : esm2_depo.index , \"Label\" : labels}\n",
    "        df_results = pd.DataFrame(dico_df)\n",
    "        dico_cluster = {}\n",
    "        for index , row in df_results.iterrows() :\n",
    "            if row[\"Label\"] not in dico_cluster :\n",
    "                tmp_list = []\n",
    "                tmp_list.append(row[\"Dpo\"])\n",
    "                dico_cluster[row[\"Label\"]] = tmp_list\n",
    "            else :\n",
    "                dico_cluster[row[\"Label\"]].append(row[\"Dpo\"])\n",
    "        with open(f\"{path_project}/DF_optimization/tmp/dico_cluster.ultimate.pref_{pref}.json\", \"w\") as outfile:\n",
    "            json.dump(dico_cluster, outfile)\n",
    "        # III.\n",
    "        dico_prophage_id = {}\n",
    "        for Dpo_label , Dpo_list in tqdm(dico_cluster.items()) :\n",
    "            prophages_carrying = set()\n",
    "            for _,dpo in enumerate(Dpo_list) :\n",
    "                prophages_df = df_info[df_info[\"index\"] == dpo]\n",
    "                for phage in prophages_df[\"Phage\"].to_list() :\n",
    "                    family = family_df[family_df[\"prophage\"] == f\"{phage}.fasta\"][\"prophage_id\"].values[0]\n",
    "                    prophages_carrying.add(family)\n",
    "            dico_prophage_id[f\"Dpo__{Dpo_label}\"] = prophages_carrying\n",
    "        df = build_dataframe(dico_prophage_id)\n",
    "        df.sort_index(inplace = True, ascending=True)\n",
    "        df.to_csv(f\"{path_project}/DF_optimization/tmp/prophage_Dpo.pref_{pref}.ultimate.csv\", sep = \"\\t\", header = True , index = True)\n",
    "    else :\n",
    "        pass\n",
    "    \n",
    "def fit_rf_model_random_search(label):\n",
    "    prophage_dpo_df = pd.read_csv(f\"{path_project}/DF_optimization/tmp/prophage_Dpo.pref_{pref}.ultimate.csv\", sep = \"\\t\", header = 0 , index_col = 0)\n",
    "    label_series = prophage_ktype_df[label]\n",
    "    dic_count = dict(Counter(label_series))\n",
    "    if 1 in dic_count:\n",
    "        n_infection = dic_count[1]\n",
    "        df = prophage_dpo_df\n",
    "        n_iter=3\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df, label_series, test_size=0.2, random_state=42)\n",
    "        param_grid = {\n",
    "            'bootstrap': [True, False],\n",
    "            'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "            'max_features': ['auto', 'sqrt'],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'n_estimators': [200, 400, 600, 800]\n",
    "        }\n",
    "        rf = RandomForestClassifier()\n",
    "        rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, \n",
    "                                       n_iter=n_iter, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "        rf_random.fit(X_train, y_train)\n",
    "        # Print the best parameters from the Randomized Search\n",
    "        predictions = rf_random.predict(X_test)\n",
    "        report = classification_report(y_test, predictions, output_dict=True)\n",
    "        # Extract relevant metrics\n",
    "        try : \n",
    "            precision, recall, f1_score = report['1']['precision'], report['1']['recall'], report['1']['f1-score']\n",
    "            # Calculate p-values for features\n",
    "            significant_features = []\n",
    "            for feature in df.columns:\n",
    "                contingency_table = pd.crosstab(df[feature], label_series)\n",
    "                _, p_value = fisher_exact(contingency_table)\n",
    "                if p_value < 0.05:\n",
    "                    significant_features.append(feature)\n",
    "            with open(f\"{path_DF}/RF_results.1605.tsv\", \"a+\") as outfile :\n",
    "                outfile.write(f\"{label}\\t{n_infection}\\t{f1_score}\\t{precision}\\t{recall}\\t{len(significant_features)}\\t{','.join(significant_features)}\\n\")\n",
    "        except Exception as e :\n",
    "            print(e, label, n_infection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from bayes_opt import BayesianOptimization, UtilityFunction\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Prepare the data.\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer[\"data\"]\n",
    "y = cancer[\"target\"]X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                            stratify = y,\n",
    "                                        random_state = 42)scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)# Define the black box function to optimize.\n",
    "def black_box_function(C):\n",
    "    # C: SVC hyper parameter to optimize for.\n",
    "    model = SVC(C = C)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_score = model.decision_function(X_test_scaled)\n",
    "    f = roc_auc_score(y_test, y_score)\n",
    "    return f# Set range of C to optimize for.\n",
    "# bayes_opt requires this to be a dictionary.\n",
    "pbounds = {\"C\": [0.1, 10]}# Create a BayesianOptimization optimizer,\n",
    "# and optimize the given black_box_function.\n",
    "optimizer = BayesianOptimization(f = black_box_function,\n",
    "                                 pbounds = pbounds, verbose = 2,\n",
    "                                 random_state = 4)optimizer.maximize(init_points = 5, n_iter = 10)print(\"Best result: {}; f(x) = {}.\".format(optimizer.max[\"params\"], optimizer.max[\"target\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "search_space = {\"bootstrap\": Categorical([True, False]), # values for boostrap can be either True or False\n",
    "        \"max_depth\": Integer(6, 20), # values of max_depth are integers from 6 to 20\n",
    "        \"max_features\": Categorical(['auto', 'sqrt','log2']), \n",
    "        \"min_samples_leaf\": Integer(2, 10),\n",
    "        \"min_samples_split\": Integer(2, 10),\n",
    "        \"n_estimators\": Integer(100, 500)\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "sklearn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
