{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The final DpoDetection Tool :\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/concha-eloko/Linux/conda_envs/ML_work/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "path_work = \"/media/concha-eloko/Linux/depolymerase_building\"\n",
    "\n",
    "esm2_model_path = f\"{path_work}/esm2_t12_35M_UR50D-finetuned-depolymerase.labels_4/checkpoint-6015\"\n",
    "DpoDetection_path = f\"{path_work}/DepoDetection.T12.4Labels.1908.model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(esm2_model_path)\n",
    "esm2_finetuned = AutoModelForTokenClassification.from_pretrained(esm2_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dpo_classifier(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(Dpo_classifier, self).__init__()\n",
    "        self.max_length = 1024\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=5, stride=1)  # Convolutional layer\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, stride=1)  # Convolutional layer\n",
    "        self.fc1 = nn.Linear(128 * (self.max_length - 2 * (5 - 1)), 32)  # calculate the output shape after 2 conv layers\n",
    "        self.classifier = nn.Linear(32, 1)  # Binary classification\n",
    "\n",
    "    def make_prediction(self, fasta_txt):\n",
    "        input_ids = tokenizer.encode(fasta_txt, truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = self.pretrained_model(input_ids)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            token_probs, token_ids = torch.max(probs, dim=-1)            \n",
    "            tokens = token_ids.view(1, -1) # ensure 2D shape\n",
    "            return tokens\n",
    "\n",
    "    def pad_or_truncate(self, tokens):\n",
    "        if tokens.size(1) < self.max_length:\n",
    "            tokens = F.pad(tokens, (0, self.max_length - tokens.size(1)))\n",
    "        elif tokens.size(1) > self.max_length:\n",
    "            tokens = tokens[:, :self.max_length]\n",
    "        return tokens\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        batch_size = len(sequences)\n",
    "        tokens_batch = []\n",
    "        for seq in sequences:\n",
    "            tokens = self.make_prediction(seq)\n",
    "            tokens = self.pad_or_truncate(tokens)\n",
    "            tokens_batch.append(tokens)\n",
    "        \n",
    "        outputs = torch.cat(tokens_batch).view(batch_size, 1, self.max_length)  # ensure 3D shape\n",
    "        outputs = outputs.float()  # Convert to float\n",
    "        \n",
    "        out = F.relu(self.conv1(outputs))\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = out.view(batch_size, -1)  # Flatten the tensor\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.classifier(out)\n",
    "        return out, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dpo_classifier(\n",
       "  (pretrained_model): EsmForTokenClassification(\n",
       "    (esm): EsmModel(\n",
       "      (embeddings): EsmEmbeddings(\n",
       "        (word_embeddings): Embedding(33, 480, padding_idx=1)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (position_embeddings): Embedding(1026, 480, padding_idx=1)\n",
       "      )\n",
       "      (encoder): EsmEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x EsmLayer(\n",
       "            (attention): EsmAttention(\n",
       "              (self): EsmSelfAttention(\n",
       "                (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (rotary_embeddings): RotaryEmbedding()\n",
       "              )\n",
       "              (output): EsmSelfOutput(\n",
       "                (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (intermediate): EsmIntermediate(\n",
       "              (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "            )\n",
       "            (output): EsmOutput(\n",
       "              (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (emb_layer_norm_after): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (contact_head): EsmContactPredictionHead(\n",
       "        (regression): Linear(in_features=240, out_features=1, bias=True)\n",
       "        (activation): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (classifier): Linear(in_features=480, out_features=4, bias=True)\n",
       "  )\n",
       "  (conv1): Conv1d(1, 64, kernel_size=(5,), stride=(1,))\n",
       "  (conv2): Conv1d(64, 128, kernel_size=(5,), stride=(1,))\n",
       "  (fc1): Linear(in_features=130048, out_features=32, bias=True)\n",
       "  (classifier): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_classifier = Dpo_classifier(esm2_finetuned) # Create an instance of Dpo_classifier\n",
    "model_classifier.load_state_dict(torch.load(DpoDetection_path), strict = False) # Load the saved weights ; weird Error with some of the keys \n",
    "model_classifier.eval() # Set the model to evaluation mode for inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(model, sequence):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sequence = [sequence]  # Wrap the sequence in a list to match the model's input format\n",
    "        outputs, sequence_outputs = model(sequence)\n",
    "        probas = torch.sigmoid(outputs)  # Apply sigmoid activation for binary classification\n",
    "        predictions = (probas > 0.5).float()  # Convert probabilities to binary predictions\n",
    "        sequence_outputs_list = sequence_outputs.cpu().numpy().tolist()[0][0]\n",
    "        prob_predicted = probas[0].item()\n",
    "        return (predictions.item(), prob_predicted), sequence_outputs_list\n",
    "\n",
    "\n",
    "def plot_token(tokens) :\n",
    "    tokens = np.array(tokens)  # convert your list to numpy array for convenience\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for i in range(len(tokens) - 1):\n",
    "        if tokens[i] == 0:\n",
    "            color = 'black'\n",
    "        elif tokens[i] == 1:\n",
    "            color = 'blue'\n",
    "        elif tokens[i] == 2:\n",
    "            color = 'red'\n",
    "        else :\n",
    "            color = 'green'\n",
    "        plt.plot([i, i+1], [tokens[i], tokens[i+1]], color=color, marker='o')\n",
    "    plt.xlabel('Token')\n",
    "    plt.ylabel('Label')\n",
    "    plt.title('Label for each token')\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.yticks(np.arange(2), ['0', '1'])  \n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Predictions Ferriol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Make predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "132it [02:03,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "from tqdm import tqdm \n",
    "from collections import Counter\n",
    "\n",
    "path_out = \"/media/concha-eloko/Linux/77_strains_phage_project\"\n",
    "\n",
    "prediction_results = {}\n",
    "path_fasta = f\"{path_out}/all_dpos.77_phages.multi.fasta\"\n",
    "fastas = SeqIO.parse(f\"{path_fasta}\" , \"fasta\")\n",
    "tmp_results = []\n",
    "for record in tqdm(fastas) :\n",
    "    if len(record.seq) >= 200 :\n",
    "        protein_seq = record.seq \n",
    "        prediction, sequence_outputs = predict_sequence(model_classifier, str(protein_seq))\n",
    "        if record.description.count(\",\") == 0 :\n",
    "            prot_id = record.description\n",
    "        else :\n",
    "            prot_id = \"_\".join(record.description.split(\",\")[0].split(\" \"))\n",
    "            pass\n",
    "        if prediction[0] == 1 :\n",
    "            a = (prot_id , dict(Counter(sequence_outputs)))\n",
    "            tmp_results.append(a)\n",
    "        else :\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('K10PH82C1_cds_50', {0.0: 501, 1.0: 523}),\n",
       " ('K10PH82C1_cds_51', {0.0: 685, 1.0: 339}),\n",
       " ('K11PH164C1_cds_45', {0.0: 694, 1.0: 330}),\n",
       " ('K11PH164C1_cds_46', {0.0: 595, 1.0: 429}),\n",
       " ('K13PH07C1L_cds_10', {0.0: 626, 1.0: 398}),\n",
       " ('K13PH07C1L_cds_11', {0.0: 917, 1.0: 107}),\n",
       " ('K13PH07C1L_cds_12', {0.0: 810, 1.0: 214}),\n",
       " ('K13PH07C1S_cds_10', {0.0: 626, 1.0: 398}),\n",
       " ('K13PH07C1S_cds_11', {0.0: 648, 1.0: 376}),\n",
       " ('K14PH164C1_cds_24', {0.0: 534, 1.0: 490}),\n",
       " ('K15PH90_cds_55', {1.0: 543, 0.0: 481}),\n",
       " ('K16PH164C3_cds_48', {0.0: 534, 1.0: 490}),\n",
       " ('K17alfa61_cds_23', {0.0: 847, 1.0: 177}),\n",
       " ('K17alfa62_cds_64', {0.0: 603, 1.0: 421}),\n",
       " ('K17alfa62_cds_66', {0.0: 614, 1.0: 410}),\n",
       " ('K18PH07C1_cds_243', {0.0: 701, 1.0: 323}),\n",
       " ('K18PH07C1_cds_245', {0.0: 716, 1.0: 308}),\n",
       " ('K1PH164C1_cds_8', {0.0: 630, 1.0: 394}),\n",
       " ('K21lambda1_cds_28', {0.0: 815, 2.0: 209}),\n",
       " ('K22PH164C1_cds_10', {0.0: 661, 1.0: 363}),\n",
       " ('K22PH164C1_cds_11', {0.0: 688, 1.0: 336}),\n",
       " ('K23PH08C2_cds_233', {0.0: 677, 1.0: 347}),\n",
       " ('K24PH164C1_cds_8', {0.0: 707, 1.0: 317}),\n",
       " ('K25PH129C1_cds_60', {0.0: 854, 1.0: 170}),\n",
       " ('K26PH128C1_cds_49', {0.0: 663, 1.0: 361}),\n",
       " ('K26PH128C1_cds_50', {0.0: 692, 1.0: 332}),\n",
       " ('K27PH129C1_cds_48', {0.0: 626, 1.0: 398}),\n",
       " ('K2PH164C1_cds_23', {0.0: 618, 1.0: 406}),\n",
       " ('K2PH164C2_cds_24', {0.0: 692, 1.0: 332}),\n",
       " ('K2alfa62_cds_23', {0.0: 621, 1.0: 403}),\n",
       " ('K35PH164C3_cds_48', {0.0: 682, 1.0: 342}),\n",
       " ('K37PH164C1_cds_47', {0.0: 854, 1.0: 170}),\n",
       " ('K37PH164C1_cds_48', {0.0: 681, 1.0: 343}),\n",
       " ('K38PH09C2_cds_24', {0.0: 541, 1.0: 483}),\n",
       " ('K39PH122C2_cds_8', {0.0: 620, 1.0: 404}),\n",
       " ('K39PH122C2_cds_55', {0.0: 848, 1.0: 176}),\n",
       " ('K40PH129C1_cds_56', {0.0: 381, 1.0: 643}),\n",
       " ('K41P2_cds_11', {0.0: 806, 2.0: 218}),\n",
       " ('K43PH164C1_cds_40', {0.0: 764, 1.0: 260}),\n",
       " ('K43PH164C1_cds_41', {0.0: 637, 1.0: 387}),\n",
       " ('K44PH129C1_cds_9', {0.0: 671, 1.0: 353}),\n",
       " ('K44PH129C1_cds_10', {0.0: 532, 1.0: 492}),\n",
       " ('K45PH128C2_cds_237', {0.0: 651, 1.0: 373}),\n",
       " ('K45PH128C2_cds_239', {0.0: 721, 1.0: 303}),\n",
       " ('K46PH129_cds_24', {0.0: 631, 1.0: 393}),\n",
       " ('K48PH164C1_cds_49', {0.0: 693, 1.0: 331}),\n",
       " ('K4PH164_cds_22', {0.0: 659, 2.0: 365}),\n",
       " ('K51PH129C1_cds_9', {0.0: 322, 1.0: 702}),\n",
       " ('K53PH164C2_cds_24', {0.0: 628, 1.0: 396}),\n",
       " ('K54lambda1_1_1_cds_238', {0.0: 703, 1.0: 321}),\n",
       " ('K54lambda2_cds_23', {0.0: 659, 1.0: 365}),\n",
       " ('K56PH164C1_cds_48', {0.0: 607, 1.0: 417}),\n",
       " ('K56PH164C1_cds_49', {0.0: 652, 1.0: 372}),\n",
       " ('K57lambda1_2_cds_92', {0.0: 633, 1.0: 391}),\n",
       " ('K57lambda1_2_cds_93', {0.0: 554, 1.0: 470}),\n",
       " ('K58PH129C2_cds_47', {0.0: 579, 1.0: 445}),\n",
       " ('K5lambda5_cds_196', {0.0: 743, 1.0: 281}),\n",
       " ('K5lambda5_cds_198', {0.0: 520, 1.0: 504}),\n",
       " ('K5lambda5_cds_199', {0.0: 502, 1.0: 522}),\n",
       " ('K5lambda5_cds_200', {0.0: 691, 1.0: 333}),\n",
       " ('K60PH164C1_cds_94', {0.0: 659, 1.0: 365}),\n",
       " ('K60PH164C1_cds_96', {0.0: 680, 1.0: 344}),\n",
       " ('K61PH164C1_cds_9', {0.0: 624, 1.0: 400}),\n",
       " ('K61PH164C1_cds_10', {0.0: 745, 1.0: 279}),\n",
       " ('K63PH128_cds_22', {0.0: 577, 1.0: 447}),\n",
       " ('K64PH164C4_cds_24', {0.0: 595, 1.0: 429}),\n",
       " ('K65PH164_cds_12', {0.0: 802, 2.0: 222}),\n",
       " ('K66PH128C1_cds_59', {0.0: 617, 1.0: 407}),\n",
       " ('K6PH25C3_cds_23', {0.0: 725, 1.0: 299}),\n",
       " ('K71PH129C1_cds_55', {0.0: 722, 1.0: 302}),\n",
       " ('K74PH129C2_cds_51', {0.0: 663, 1.0: 361}),\n",
       " ('K74PH129C2_cds_52', {0.0: 613, 1.0: 411}),\n",
       " ('K80PH1317a_cds_53', {0.0: 973, 1.0: 51}),\n",
       " ('K80PH1317a_cds_54', {0.0: 790, 1.0: 234}),\n",
       " ('K80PH1317b_cds_53', {0.0: 950, 1.0: 74}),\n",
       " ('K80PH1317b_cds_54', {0.0: 790, 1.0: 234}),\n",
       " ('K82P1_cds_45', {0.0: 606, 1.0: 418}),\n",
       " ('K82P1_cds_46', {0.0: 666, 1.0: 358}),\n",
       " ('K8PH128_cds_46', {0.0: 704, 1.0: 320}),\n",
       " ('K10PH82C1_cds_45', {0.0: 740, 2.0: 284}),\n",
       " ('K11PH164C1_cds_39', {0.0: 738, 2.0: 286}),\n",
       " ('K12P1_1_cds_43', {0.0: 741, 2.0: 283}),\n",
       " ('K13PH07C1L_cds_54', {0.0: 755, 2.0: 269}),\n",
       " ('K13PH07C1S_cds_53', {0.0: 754, 2.0: 270}),\n",
       " ('K15PH90_cds_49', {0.0: 741, 2.0: 283}),\n",
       " ('K16PH164C3_cds_43', {0.0: 737, 2.0: 287}),\n",
       " ('K19PH14C4P1_cds_43', {0.0: 740, 2.0: 284}),\n",
       " ('K1PH164C1_cds_53', {0.0: 748, 2.0: 276}),\n",
       " ('K2069PH1_cds_25', {0.0: 969, 1.0: 55}),\n",
       " ('K22PH164C1_cds_50', {0.0: 737, 2.0: 287}),\n",
       " ('K24PH164C1_cds_55', {0.0: 740, 2.0: 284}),\n",
       " ('K25PH129C1_cds_56', {0.0: 749, 2.0: 275}),\n",
       " ('K26PH128C1_cds_44', {0.0: 738, 2.0: 286}),\n",
       " ('K27PH129C1_cds_43', {0.0: 740, 2.0: 284}),\n",
       " ('K35PH164C3_cds_43', {0.0: 739, 2.0: 285}),\n",
       " ('K37PH164C1_cds_41', {0.0: 738, 2.0: 286}),\n",
       " ('K39PH122C2_cds_50', {0.0: 742, 2.0: 282}),\n",
       " ('K40PH129C1_cds_52', {0.0: 741, 2.0: 283}),\n",
       " ('K42PH8_cds_43', {0.0: 739, 2.0: 285}),\n",
       " ('K42PH8_cds_48', {0.0: 580, 1.0: 444}),\n",
       " ('K43PH164C1_cds_35', {0.0: 737, 2.0: 287}),\n",
       " ('K44PH129C1_cds_45', {0.0: 890, 2.0: 134}),\n",
       " ('K48PH164C1_cds_43', {0.0: 738, 2.0: 286}),\n",
       " ('K51PH129C1_cds_56', {0.0: 743, 2.0: 281}),\n",
       " ('K56PH164C1_cds_43', {0.0: 737, 2.0: 287}),\n",
       " ('K58PH129C2_cds_40', {0.0: 738, 2.0: 286}),\n",
       " ('K59PH2_cds_46', {0.0: 681, 2.0: 343}),\n",
       " ('K61PH164C1_cds_52', {0.0: 755, 2.0: 269}),\n",
       " ('K66PH128C1_cds_55', {0.0: 741, 2.0: 283}),\n",
       " ('K71PH129C1_cds_51', {0.0: 742, 2.0: 282}),\n",
       " ('K72PH164C2_cds_51', {0.0: 759, 2.0: 265}),\n",
       " ('K74PH129C2_cds_46', {0.0: 737, 2.0: 287}),\n",
       " ('K80PH1317a_cds_47', {0.0: 736, 2.0: 288}),\n",
       " ('K80PH1317b_cds_47', {0.0: 736, 2.0: 288}),\n",
       " ('K82P1_cds_40', {0.0: 737, 2.0: 287}),\n",
       " ('K8PH128_cds_41', {0.0: 738, 2.0: 286}),\n",
       " ('K2069PH1_cds_25', {0.0: 969, 1.0: 55})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'K10PH82C1_cds_50': 'right-handed beta-helix',\n",
       " 'K10PH82C1_cds_51': 'right-handed beta-helix',\n",
       " 'K11PH164C1_cds_45': 'right-handed beta-helix',\n",
       " 'K11PH164C1_cds_46': 'right-handed beta-helix',\n",
       " 'K13PH07C1L_cds_10': 'right-handed beta-helix',\n",
       " 'K13PH07C1L_cds_11': 'right-handed beta-helix',\n",
       " 'K13PH07C1L_cds_12': 'right-handed beta-helix',\n",
       " 'K13PH07C1S_cds_10': 'right-handed beta-helix',\n",
       " 'K13PH07C1S_cds_11': 'right-handed beta-helix',\n",
       " 'K14PH164C1_cds_24': 'right-handed beta-helix',\n",
       " 'K15PH90_cds_55': 'right-handed beta-helix',\n",
       " 'K16PH164C3_cds_48': 'right-handed beta-helix',\n",
       " 'K17alfa61_cds_23': 'right-handed beta-helix',\n",
       " 'K17alfa62_cds_64': 'right-handed beta-helix',\n",
       " 'K17alfa62_cds_66': 'right-handed beta-helix',\n",
       " 'K18PH07C1_cds_243': 'right-handed beta-helix',\n",
       " 'K18PH07C1_cds_245': 'right-handed beta-helix',\n",
       " 'K1PH164C1_cds_8': 'right-handed beta-helix',\n",
       " 'K21lambda1_cds_28': '6-bladed beta-propeller',\n",
       " 'K22PH164C1_cds_10': 'right-handed beta-helix',\n",
       " 'K22PH164C1_cds_11': 'right-handed beta-helix',\n",
       " 'K23PH08C2_cds_233': 'right-handed beta-helix',\n",
       " 'K24PH164C1_cds_8': 'right-handed beta-helix',\n",
       " 'K25PH129C1_cds_60': 'right-handed beta-helix',\n",
       " 'K26PH128C1_cds_49': 'right-handed beta-helix',\n",
       " 'K26PH128C1_cds_50': 'right-handed beta-helix',\n",
       " 'K27PH129C1_cds_48': 'right-handed beta-helix',\n",
       " 'K2PH164C1_cds_23': 'right-handed beta-helix',\n",
       " 'K2PH164C2_cds_24': 'right-handed beta-helix',\n",
       " 'K2alfa62_cds_23': 'right-handed beta-helix',\n",
       " 'K35PH164C3_cds_48': 'right-handed beta-helix',\n",
       " 'K37PH164C1_cds_47': 'right-handed beta-helix',\n",
       " 'K37PH164C1_cds_48': 'right-handed beta-helix',\n",
       " 'K38PH09C2_cds_24': 'right-handed beta-helix',\n",
       " 'K39PH122C2_cds_8': 'right-handed beta-helix',\n",
       " 'K39PH122C2_cds_55': 'right-handed beta-helix',\n",
       " 'K40PH129C1_cds_56': 'right-handed beta-helix',\n",
       " 'K41P2_cds_11': '6-bladed beta-propeller',\n",
       " 'K43PH164C1_cds_40': 'right-handed beta-helix',\n",
       " 'K43PH164C1_cds_41': 'right-handed beta-helix',\n",
       " 'K44PH129C1_cds_9': 'right-handed beta-helix',\n",
       " 'K44PH129C1_cds_10': 'right-handed beta-helix',\n",
       " 'K45PH128C2_cds_237': 'right-handed beta-helix',\n",
       " 'K45PH128C2_cds_239': 'right-handed beta-helix',\n",
       " 'K46PH129_cds_24': 'right-handed beta-helix',\n",
       " 'K48PH164C1_cds_49': 'right-handed beta-helix',\n",
       " 'K4PH164_cds_22': '6-bladed beta-propeller',\n",
       " 'K51PH129C1_cds_9': 'right-handed beta-helix',\n",
       " 'K53PH164C2_cds_24': 'right-handed beta-helix',\n",
       " 'K54lambda1_1_1_cds_238': 'right-handed beta-helix',\n",
       " 'K54lambda2_cds_23': 'right-handed beta-helix',\n",
       " 'K56PH164C1_cds_48': 'right-handed beta-helix',\n",
       " 'K56PH164C1_cds_49': 'right-handed beta-helix',\n",
       " 'K57lambda1_2_cds_92': 'right-handed beta-helix',\n",
       " 'K57lambda1_2_cds_93': 'right-handed beta-helix',\n",
       " 'K58PH129C2_cds_47': 'right-handed beta-helix',\n",
       " 'K5lambda5_cds_196': 'right-handed beta-helix',\n",
       " 'K5lambda5_cds_198': 'right-handed beta-helix',\n",
       " 'K5lambda5_cds_199': 'right-handed beta-helix',\n",
       " 'K5lambda5_cds_200': 'right-handed beta-helix',\n",
       " 'K60PH164C1_cds_94': 'right-handed beta-helix',\n",
       " 'K60PH164C1_cds_96': 'right-handed beta-helix',\n",
       " 'K61PH164C1_cds_9': 'right-handed beta-helix',\n",
       " 'K61PH164C1_cds_10': 'right-handed beta-helix',\n",
       " 'K63PH128_cds_22': 'right-handed beta-helix',\n",
       " 'K64PH164C4_cds_24': 'right-handed beta-helix',\n",
       " 'K65PH164_cds_12': '6-bladed beta-propeller',\n",
       " 'K66PH128C1_cds_59': 'right-handed beta-helix',\n",
       " 'K6PH25C3_cds_23': 'right-handed beta-helix',\n",
       " 'K71PH129C1_cds_55': 'right-handed beta-helix',\n",
       " 'K74PH129C2_cds_51': 'right-handed beta-helix',\n",
       " 'K74PH129C2_cds_52': 'right-handed beta-helix',\n",
       " 'K80PH1317a_cds_53': 'right-handed beta-helix',\n",
       " 'K80PH1317a_cds_54': 'right-handed beta-helix',\n",
       " 'K80PH1317b_cds_53': 'right-handed beta-helix',\n",
       " 'K80PH1317b_cds_54': 'right-handed beta-helix',\n",
       " 'K82P1_cds_45': 'right-handed beta-helix',\n",
       " 'K82P1_cds_46': 'right-handed beta-helix',\n",
       " 'K8PH128_cds_46': 'right-handed beta-helix',\n",
       " 'K10PH82C1_cds_45': '6-bladed beta-propeller',\n",
       " 'K11PH164C1_cds_39': '6-bladed beta-propeller',\n",
       " 'K12P1_1_cds_43': '6-bladed beta-propeller',\n",
       " 'K13PH07C1L_cds_54': '6-bladed beta-propeller',\n",
       " 'K13PH07C1S_cds_53': '6-bladed beta-propeller',\n",
       " 'K15PH90_cds_49': '6-bladed beta-propeller',\n",
       " 'K16PH164C3_cds_43': '6-bladed beta-propeller',\n",
       " 'K19PH14C4P1_cds_43': '6-bladed beta-propeller',\n",
       " 'K1PH164C1_cds_53': '6-bladed beta-propeller',\n",
       " 'K2069PH1_cds_25': 'right-handed beta-helix',\n",
       " 'K22PH164C1_cds_50': '6-bladed beta-propeller',\n",
       " 'K24PH164C1_cds_55': '6-bladed beta-propeller',\n",
       " 'K25PH129C1_cds_56': '6-bladed beta-propeller',\n",
       " 'K26PH128C1_cds_44': '6-bladed beta-propeller',\n",
       " 'K27PH129C1_cds_43': '6-bladed beta-propeller',\n",
       " 'K35PH164C3_cds_43': '6-bladed beta-propeller',\n",
       " 'K37PH164C1_cds_41': '6-bladed beta-propeller',\n",
       " 'K39PH122C2_cds_50': '6-bladed beta-propeller',\n",
       " 'K40PH129C1_cds_52': '6-bladed beta-propeller',\n",
       " 'K42PH8_cds_43': '6-bladed beta-propeller',\n",
       " 'K42PH8_cds_48': 'right-handed beta-helix',\n",
       " 'K43PH164C1_cds_35': '6-bladed beta-propeller',\n",
       " 'K44PH129C1_cds_45': '6-bladed beta-propeller',\n",
       " 'K48PH164C1_cds_43': '6-bladed beta-propeller',\n",
       " 'K51PH129C1_cds_56': '6-bladed beta-propeller',\n",
       " 'K56PH164C1_cds_43': '6-bladed beta-propeller',\n",
       " 'K58PH129C2_cds_40': '6-bladed beta-propeller',\n",
       " 'K59PH2_cds_46': '6-bladed beta-propeller',\n",
       " 'K61PH164C1_cds_52': '6-bladed beta-propeller',\n",
       " 'K66PH128C1_cds_55': '6-bladed beta-propeller',\n",
       " 'K71PH129C1_cds_51': '6-bladed beta-propeller',\n",
       " 'K72PH164C2_cds_51': '6-bladed beta-propeller',\n",
       " 'K74PH129C2_cds_46': '6-bladed beta-propeller',\n",
       " 'K80PH1317a_cds_47': '6-bladed beta-propeller',\n",
       " 'K80PH1317b_cds_47': '6-bladed beta-propeller',\n",
       " 'K82P1_cds_40': '6-bladed beta-propeller',\n",
       " 'K8PH128_cds_41': '6-bladed beta-propeller'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds_label = {1.0 : \"right-handed beta-helix\", 2.0 : \"6-bladed beta-propeller\", 3.0 : \"triple-helix\"}\n",
    "fold_dpoes = {}\n",
    "\n",
    "for dpo in tmp_results :\n",
    "    for label in dpo[1] : \n",
    "        if label in folds_label :\n",
    "            fold = folds_label[label]\n",
    "            fold_dpoes[dpo[0]] = fold\n",
    "            break\n",
    "fold_dpoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"/media/concha-eloko/Linux/PPT_clean/in_vitro/Celia/dpos_folds.celia.tsv\", \"w\") as outfile : \n",
    "    outfile.write(f\"protein_id\\tFold\\n\")\n",
    "    for protein,fold in fold_dpoes.items():\n",
    "        outfile.write(f\"{protein}\\t{fold}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Save / Open predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "\n",
    "path_bea = \"/media/concha-eloko/Linux/PPT_clean/in_vitro/Bea\"\n",
    "\n",
    "#dpos = set([prot_id[1] for file in prediction_results for prot_id in prediction_results[file]])\n",
    "\n",
    "#with open(\"/media/concha-eloko/Linux/PPT_clean/in_vitro/Bea/DepoScope_predictions.tsv\", \"w\") as outfile : \n",
    "#    for dpo in dpos :\n",
    "#        outfile.write(dpo + \"\\n\")\n",
    "\n",
    "dpos = open(\"/media/concha-eloko/Linux/PPT_clean/in_vitro/Bea/DepoScope_predictions.tsv\").read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_work",
   "language": "python",
   "name": "ml_work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
