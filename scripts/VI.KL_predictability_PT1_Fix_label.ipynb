{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal here is to correct the label before the finetuning of ESM2\n",
    "***\n",
    "## I. Generate the DF_info and the DF_embeddings \n",
    "## II. Tryout multiple algoritms : \n",
    "> A. Ensemble method : Random Forest <br>\n",
    "> B. Ensemble method : XGboost (?) <br>\n",
    "> C. NN : Feed-forward Neural Network (FNN) <br>\n",
    "> D. NN : Recurrent Neural Networks ; LSTM <br>\n",
    "> E. NN : Graph Neural Networks  <br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_prophage_id[\"prophage\"] = df_prophage_id[\"prophage\"].apply(lambda x : x.split(\".fasta\")[0])\n",
    "df_prophage_id.columns = [\"prophage_id\", \"Phage\"]\n",
    "\n",
    "DF_penultimate = DF_penultimate.merge(df_prophage_id , on = \"Phage\")\n",
    "DF_penultimate.to_csv(f\"{path_work}/DF_Dpo.final.2705.tsv\", sep = \"\\t\" , header = True, index = False)\n",
    "\n",
    "embeddings_anubis = pd.read_csv(f\"{path_work}/anubis.esm2.embedding.csv\", sep = \",\" , header = None)\n",
    "# embeddings_anubis = embeddings_anubis.drop([1281] , axis = 1)\n",
    "# embeddings_anubis.to_csv(f\"{path_work}/anubis.esm2.embedding.csv\", sep = \",\" , header = None , index = False)\n",
    "\n",
    "embeddings_dpo_b1 = pd.read_csv(f\"{path_work}/Dpo.0805.embeddings.ultimate.csv\", sep = \",\" , header = None)\n",
    "\n",
    "embeddings_2705 = pd.concat([embeddings_dpo_b1 , embeddings_anubis], axis = 0)\n",
    "embeddings_2705.to_csv(f\"{path_work}/Dpo.2705.embeddings.ultimate.csv\", sep = \",\" , header = None , index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/concha-eloko/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os \n",
    "import pandas as pd\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_work = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "\n",
    "DF_info = pd.read_csv(f\"{path_work}/DF_Dpo.final.2705.tsv\", sep = \"\\t\" ,  header = 0 )\n",
    "DF_embeddings = pd.read_csv(f\"{path_work}/Dpo.2705.embeddings.ultimate.csv\", sep = \",\", header= None )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1271</th>\n",
       "      <th>1272</th>\n",
       "      <th>1273</th>\n",
       "      <th>1274</th>\n",
       "      <th>1275</th>\n",
       "      <th>1276</th>\n",
       "      <th>1277</th>\n",
       "      <th>1278</th>\n",
       "      <th>1279</th>\n",
       "      <th>1280</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ppt__2930</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-0.017329</td>\n",
       "      <td>0.012884</td>\n",
       "      <td>0.037123</td>\n",
       "      <td>-0.123747</td>\n",
       "      <td>0.004186</td>\n",
       "      <td>-0.061367</td>\n",
       "      <td>-0.056718</td>\n",
       "      <td>-0.037215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098806</td>\n",
       "      <td>0.012989</td>\n",
       "      <td>-0.001155</td>\n",
       "      <td>0.139749</td>\n",
       "      <td>-0.030987</td>\n",
       "      <td>0.059306</td>\n",
       "      <td>0.107041</td>\n",
       "      <td>-0.041463</td>\n",
       "      <td>-0.085581</td>\n",
       "      <td>0.114973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ppt__3300</td>\n",
       "      <td>0.004044</td>\n",
       "      <td>0.040011</td>\n",
       "      <td>-0.001234</td>\n",
       "      <td>-0.095745</td>\n",
       "      <td>-0.058056</td>\n",
       "      <td>-0.002394</td>\n",
       "      <td>0.007648</td>\n",
       "      <td>-0.059740</td>\n",
       "      <td>0.060850</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020369</td>\n",
       "      <td>0.016287</td>\n",
       "      <td>0.062586</td>\n",
       "      <td>-0.024336</td>\n",
       "      <td>0.019276</td>\n",
       "      <td>0.069623</td>\n",
       "      <td>0.035261</td>\n",
       "      <td>-0.118962</td>\n",
       "      <td>0.035672</td>\n",
       "      <td>0.085582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ppt__1182</td>\n",
       "      <td>0.018767</td>\n",
       "      <td>0.068116</td>\n",
       "      <td>-0.009109</td>\n",
       "      <td>-0.012598</td>\n",
       "      <td>-0.107001</td>\n",
       "      <td>0.011569</td>\n",
       "      <td>-0.030943</td>\n",
       "      <td>-0.045359</td>\n",
       "      <td>0.048923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014524</td>\n",
       "      <td>-0.024645</td>\n",
       "      <td>0.071878</td>\n",
       "      <td>0.018206</td>\n",
       "      <td>0.042790</td>\n",
       "      <td>0.088410</td>\n",
       "      <td>0.031970</td>\n",
       "      <td>-0.124592</td>\n",
       "      <td>0.070040</td>\n",
       "      <td>0.065348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ppt__3540</td>\n",
       "      <td>-0.028261</td>\n",
       "      <td>-0.047253</td>\n",
       "      <td>-0.027340</td>\n",
       "      <td>-0.052824</td>\n",
       "      <td>-0.089644</td>\n",
       "      <td>-0.023079</td>\n",
       "      <td>0.094861</td>\n",
       "      <td>0.026104</td>\n",
       "      <td>0.024001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051728</td>\n",
       "      <td>0.005634</td>\n",
       "      <td>-0.077874</td>\n",
       "      <td>0.030336</td>\n",
       "      <td>-0.037648</td>\n",
       "      <td>0.050625</td>\n",
       "      <td>0.046142</td>\n",
       "      <td>-0.158841</td>\n",
       "      <td>-0.007670</td>\n",
       "      <td>0.034556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ppt__942</td>\n",
       "      <td>0.014863</td>\n",
       "      <td>0.028030</td>\n",
       "      <td>0.014927</td>\n",
       "      <td>-0.025997</td>\n",
       "      <td>-0.096138</td>\n",
       "      <td>0.016290</td>\n",
       "      <td>0.015008</td>\n",
       "      <td>-0.066254</td>\n",
       "      <td>0.077959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008521</td>\n",
       "      <td>-0.019820</td>\n",
       "      <td>0.123201</td>\n",
       "      <td>-0.040306</td>\n",
       "      <td>0.030893</td>\n",
       "      <td>0.051362</td>\n",
       "      <td>0.047316</td>\n",
       "      <td>-0.102698</td>\n",
       "      <td>0.044830</td>\n",
       "      <td>0.084530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3603</th>\n",
       "      <td>anubis__304</td>\n",
       "      <td>0.006264</td>\n",
       "      <td>0.006471</td>\n",
       "      <td>-0.031665</td>\n",
       "      <td>0.078502</td>\n",
       "      <td>-0.131247</td>\n",
       "      <td>0.077167</td>\n",
       "      <td>0.043005</td>\n",
       "      <td>-0.183636</td>\n",
       "      <td>-0.022181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044299</td>\n",
       "      <td>-0.061847</td>\n",
       "      <td>0.017696</td>\n",
       "      <td>0.054798</td>\n",
       "      <td>-0.035830</td>\n",
       "      <td>-0.030202</td>\n",
       "      <td>0.039051</td>\n",
       "      <td>-0.127020</td>\n",
       "      <td>-0.113630</td>\n",
       "      <td>0.211258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3604</th>\n",
       "      <td>anubis__1273</td>\n",
       "      <td>-0.019114</td>\n",
       "      <td>0.063302</td>\n",
       "      <td>0.006635</td>\n",
       "      <td>-0.060343</td>\n",
       "      <td>-0.034054</td>\n",
       "      <td>-0.003895</td>\n",
       "      <td>0.033920</td>\n",
       "      <td>-0.080352</td>\n",
       "      <td>0.073579</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004504</td>\n",
       "      <td>-0.007906</td>\n",
       "      <td>0.075141</td>\n",
       "      <td>-0.052423</td>\n",
       "      <td>0.027127</td>\n",
       "      <td>0.073984</td>\n",
       "      <td>0.030664</td>\n",
       "      <td>-0.096409</td>\n",
       "      <td>0.011906</td>\n",
       "      <td>0.124885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3605</th>\n",
       "      <td>anubis__1311</td>\n",
       "      <td>0.051261</td>\n",
       "      <td>0.067942</td>\n",
       "      <td>0.005061</td>\n",
       "      <td>-0.019131</td>\n",
       "      <td>-0.060296</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.037515</td>\n",
       "      <td>-0.033887</td>\n",
       "      <td>0.091774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044678</td>\n",
       "      <td>0.052609</td>\n",
       "      <td>0.112994</td>\n",
       "      <td>-0.000592</td>\n",
       "      <td>0.027122</td>\n",
       "      <td>0.086020</td>\n",
       "      <td>0.013660</td>\n",
       "      <td>-0.055491</td>\n",
       "      <td>0.021665</td>\n",
       "      <td>0.049301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3606</th>\n",
       "      <td>anubis__1525</td>\n",
       "      <td>-0.010655</td>\n",
       "      <td>0.083864</td>\n",
       "      <td>0.009084</td>\n",
       "      <td>-0.042220</td>\n",
       "      <td>-0.066479</td>\n",
       "      <td>0.008724</td>\n",
       "      <td>0.010109</td>\n",
       "      <td>-0.078033</td>\n",
       "      <td>0.065285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020752</td>\n",
       "      <td>0.024543</td>\n",
       "      <td>0.071302</td>\n",
       "      <td>0.035980</td>\n",
       "      <td>0.012171</td>\n",
       "      <td>0.054399</td>\n",
       "      <td>0.032167</td>\n",
       "      <td>-0.151018</td>\n",
       "      <td>0.042541</td>\n",
       "      <td>0.035221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3607</th>\n",
       "      <td>anubis__794</td>\n",
       "      <td>0.003410</td>\n",
       "      <td>0.035987</td>\n",
       "      <td>-0.006781</td>\n",
       "      <td>-0.006114</td>\n",
       "      <td>-0.096747</td>\n",
       "      <td>0.055312</td>\n",
       "      <td>0.020362</td>\n",
       "      <td>-0.106669</td>\n",
       "      <td>0.007063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040264</td>\n",
       "      <td>-0.011636</td>\n",
       "      <td>0.056235</td>\n",
       "      <td>0.041319</td>\n",
       "      <td>-0.008228</td>\n",
       "      <td>0.036652</td>\n",
       "      <td>0.048385</td>\n",
       "      <td>-0.104287</td>\n",
       "      <td>0.016014</td>\n",
       "      <td>0.068212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3608 rows Ã— 1281 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5     \\\n",
       "0        ppt__2930 -0.000061 -0.017329  0.012884  0.037123 -0.123747   \n",
       "1        ppt__3300  0.004044  0.040011 -0.001234 -0.095745 -0.058056   \n",
       "2        ppt__1182  0.018767  0.068116 -0.009109 -0.012598 -0.107001   \n",
       "3        ppt__3540 -0.028261 -0.047253 -0.027340 -0.052824 -0.089644   \n",
       "4         ppt__942  0.014863  0.028030  0.014927 -0.025997 -0.096138   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "3603   anubis__304  0.006264  0.006471 -0.031665  0.078502 -0.131247   \n",
       "3604  anubis__1273 -0.019114  0.063302  0.006635 -0.060343 -0.034054   \n",
       "3605  anubis__1311  0.051261  0.067942  0.005061 -0.019131 -0.060296   \n",
       "3606  anubis__1525 -0.010655  0.083864  0.009084 -0.042220 -0.066479   \n",
       "3607   anubis__794  0.003410  0.035987 -0.006781 -0.006114 -0.096747   \n",
       "\n",
       "          6         7         8         9     ...      1271      1272  \\\n",
       "0     0.004186 -0.061367 -0.056718 -0.037215  ...  0.098806  0.012989   \n",
       "1    -0.002394  0.007648 -0.059740  0.060850  ... -0.020369  0.016287   \n",
       "2     0.011569 -0.030943 -0.045359  0.048923  ...  0.014524 -0.024645   \n",
       "3    -0.023079  0.094861  0.026104  0.024001  ...  0.051728  0.005634   \n",
       "4     0.016290  0.015008 -0.066254  0.077959  ...  0.008521 -0.019820   \n",
       "...        ...       ...       ...       ...  ...       ...       ...   \n",
       "3603  0.077167  0.043005 -0.183636 -0.022181  ...  0.044299 -0.061847   \n",
       "3604 -0.003895  0.033920 -0.080352  0.073579  ... -0.004504 -0.007906   \n",
       "3605  0.000984  0.037515 -0.033887  0.091774  ...  0.044678  0.052609   \n",
       "3606  0.008724  0.010109 -0.078033  0.065285  ...  0.020752  0.024543   \n",
       "3607  0.055312  0.020362 -0.106669  0.007063  ...  0.040264 -0.011636   \n",
       "\n",
       "          1273      1274      1275      1276      1277      1278      1279  \\\n",
       "0    -0.001155  0.139749 -0.030987  0.059306  0.107041 -0.041463 -0.085581   \n",
       "1     0.062586 -0.024336  0.019276  0.069623  0.035261 -0.118962  0.035672   \n",
       "2     0.071878  0.018206  0.042790  0.088410  0.031970 -0.124592  0.070040   \n",
       "3    -0.077874  0.030336 -0.037648  0.050625  0.046142 -0.158841 -0.007670   \n",
       "4     0.123201 -0.040306  0.030893  0.051362  0.047316 -0.102698  0.044830   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3603  0.017696  0.054798 -0.035830 -0.030202  0.039051 -0.127020 -0.113630   \n",
       "3604  0.075141 -0.052423  0.027127  0.073984  0.030664 -0.096409  0.011906   \n",
       "3605  0.112994 -0.000592  0.027122  0.086020  0.013660 -0.055491  0.021665   \n",
       "3606  0.071302  0.035980  0.012171  0.054399  0.032167 -0.151018  0.042541   \n",
       "3607  0.056235  0.041319 -0.008228  0.036652  0.048385 -0.104287  0.016014   \n",
       "\n",
       "          1280  \n",
       "0     0.114973  \n",
       "1     0.085582  \n",
       "2     0.065348  \n",
       "3     0.034556  \n",
       "4     0.084530  \n",
       "...        ...  \n",
       "3603  0.211258  \n",
       "3604  0.124885  \n",
       "3605  0.049301  \n",
       "3606  0.035221  \n",
       "3607  0.068212  \n",
       "\n",
       "[3608 rows x 1281 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### II. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, prophage_vocab_size, prophage_embedding_dim, ancestor_vocab_size, ancestor_embedding_dim, depolymerase_embedding_dim, lstm_hidden_dim, lstm_layers, output_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        # Embedding layers for the categorical inputs\n",
    "        self.prophage_embedding = nn.Embedding(prophage_vocab_size, prophage_embedding_dim)\n",
    "        self.ancestor_embedding = nn.Embedding(ancestor_vocab_size, ancestor_embedding_dim)\n",
    "        \n",
    "        # LSTM for the depolymerase embeddings\n",
    "        self.lstm = nn.LSTM(depolymerase_embedding_dim, lstm_hidden_dim, num_layers=lstm_layers, batch_first=True)\n",
    "\n",
    "        # Fully connected layer for output\n",
    "        self.fc = nn.Linear(prophage_embedding_dim + ancestor_embedding_dim + lstm_hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, prophage, ancestor, depolymerase):\n",
    "        prophage_embed = self.prophage_embedding(prophage)\n",
    "        ancestor_embed = self.ancestor_embedding(ancestor)\n",
    "\n",
    "        # The LSTM expects inputs of the form (batch_size, seq_length, num_features)\n",
    "        lstm_out, _ = self.lstm(depolymerase.unsqueeze(0))\n",
    "\n",
    "        # Concatenate the embeddings\n",
    "        x = torch.cat((prophage_embed, ancestor_embed, lstm_out[0, -1]), dim=-1)\n",
    "\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "search_space = {\n",
    "    'prophage_embedding_dim': Integer(10, 300),\n",
    "    'ancestor_embedding_dim': Integer(10, 300),\n",
    "    'depolymerase_embedding_dim': Integer(10, 300),\n",
    "    'lstm_hidden_dim': Integer(10, 300),\n",
    "    'lstm_layers': Integer(1, 3),\n",
    "    'output_dim': Integer(1, 10),\n",
    "    'learning_rate': Real(1e-6, 1e-2, prior='log-uniform'),\n",
    "}\n",
    "\n",
    "# Wrap your PyTorch model in a scikit-learn compatible estimator\n",
    "estimator = PyTorchEstimator(\n",
    "    build_fn=lambda: SimpleModel(prophage_vocab_size=PROPHAGE_VOCAB_SIZE, ancestor_vocab_size=ANCESTOR_VOCAB_SIZE),\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    optimizer=lambda params: torch.optim.Adam(params, lr=0.001),\n",
    "    train_split=None,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Initialize the BayesSearchCV object\n",
    "bayes_search = BayesSearchCV(estimator, search_space, n_iter=50, cv=3)\n",
    "\n",
    "# Perform the search\n",
    "bayes_search.fit([prophage_data, ancestor_data, depolymerase_data], labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophage_data = torch.tensor(prophage_data)  # Shape: [num_samples]\n",
    "ancestor_data = torch.tensor(ancestor_data)  # Shape: [num_samples]\n",
    "depolymerase_data = torch.tensor(depolymerase_data)  # Shape: [num_samples, seq_length, num_features]\n",
    "labels = torch.tensor(labels)  # Shape: [num_samples]\n",
    "\n",
    "# Create the model\n",
    "model = SimpleModel(prophage_vocab_size=PROPHAGE_VOCAB_SIZE, prophage_embedding_dim=PROPHAGE_EMBEDDING_DIM, ancestor_vocab_size=ANCESTOR_VOCAB_SIZE, ancestor_embedding_dim=ANCESTOR_EMBEDDING_DIM, depolymerase_embedding_dim=DEPOLYMERASE_EMBEDDING_DIM, lstm_hidden_dim=LSTM_HIDDEN_DIM, lstm_layers=LSTM_LAYERS, output_dim=OUTPUT_DIM)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(prophage_data, ancestor_data, depolymerase_data)\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, NUM_EPOCHS, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, depolymerase_test_data, labels_test_data, criterion):\n",
    "    model.eval()  # switch the model to evaluation mode\n",
    "    with torch.no_grad():  # don't compute gradients while evaluating\n",
    "        outputs_test = model(depolymerase_test_data)\n",
    "        loss_test = criterion(outputs_test, labels_test_data)\n",
    "    print('Test Loss: {:.4f}'.format(loss_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After initial training\n",
    "evaluate_model(model, depolymerase_test_data, labels_test_data, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transfer learning part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferLearningModel(nn.Module):\n",
    "    def __init__(self, pretrained_model, new_input_dim, new_output_dim):\n",
    "        super(TransferLearningModel, self).__init__()\n",
    "        \n",
    "        # Reuse the LSTM and fully connected layer from the pretrained model\n",
    "        self.lstm = pretrained_model.lstm\n",
    "        self.fc = nn.Linear(new_input_dim, new_output_dim)\n",
    "\n",
    "    def forward(self, depolymerase):\n",
    "        # Pass the data through the LSTM\n",
    "        lstm_out, _ = self.lstm(depolymerase.unsqueeze(0))\n",
    "\n",
    "        # Pass the output through the fully connected layer\n",
    "        return self.fc(lstm_out[0, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(transfer_learning_model, depolymerase_test_data, labels_test_data, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "> Graph Convolutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=-UjytpbqX4A <br>\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/notes/heterogeneous.html<br>\n",
    "https://www.youtube.com/watch?v=-UjytpbqX4A<br>\n",
    "Inputs Explanation:\n",
    "\n",
    "    data.x: This should be a [num_nodes, num_node_features] matrix containing your node features. In your case, these are the amino acid sequence and embedding representation of the depolymerase sequences.\n",
    "    \n",
    "    data.edge_index: This is your graph adjacency matrix, represented as a [2, num_edges] matrix where each column represents the two nodes that define an edge. The entries in edge_index are the indices of the nodes in the node feature matrix data.x.\n",
    "    \n",
    "    data.y: This should be a [num_nodes] vector containing your target labels. In your case, these are the true KL_type of each node.\n",
    "    \n",
    "    data.train_mask: This should be a [num_nodes] boolean vector indicating which nodes should be used for training. You could create training, validation, and test masks depending on how you want to split your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Define a Graph Convolutional Network\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x  # This is now your embedding\n",
    "\n",
    "# Define a Classifier\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.fc(x), dim=1)\n",
    "\n",
    "# Bayesian search\n",
    "def objective(trial):\n",
    "    # Suggest values of the hyperparameters using a trial object.\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "    layers = trial.suggest_int('layers', 1, 3)\n",
    "\n",
    "    # Define and train your model here\n",
    "    gcn_model = GCN(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, layers=layers)\n",
    "    classifier_model = Classifier(input_dim=HIDDEN_DIM, output_dim=OUTPUT_DIM)\n",
    "\n",
    "    gcn_optimizer = torch.optim.Adam(gcn_model.parameters(), lr=lr)\n",
    "    classifier_optimizer = torch.optim.Adam(classifier_model.parameters(), lr=lr)\n",
    "\n",
    "    # The rest of your training code here...\n",
    "    # Be sure to return a value to minimize (e.g., validation loss)\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(' Value: ', trial.value)\n",
    "print(' Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print('    {}: {}'.format(key, value))\n",
    "\n",
    "# Instantiate the models\n",
    "gcn_model = GCN(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM)\n",
    "classifier_model = Classifier(input_dim=HIDDEN_DIM, output_dim=OUTPUT_DIM)\n",
    "\n",
    "# Define your optimizer and loss function for each model\n",
    "gcn_optimizer = torch.optim.Adam(gcn_model.parameters(), lr=0.01)\n",
    "classifier_optimizer = torch.optim.Adam(classifier_model.parameters(), lr=0.01)\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop for GCN\n",
    "for epoch in range(200): # adjust number of epochs as needed\n",
    "    gcn_model.train()\n",
    "    gcn_optimizer.zero_grad()\n",
    "    embeddings = gcn_model(data)  # forward pass\n",
    "\n",
    "    # We don't have a loss for the GCN itself, because it's unsupervised. We just need the embeddings\n",
    "\n",
    "# Training loop for Classifier\n",
    "for epoch in range(200): # adjust number of epochs as needed\n",
    "    classifier_model.train()\n",
    "    classifier_optimizer.zero_grad()\n",
    "\n",
    "    out = classifier_model(embeddings)  # forward pass using the embeddings as input\n",
    "\n",
    "    # Compute the loss only for training nodes\n",
    "    loss = loss_func(out[data.train_mask], data.y[data.train_mask]) \n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    classifier_optimizer.step()\n",
    "\n",
    "    # Print loss\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With heterogenous nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "# Create feature matrices for prophages and bacteria nodes\n",
    "prophage_features = torch.randn((num_prophages, num_prophage_features))  # Including depolymerase embeddings\n",
    "bacteria_features = torch.randn((num_bacteria, num_bacteria_features))  # Including KL_type\n",
    "\n",
    "# Create edge index for infection events\n",
    "edge_index = torch.tensor([\n",
    "    prophage_indices,  # Source nodes (prophages)\n",
    "    bacteria_indices,  # Target nodes (bacteria)\n",
    "], dtype=torch.long)\n",
    "\n",
    "# Create data object\n",
    "data = Data(\n",
    "    x={'prophage': prophage_features, 'bacteria': bacteria_features},\n",
    "    edge_index={'infection': edge_index},\n",
    "    # Add any additional data here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "        self.conv_prophage = GCNConv(input_dim['prophage'], hidden_dim)\n",
    "        self.conv_bacteria = GCNConv(input_dim['bacteria'], hidden_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x['prophage'] = self.conv_prophage(x['prophage'], edge_index['infection'])\n",
    "        x['bacteria'] = self.conv_bacteria(x['bacteria'], edge_index['infection'])\n",
    "\n",
    "        return x  # This is now your embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Instantiate the model with input, hidden and output dimensions\n",
    "model = GCN(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, output_dim=OUTPUT_DIM)\n",
    "\n",
    "# Define your optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(200): # adjust number of epochs as needed\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data) # forward pass\n",
    "\n",
    "    # Compute the loss only for training nodes\n",
    "    loss = loss_func(out[data.train_mask], data.y[data.train_mask]) \n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class GCNWithEmbeddings(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCNWithEmbeddings, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)  # The output is now an embedding\n",
    "        self.classifier = torch.nn.Linear(hidden_dim, output_dim)  # This is a separate layer for classification\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)  # This is now your embedding\n",
    "\n",
    "        out = self.classifier(x)  # This is your KL_type prediction\n",
    "\n",
    "        return out, x  # Return both the prediction and the embedding\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "sklearn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
