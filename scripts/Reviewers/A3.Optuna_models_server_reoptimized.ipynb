{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f094585-b4f3-409f-b1d1-dcd2ba6cb5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData, DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import HeteroConv , GATv2Conv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder , label_binarize , OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score , matthews_corrcoef\n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import random\n",
    "from collections import Counter\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import warnings\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# TropiGAT modules\n",
    "import TropiGAT_graph\n",
    "import TropiGAT_models\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# *****************************************************************************\n",
    "# Load the Dataframes :\n",
    "path_work = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023/train_nn\"\n",
    "\n",
    "DF_info = pd.read_csv(f\"{path_work}/TropiGATv2.final_df_v2.tsv\", sep = \"\\t\" ,  header = 0)\n",
    "DF_info = DF_info.drop_duplicates(subset = [\"Protein_name\"])\n",
    "DF_info = DF_info[~DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "\n",
    "df_prophages = DF_info.drop_duplicates(subset = [\"Phage\"], keep = \"first\")\n",
    "dico_prophage_info = {row[\"Phage\"] : {\"prophage_strain\" : row[\"prophage_id\"] , \"ancestor\" : row[\"Infected_ancestor\"], \"KL_type\" : row[\"KL_type_LCA\"]} for _,row in df_prophages.iterrows()}\n",
    "\n",
    "# *****************************************************************************\n",
    "# The model : Classifier\n",
    "class TropiGAT_small_module(torch.nn.Module):\n",
    "    def __init__(self,hidden_channels, heads, edge_type = (\"B2\", \"expressed\", \"B1\") ,dropout = 0.2, conv = GATv2Conv):\n",
    "        super().__init__()\n",
    "        # GATv2 module :\n",
    "        self.conv = conv((-1,-1), hidden_channels, add_self_loops = False, heads = heads, dropout = dropout, shared_weights = True)\n",
    "        self.hetero_conv = HeteroConv({edge_type: self.conv})\n",
    "        # FNN layers : \n",
    "        self.linear_layers = nn.Sequential(nn.Linear(heads*hidden_channels, 1280),\n",
    "                                           nn.BatchNorm1d(1280),\n",
    "                                           nn.LeakyReLU(),\n",
    "                                           torch.nn.Dropout(dropout),\n",
    "                                           nn.Linear(1280, 480),\n",
    "                                           nn.BatchNorm1d(480),\n",
    "                                           nn.LeakyReLU(),\n",
    "                                           torch.nn.Dropout(dropout),\n",
    "                                           nn.Linear(480 , 1))\n",
    "        \n",
    "    def forward(self, graph_data):\n",
    "        x_B1_dict  = self.hetero_conv(graph_data.x_dict, graph_data.edge_index_dict)\n",
    "        x = self.linear_layers(x_B1_dict[\"B1\"])\n",
    "        return x.view(-1) \n",
    "\n",
    "# *****************************************************************************\n",
    "# Pre-process data :\n",
    "# First filtration step :\n",
    "def get_filtered_prophages(prophage) :\n",
    "    combinations = []\n",
    "    to_exclude = set()\n",
    "    to_keep = set()\n",
    "    to_keep.add(prophage)\n",
    "    df_prophage_group = DF_info[(DF_info[\"prophage_id\"] == dico_prophage_info[prophage][\"prophage_strain\"]) & (DF_info[\"Infected_ancestor\"] == dico_prophage_info[prophage][\"ancestor\"])]\n",
    "    if len(df_prophage_group) == 1 :\n",
    "        pass\n",
    "    else :\n",
    "        depo_set = set(df_prophage_group[df_prophage_group[\"Phage\"] == prophage][\"domain_seq\"].values)\n",
    "        for prophage_tmp in df_prophage_group[\"Phage\"].unique().tolist() :\n",
    "            if prophage_tmp != prophage :\n",
    "                tmp_depo_set = set(df_prophage_group[df_prophage_group[\"Phage\"] == prophage_tmp][\"domain_seq\"].values)\n",
    "                if depo_set == tmp_depo_set :\n",
    "                    to_exclude.add(prophage_tmp)\n",
    "                else :\n",
    "                    if tmp_depo_set not in combinations :\n",
    "                        to_keep.add(prophage_tmp)\n",
    "                        combinations.append(tmp_depo_set)\n",
    "                    else :\n",
    "                        to_exclude.add(prophage_tmp)\n",
    "    return df_prophage_group , to_exclude , to_keep\n",
    "\n",
    "good_prophages = set()\n",
    "excluded_prophages = set()\n",
    "\n",
    "for prophage, info_prophage in tqdm(dico_prophage_info.items()) :\n",
    "    if prophage not in excluded_prophages and prophage not in good_prophages:\n",
    "        _, excluded_members , kept_members = get_filtered_prophages(prophage)\n",
    "        good_prophages.update(kept_members)\n",
    "        excluded_prophages.update(excluded_members)\n",
    "\n",
    "DF_info_lvl_0_filtered = DF_info[DF_info[\"Phage\"].isin(good_prophages)]\n",
    "DF_info_lvl_0_final = DF_info_lvl_0_filtered[~DF_info_lvl_0_filtered[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "\n",
    "\n",
    "# Second filtration step :\n",
    "duplicate_prophage = []\n",
    "dico_kltype_duplica = {}\n",
    "for kltype in DF_info_lvl_0_final[\"KL_type_LCA\"].unique():\n",
    "    df_kl = DF_info_lvl_0_final[DF_info_lvl_0_final[\"KL_type_LCA\"] == kltype][[\"Phage\", \"Protein_name\", \"KL_type_LCA\", \"Infected_ancestor\", \"index\", \"seq\", \"domain_seq\"]]\n",
    "    prophages_tmp_list = df_kl[\"Phage\"].unique().tolist()\n",
    "    set_sets_depo = []\n",
    "    duplicated = {}  \n",
    "    for prophage_tmp in prophages_tmp_list: \n",
    "        set_depo = frozenset(df_kl[df_kl[\"Phage\"] == prophage_tmp][\"domain_seq\"].values)\n",
    "        for past_set in set_sets_depo:\n",
    "            if past_set == set_depo:\n",
    "                duplicated[past_set] = duplicated.get(past_set, 0) + 1\n",
    "                duplicate_prophage.append(prophage_tmp)\n",
    "                break\n",
    "        else:\n",
    "            set_sets_depo.append(set_depo)\n",
    "            duplicated[set_depo] = 1\n",
    "    dico_kltype_duplica[kltype] = duplicated\n",
    "    \n",
    "DF_info_lvl_0_final_ultrafiltered = DF_info_lvl_0_final[~DF_info_lvl_0_final[\"Phage\"].isin(duplicate_prophage)]\n",
    "DF_info_lvl_0 = DF_info_lvl_0_final_ultrafiltered.copy()\n",
    "\n",
    "# Input graph: \n",
    "# graph_baseline , dico_prophage_kltype_associated = TropiGAT_graph.build_graph_baseline(DF_info_lvl_0)\n",
    "# graph_data_kltype = TropiGAT_graph.build_graph_masking_v2(graph_baseline , dico_prophage_kltype_associated, DF_info_lvl_0, KL_type, 5, 0.7, 0.2, 0.1, seed = seed)\n",
    "\n",
    "# *****************************************************************************\n",
    "# Training :\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class TropiGAT_small_module(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, heads, edge_type=(\"B2\", \"expressed\", \"B1\"), dropout=0.2, conv=GATv2Conv):\n",
    "        super().__init__()\n",
    "        # GATv2 module :\n",
    "        self.conv = conv((-1, -1), hidden_channels, add_self_loops=False, heads=heads, dropout=dropout, shared_weights=True)\n",
    "        self.hetero_conv = HeteroConv({edge_type: self.conv})\n",
    "        # FNN layers :\n",
    "        self.linear_layers = nn.Sequential(nn.Linear(heads*hidden_channels, 1280),\n",
    "                                           nn.BatchNorm1d(1280),\n",
    "                                           nn.LeakyReLU(),\n",
    "                                           torch.nn.Dropout(dropout),\n",
    "                                           nn.Linear(1280, 480),\n",
    "                                           nn.BatchNorm1d(480),\n",
    "                                           nn.LeakyReLU(),\n",
    "                                           torch.nn.Dropout(dropout),\n",
    "                                           nn.Linear(480, 1))\n",
    "\n",
    "    def forward(self, graph_data):\n",
    "        x_B1_dict = self.hetero_conv(graph_data.x_dict, graph_data.edge_index_dict)\n",
    "        x = self.linear_layers(x_B1_dict[\"B1\"])\n",
    "        return x.view(-1)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=60, verbose=True, path='best_model.pt', delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, graph, criterion, mask):\n",
    "    model.eval()\n",
    "    out_eval = model(graph)\n",
    "    pred = torch.sigmoid(out_eval[mask]).round()\n",
    "    labels = graph[\"B1\"].y[mask]\n",
    "    val_loss = criterion(out_eval[mask], labels.float())\n",
    "\n",
    "    # Calculate the metrics\n",
    "    f1 = f1_score(labels.cpu(), pred.cpu(), average='binary')\n",
    "    precision = precision_score(labels.cpu(), pred.cpu(), average='binary')\n",
    "    recall = recall_score(labels.cpu(), pred.cpu(), average='binary')\n",
    "    mcc = matthews_corrcoef(labels.cpu(), pred.cpu())\n",
    "    accuracy = accuracy_score(labels.cpu(), pred.cpu())\n",
    "    auc = roc_auc_score(labels.cpu(), out_eval[mask].cpu())\n",
    "\n",
    "    return val_loss.item(), (f1, precision, recall, mcc, accuracy, auc)\n",
    "\n",
    "def train(model, data, optimizer, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data[\"B1\"].train_mask], data[\"B1\"].y[data[\"B1\"].train_mask].float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def objective(trial):\n",
    "    try:\n",
    "        # Define the hyperparameters\n",
    "        lr = trial.suggest_loguniform('lr', 1e-6, 1e-3)\n",
    "        weight_decay = trial.suggest_loguniform('weight_decay', 1e-7, 1e-4)\n",
    "        heads = trial.suggest_int('heads', 1, 6, step=1)\n",
    "        dropout = trial.suggest_uniform('dropout', 0, 0.5)\n",
    "\n",
    "        # Fixed hidden channels\n",
    "        hidden_channels = 1280\n",
    "\n",
    "        # Define the model\n",
    "        model = TropiGAT_small_module(hidden_channels=hidden_channels, heads=heads, dropout=dropout)\n",
    "        # Input graph:\n",
    "        graph_baseline , dico_prophage_kltype_associated = TropiGAT_graph.build_graph_baseline(DF_info_lvl_0)\n",
    "        graph_data_kltype = TropiGAT_graph.build_graph_masking_v2(graph_baseline , dico_prophage_kltype_associated, DF_info_lvl_0, KL_type, 5, 0.7, 0.2, 0.1, seed = 243)\n",
    "\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "        early_stopping = EarlyStopping(patience=100, verbose=True, path=f\"best_model_trial_{trial.number}_{KL_type}.pt\")\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "\n",
    "        for epoch in range(500):\n",
    "            train_loss = train(model, graph_data_kltype, optimizer, criterion)\n",
    "            val_loss, metrics = evaluate(model, graph_data_kltype, criterion, graph_data_kltype[\"B1\"].test_mask)\n",
    "\n",
    "            scheduler.step(val_loss)\n",
    "            early_stopping(val_loss, model)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                logging.info(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "        # Final evaluation\n",
    "        model.load_state_dict(torch.load(f\"best_model_trial_{trial.number}_{KL_type}.pt\"))\n",
    "        final_val_loss, metrics = evaluate(model, graph_data_kltype, criterion, graph_data_kltype[\"B1\"].eval_mask)\n",
    "\n",
    "        # Log the results\n",
    "        logging.info(f\"Trial {trial.number}: Val Loss: {final_val_loss:.4f}, MCC: {metrics[3]:.4f}, AUC: {metrics[5]:.4f}\")\n",
    "\n",
    "        return final_val_loss  # Return validation loss for minimization\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in trial {trial.number}: {str(e)}\")\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "# Optimize\n",
    "KL_type = \"KL10\"\n",
    "\n",
    "logging.basicConfig(filename = f\"{path_work}/GATv2Conv.{KL_type}.loss.1209.optuna.log\",format='%(asctime)s | %(levelname)s: %(message)s', level=logging.NOTSET, filemode='w')\n",
    "logging.info(\"Starting hyperparameter optimization\")\n",
    "study = optuna.create_study(sampler=TPESampler(), direction='minimize')\n",
    "\n",
    "\n",
    "try:\n",
    "    study.optimize(objective, n_trials=200, n_jobs=-1, catch=(Exception,))\n",
    "\n",
    "    if study.best_trial is not None:\n",
    "        print(f\"Best parameters: {study.best_params}\")\n",
    "        logging.info(f\"Best parameters: {study.best_params}\")\n",
    "        best_trial = study.best_trial\n",
    "        print(f\"Best trial: Val Loss: {best_trial.value:.4f}\")\n",
    "        logging.info(f\"Best trial: Val Loss: {best_trial.value:.4f}\")\n",
    "\n",
    "        # Optionally, you can retrain the model with the best parameters and evaluate on the test set\n",
    "        best_model = TropiGAT_small_module(hidden_channels=1280, heads=best_trial.params['heads'], dropout=best_trial.params['dropout'])\n",
    "        # ... (retrain with best parameters)\n",
    "        # final_test_loss, final_metrics = evaluate(best_model, graph_data, criterion, graph_data[\"B1\"].test_mask)\n",
    "        # print(f\"Final Test Results: Loss: {final_test_loss:.4f}, MCC: {final_metrics[3]:.4f}, AUC: {final_metrics[5]:.4f}\")\n",
    "    else:\n",
    "        logging.warning(\"No trials were successfully completed.\")\n",
    "        print(\"No trials were successfully completed. Check the logs for more information.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred during optimization: {str(e)}\")\n",
    "    print(f\"An error occurred during optimization. Check the logs for more information.\")\n",
    "\n",
    "finally:\n",
    "    # Print a summary of the study\n",
    "    trial_data = study.trials_dataframe()\n",
    "    if not trial_data.empty:\n",
    "        print(\"\\nStudy Summary:\")\n",
    "        print(f\"Number of completed trials: {len(study.trials)}\")\n",
    "        print(f\"Number of pruned trials: {len(study.get_trials(states=[optuna.trial.TrialState.PRUNED]))}\")\n",
    "        print(f\"Number of completed trials: {len(study.get_trials(states=[optuna.trial.TrialState.COMPLETE]))}\")\n",
    "        print(\"\\nBest 5 trials:\")\n",
    "        print(trial_data.sort_values('value').head())\n",
    "    else:\n",
    "        print(\"No trial data available.\")\n",
    "\n",
    "# Save the study results\n",
    "study.trials_dataframe().to_csv(f\"{path_work}/optuna_study_results.{KL_type}.1209.csv\", index=False)\n",
    "logging.info(\"Study results saved to 'optuna_study_results.{KL_type}.1209.csv'\")\n",
    "#best_model = TropiGAT_small_module(hidden_channels=1280, heads=best_trial.params['heads'], dropout=best_trial.params['dropout'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_geometric]",
   "language": "python",
   "name": "conda-env-torch_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
