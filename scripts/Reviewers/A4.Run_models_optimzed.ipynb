{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eed841-e94f-4875-8592-ad90fbd142b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TropiGAT: A Graph Neural Network for Prophage Prediction\n",
    "# This script processes prophage data, builds graphs, and trains models for prediction\n",
    "\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import (accuracy_score, f1_score, matthews_corrcoef,\n",
    "                             precision_score, recall_score, roc_auc_score)\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, label_binarize\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.data import DataLoader, HeteroData\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import GATv2Conv, HeteroConv, to_hetero\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from tqdm import tqdm\n",
    "\n",
    "import TropiGAT_graph\n",
    "import TropiGAT_models\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Constants\n",
    "PATH_WORK = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023\"\n",
    "DATE = \"1209\"\n",
    "ENSEMBLE_PATH = f\"{PATH_WORK}/train_nn/ensemble_{DATE}2024_optimized\"\n",
    "ENSEMBLE_PATH_log = f\"{PATH_WORK}/train_nn/ensemble_{DATE}2024_log_optimized\"\n",
    "os.makedirs(ENSEMBLE_PATH, exist_ok=True)\n",
    "os.makedirs(ENSEMBLE_PATH_log, exist_ok=True)\n",
    "# Hyperparameters\n",
    "DICO_OPTUNA = {\n",
    "    \"KL64\": {\n",
    "        \"para_heads\": 5,  \n",
    "        \"para_lr\": 0.000246, \n",
    "        \"para_wd\": 0.000080,\n",
    "        \"para_dropout\": 0.063113}, \n",
    "    \"KL1\": {\n",
    "        \"para_heads\": 5,  \n",
    "        \"para_lr\":  0.0009415397708661039,\n",
    "        \"para_wd\": 1.132790862878068e-06,\n",
    "        \"para_dropout\": 0.007657626670776924},\n",
    "    \"KL10\": {\n",
    "        \"para_heads\": 2,  \n",
    "        \"para_lr\": 0.0006633594884735811,\n",
    "        \"para_wd\": 3.7430295738223034e-06,\n",
    "        \"para_dropout\":0.4493747213067273 }, \n",
    "    \"KL15\": {\n",
    "        \"para_heads\": 5,  \n",
    "        \"para_lr\": 0.00017766142057218653,\n",
    "        \"para_wd\": 5.245213610566463e-05,\n",
    "        \"para_dropout\":0.15214512795626994 }, \n",
    "    \"KL17\": {\n",
    "        \"para_heads\": 2,  \n",
    "        \"para_lr\": 0.0002068133316219641,\n",
    "        \"para_wd\": 5.303964308479191e-05,\n",
    "        \"para_dropout\":0.4810681327179018 }, \n",
    "    \"KL19\": {\n",
    "        \"para_heads\": 5,  \n",
    "        \"para_lr\": 0.00028386856144729176,\n",
    "        \"para_wd\": 6.667568504410857e-07,\n",
    "        \"para_dropout\":0.4460345479421262 }, \n",
    "    \"KL2\": {\n",
    "        \"para_heads\": 2,  \n",
    "        \"para_lr\": 0.0006115983973072073,\n",
    "        \"para_wd\": 3.521041854903662e-06,\n",
    "        \"para_dropout\":0.16320044607028428 }, \n",
    "    \"KL47\": {\n",
    "        \"para_heads\": 2,  \n",
    "        \"para_lr\": 0.0007352151826846244,\n",
    "        \"para_wd\": 8.666317429082471e-06,\n",
    "        \"para_dropout\":0.1877399746783721 }, \n",
    "    \"KL74\": {\n",
    "        \"para_heads\": 1,  \n",
    "        \"para_lr\": 0.0004137122657073261,\n",
    "        \"para_wd\": 3.5238343953806846e-05,\n",
    "        \"para_dropout\":0.3464829958840639 }, \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load and preprocess the prophage data.\"\"\"\n",
    "    df_info = pd.read_csv(f\"{PATH_WORK}/train_nn/TropiGATv2.final_df_v2.tsv\", sep=\"\\t\", header=0)\n",
    "    df_info = df_info.drop_duplicates(subset=[\"Protein_name\"])\n",
    "    \n",
    "    df_prophages = df_info.drop_duplicates(subset=[\"Phage\"], keep=\"first\")\n",
    "    dico_prophage_info = {row[\"Phage\"]: {\"prophage_strain\": row[\"prophage_id\"], \"ancestor\": row[\"Infected_ancestor\"]} \n",
    "                          for _, row in df_prophages.iterrows()}\n",
    "    \n",
    "    return df_info, dico_prophage_info\n",
    "\n",
    "def filter_prophages(df_info, dico_prophage_info):\n",
    "    \"\"\"Filter prophages to remove duplicates and ensure diversity.\"\"\"\n",
    "    def get_filtered_prophages(prophage):\n",
    "        combinations = []\n",
    "        to_exclude = set()\n",
    "        to_keep = set()\n",
    "        to_keep.add(prophage)\n",
    "        df_prophage_group = df_info[\n",
    "            (df_info[\"prophage_id\"] == dico_prophage_info[prophage][\"prophage_strain\"]) & \n",
    "            (df_info[\"Infected_ancestor\"] == dico_prophage_info[prophage][\"ancestor\"])\n",
    "        ]\n",
    "        if len(df_prophage_group) == 1:\n",
    "            return df_prophage_group, to_exclude, to_keep\n",
    "        \n",
    "        depo_set = set(df_prophage_group[df_prophage_group[\"Phage\"] == prophage][\"domain_seq\"].values)\n",
    "        for prophage_tmp in df_prophage_group[\"Phage\"].unique():\n",
    "            if prophage_tmp != prophage:\n",
    "                tmp_depo_set = set(df_prophage_group[df_prophage_group[\"Phage\"] == prophage_tmp][\"domain_seq\"].values)\n",
    "                if depo_set == tmp_depo_set:\n",
    "                    to_exclude.add(prophage_tmp)\n",
    "                elif tmp_depo_set not in combinations:\n",
    "                    to_keep.add(prophage_tmp)\n",
    "                    combinations.append(tmp_depo_set)\n",
    "                else:\n",
    "                    to_exclude.add(prophage_tmp)\n",
    "        return df_prophage_group, to_exclude, to_keep\n",
    "\n",
    "    good_prophages = set()\n",
    "    excluded_prophages = set()\n",
    "\n",
    "    for prophage in tqdm(dico_prophage_info.keys()):\n",
    "        if prophage not in excluded_prophages and prophage not in good_prophages:\n",
    "            _, excluded_members, kept_members = get_filtered_prophages(prophage)\n",
    "            good_prophages.update(kept_members)\n",
    "            excluded_prophages.update(excluded_members)\n",
    "\n",
    "    df_info_filtered = df_info[df_info[\"Phage\"].isin(good_prophages)]\n",
    "    df_info_final = df_info_filtered[~df_info_filtered[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "\n",
    "    return df_info_final\n",
    "\n",
    "def ultrafilter_prophages(df_info):\n",
    "    \"\"\"Perform ultra-filtration to remove duplicate prophages within KL types.\"\"\"\n",
    "    duplicate_prophage = []\n",
    "    dico_kltype_duplica = {}\n",
    "\n",
    "    for kltype in df_info[\"KL_type_LCA\"].unique():\n",
    "        df_kl = df_info[df_info[\"KL_type_LCA\"] == kltype][[\"Phage\", \"Protein_name\", \"KL_type_LCA\", \"Infected_ancestor\", \"index\", \"seq\", \"domain_seq\"]]\n",
    "        prophages_tmp_list = df_kl[\"Phage\"].unique().tolist()\n",
    "        set_sets_depo = []\n",
    "        duplicated = {}  \n",
    "        for prophage_tmp in prophages_tmp_list: \n",
    "            set_depo = frozenset(df_kl[df_kl[\"Phage\"] == prophage_tmp][\"domain_seq\"].values)\n",
    "            for past_set in set_sets_depo:\n",
    "                if past_set == set_depo:\n",
    "                    duplicated[past_set] = duplicated.get(past_set, 0) + 1\n",
    "                    duplicate_prophage.append(prophage_tmp)\n",
    "                    break\n",
    "            else:\n",
    "                set_sets_depo.append(set_depo)\n",
    "                duplicated[set_depo] = 1\n",
    "        dico_kltype_duplica[kltype] = duplicated\n",
    "\n",
    "    df_info_ultrafiltered = df_info[~df_info[\"Phage\"].isin(duplicate_prophage)]\n",
    "    return df_info_ultrafiltered\n",
    "\n",
    "def prepare_kltypes(df_info):\n",
    "    \"\"\"Prepare KL types for training.\"\"\"\n",
    "    df_prophages = df_info.drop_duplicates(subset=[\"Phage\"])\n",
    "    dico_prophage_count = dict(Counter(df_prophages[\"KL_type_LCA\"]))\n",
    "    kltypes = [kltype for kltype, count in dico_prophage_count.items() if count >= 10]\n",
    "    return kltypes, dico_prophage_count\n",
    "\n",
    "def train_graph(kl_type, graph_baseline, dico_prophage_kltype_associated, df_info, dico_prophage_count):\n",
    "    \"\"\"Train the graph neural network for a specific KL type.\"\"\"\n",
    "    for seed in range(1, 6):\n",
    "        log_file = f\"{ENSEMBLE_PATH_log}/{kl_type}__{seed}__node_classification.{DATE}.log\"\n",
    "        with open(log_file, \"w\") as log_outfile:\n",
    "            try:\n",
    "                n_prophage = dico_prophage_count[kl_type]\n",
    "                graph_data_kltype = TropiGAT_graph.build_graph_masking_v2(\n",
    "                    graph_baseline, dico_prophage_kltype_associated, df_info, \n",
    "                    kl_type, 5, 0.7, 0.2, 0.1, seed=seed\n",
    "                )\n",
    "                \n",
    "                model = TropiGAT_models.TropiGAT_small_module(\n",
    "                    hidden_channels=1280, heads=DICO_OPTUNA[kl_type][\"para_heads\"], dropout = DICO_OPTUNA[kl_type][\"para_dropout\"]\n",
    "                )\n",
    "                model(graph_data_kltype)\n",
    "                \n",
    "                optimizer = torch.optim.AdamW(\n",
    "                    model.parameters(), \n",
    "                    lr=DICO_OPTUNA[kl_type][\"para_lr\"], \n",
    "                    weight_decay=DICO_OPTUNA[kl_type][\"para_wd\"]\n",
    "                )\n",
    "                scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "                criterion = torch.nn.BCEWithLogitsLoss()\n",
    "                early_stopping = TropiGAT_models.EarlyStopping(\n",
    "                    patience=100, \n",
    "                    verbose=True, \n",
    "                    path=f\"{ENSEMBLE_PATH}/{kl_type}__{seed}.TropiGATv2.{DATE}.pt\", \n",
    "                    metric='MCC'\n",
    "                )\n",
    "                \n",
    "                for epoch in range(500):\n",
    "                    train_loss = TropiGAT_models.train(model, graph_data_kltype, optimizer, criterion)\n",
    "                    if epoch % 5 == 0:\n",
    "                        test_loss, metrics = TropiGAT_models.evaluate(\n",
    "                            model, graph_data_kltype, criterion, graph_data_kltype[\"B1\"].test_mask\n",
    "                        )\n",
    "                        log_outfile.write(f'Epoch: {epoch}\\tTrain Loss: {train_loss:.4f}\\t'\n",
    "                                          f'Test Loss: {test_loss:.4f}\\tMCC: {metrics[3]:.4f}\\t'\n",
    "                                          f'AUC: {metrics[5]:.4f}\\tAccuracy: {metrics[4]:.4f}\\n')\n",
    "                        scheduler.step(test_loss)\n",
    "                        early_stopping(metrics[3], model, epoch)\n",
    "                        if early_stopping.early_stop:\n",
    "                            log_outfile.write(f\"Early stopping at epoch {epoch}\\n\")\n",
    "                            break\n",
    "                else:\n",
    "                    torch.save(model, f\"{ENSEMBLE_PATH}/{kl_type}__{seed}.TropiGATv2.{DATE}.pt\")\n",
    "                \n",
    "                # Final evaluation\n",
    "                model_final = TropiGAT_models.TropiGAT_small_module(\n",
    "                    hidden_channels=1280, heads=DICO_OPTUNA[kl_type][\"para_heads\"], dropout =DICO_OPTUNA[kl_type][\"para_dropout\"]\n",
    "                )\n",
    "                model_final.load_state_dict(torch.load(f\"{ENSEMBLE_PATH}/{kl_type}__{seed}.TropiGATv2.{DATE}.pt\"))\n",
    "                eval_loss, metrics = TropiGAT_models.evaluate(\n",
    "                    model_final, graph_data_kltype, criterion, graph_data_kltype[\"B1\"].eval_mask\n",
    "                )\n",
    "                \n",
    "                with open(f\"{ENSEMBLE_PATH_log}/Metric_Report.{DATE}.tsv\", \"a+\") as metric_outfile:\n",
    "                    metric_outfile.write(f\"{kl_type}__{seed}\\t{n_prophage}\\t\"\n",
    "                                         f\"{metrics[0]:.4f}\\t{metrics[1]:.4f}\\t{metrics[2]:.4f}\\t\"\n",
    "                                         f\"{metrics[3]:.4f}\\t{metrics[4]:.4f}\\t{metrics[5]:.4f}\\n\")\n",
    "                \n",
    "                log_outfile.write(f\"Final evaluation:\\n\"\n",
    "                                  f\"F1 Score: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, \"\n",
    "                                  f\"Recall: {metrics[2]:.4f}, MCC: {metrics[3]:.4f}, \"\n",
    "                                  f\"Accuracy: {metrics[4]:.4f}, AUC: {metrics[5]:.4f}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                log_outfile.write(f\"Error occurred: {str(e)}\")\n",
    "                with open(f\"{ENSEMBLE_PATH_log}/Metric_Report.{DATE}.tsv\", \"a+\") as metric_outfile:\n",
    "                    metric_outfile.write(f\"{kl_type}__{seed}\\t{n_prophage}\\t***Error***\\n\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the TropiGAT workflow.\"\"\"\n",
    "    df_info, dico_prophage_info = load_and_preprocess_data()\n",
    "    df_info_filtered = filter_prophages(df_info, dico_prophage_info)\n",
    "    df_info_final = ultrafilter_prophages(df_info_filtered)\n",
    "    \n",
    "    kltypes, dico_prophage_count = prepare_kltypes(df_info_final)\n",
    "    \n",
    "    graph_baseline, dico_prophage_kltype_associated = TropiGAT_graph.build_graph_baseline(df_info_final)\n",
    "    \n",
    "    with ThreadPool(5) as p:\n",
    "        p.starmap(train_graph, [(kl_type, graph_baseline, dico_prophage_kltype_associated, df_info_final, dico_prophage_count) for kl_type in DICO_OPTUNA if os.path.isfile(f\"{ENSEMBLE_PATH_log}/{kl_type}__1__node_classification.{DATE}.log\")==False])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_geometric]",
   "language": "python",
   "name": "conda-env-torch_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
