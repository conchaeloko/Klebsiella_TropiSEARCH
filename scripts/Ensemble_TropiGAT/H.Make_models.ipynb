{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d53a6c4-1c25-4605-b547-e74e7cf402a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData, DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import HeteroConv , GATv2Conv \n",
    "#from torch_geometric.utils import negative_sampling\n",
    "#from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder , label_binarize , OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import random\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4e809bc-6f74-4894-b3a0-bfedcd403902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import TropiGAT_models\n",
    "import TropiGAT_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd5cbc63-bbdb-4e32-a01f-55bfdad64432",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def build_graph_baseline(df_info) : \n",
    "    # **************************************************************\n",
    "    # initialize the graph\n",
    "    graph_data = HeteroData()\n",
    "    # Indexation process  \n",
    "    indexation_nodes_A = df_info[\"Infected_ancestor\"].unique().tolist()  \n",
    "    indexation_nodes_B1 = df_info[\"Phage\"].unique().tolist()\n",
    "    indexation_nodes_B2 = df_info[\"index\"].unique().tolist() \n",
    "    ID_nodes_A = {item:index for index, item in enumerate(indexation_nodes_A)}\n",
    "    ID_nodes_A_r = {index:item for index, item in enumerate(indexation_nodes_A)}\n",
    "    ID_nodes_B1 = {item:index for index, item in enumerate(indexation_nodes_B1)}\n",
    "    ID_nodes_B1_r = {index:item for index, item in enumerate(indexation_nodes_B1)}\n",
    "    ID_nodes_B2 = {item:index for index, item in enumerate(indexation_nodes_B2)}\n",
    "    ID_nodes_B2_r = {index:item for index, item in enumerate(indexation_nodes_B2)}\n",
    "    # **************************************************************\n",
    "    # Make the node feature file : \n",
    "    OHE = OneHotEncoder(sparse=False)\n",
    "    one_hot_encoded = OHE.fit_transform(df_info[[\"KL_type_LCA\"]])\n",
    "    label_mapping = {label: one_hot_encoded[i] for i, label in enumerate(OHE.categories_[0])}\n",
    "    embeddings_columns = [str(i) for i in range(1, 1281)]\n",
    "    node_feature_A = torch.tensor([label_mapping[df_info[df_info[\"Infected_ancestor\"] == ID_nodes_A_r[i]][\"KL_type_LCA\"].values[0]] for i in range(0,len(ID_nodes_A_r))], dtype=torch.float)\n",
    "    node_feature_B1 = torch.zeros((len(ID_nodes_B1), 0), dtype=torch.float)\n",
    "    node_feature_B2 = torch.tensor([df_info[df_info[\"index\"] == ID_nodes_B2_r[i]][embeddings_columns].values[0].tolist() for i in range(0,len(ID_nodes_B2_r))] , dtype=torch.float)\n",
    "    # feed the graph\n",
    "    graph_data[\"A\"].x = node_feature_A\n",
    "    graph_data[\"B1\"].x = node_feature_B1\n",
    "    graph_data[\"B2\"].x = node_feature_B2\n",
    "    # **************************************************************\n",
    "    # Make edge file\n",
    "    # Node B1 (prophage) - Node A (bacteria) :\n",
    "    edge_index_B1_A = []\n",
    "    track_B1_A = set()\n",
    "    for _, row in df_info.iterrows() :\n",
    "        pair = [ID_nodes_B1[row[\"Phage\"]], ID_nodes_A[row[\"Infected_ancestor\"]]]\n",
    "        if tuple(pair) not in track_B1_A : \n",
    "            track_B1_A.add(tuple(pair))\n",
    "            edge_index_B1_A.append(pair)\n",
    "        else :\n",
    "            continue\n",
    "    edge_index_B1_A = torch.tensor(edge_index_B1_A , dtype=torch.long)\n",
    "    # Node A (bacteria) - Node B1 (prophage) :\n",
    "    edge_index_A_B1 = []\n",
    "    track_A_B1 = set()\n",
    "    for _, row in df_info.iterrows() :\n",
    "        pair = [ID_nodes_A[row[\"Infected_ancestor\"]] , ID_nodes_B1[row[\"Phage\"]]]\n",
    "        if tuple(pair) not in track_A_B1 :\n",
    "            track_A_B1.add(tuple(pair))\n",
    "            edge_index_A_B1.append(pair)\n",
    "    edge_index_A_B1 = torch.tensor(edge_index_A_B1 , dtype=torch.long)\n",
    "    # Node B2 (depolymerase) - Node B1 (prophage) :\n",
    "    edge_index_B2_B1 = []\n",
    "    for phage in df_info.Phage.unique() :\n",
    "        all_data_phage = df_info[df_info[\"Phage\"] == phage]\n",
    "        for _, row in all_data_phage.iterrows() :\n",
    "            edge_index_B2_B1.append([ID_nodes_B2[row[\"index\"]], ID_nodes_B1[row[\"Phage\"]]])\n",
    "    edge_index_B2_B1 = torch.tensor(edge_index_B2_B1 , dtype=torch.long)\n",
    "    # feed the graph\n",
    "    graph_data['B1', 'infects', 'A'].edge_index = edge_index_B1_A.t().contiguous()\n",
    "    graph_data['B2', 'expressed', 'B1'].edge_index = edge_index_B2_B1.t().contiguous()\n",
    "    # That one is optional  \n",
    "    graph_data['A', 'harbors', 'B1'].edge_index = edge_index_A_B1.t().contiguous()\n",
    "    dico_prophage_kltype_associated = {}\n",
    "    for negative_index,phage in tqdm(enumerate(df_info[\"Phage\"].unique().tolist())) :\n",
    "        kltypes = set()\n",
    "        dpos = df_info[df_info[\"Phage\"] == phage][\"index\"]\n",
    "        for dpo in dpos : \n",
    "            tmp_kltypes = df_info[df_info[\"index\"] == dpo][\"KL_type_LCA\"].values\n",
    "            kltypes.update(tmp_kltypes)\n",
    "        dico_prophage_kltype_associated[phage] = kltypes\n",
    "    return graph_data , dico_prophage_kltype_associated\n",
    "\n",
    "\n",
    "def build_graph_masking(graph_data, dico_prophage_kltype_associated , df_info, KL_type, ratio , f_train, f_test, f_eval) : \n",
    "    # **************************************************************\n",
    "    # Indexation process  \n",
    "    indexation_nodes_A = df_info[\"Infected_ancestor\"].unique().tolist()  \n",
    "    indexation_nodes_B1 = df_info[\"Phage\"].unique().tolist()\n",
    "    indexation_nodes_B2 = df_info[\"index\"].unique().tolist() \n",
    "    ID_nodes_A = {item:index for index, item in enumerate(indexation_nodes_A)}\n",
    "    ID_nodes_A_r = {index:item for index, item in enumerate(indexation_nodes_A)}\n",
    "    ID_nodes_B1 = {item:index for index, item in enumerate(indexation_nodes_B1)}\n",
    "    ID_nodes_B1_r = {index:item for index, item in enumerate(indexation_nodes_B1)}\n",
    "    ID_nodes_B2 = {item:index for index, item in enumerate(indexation_nodes_B2)}\n",
    "    ID_nodes_B2_r = {index:item for index, item in enumerate(indexation_nodes_B2)}\n",
    "    # **************************************************************\n",
    "    # Make the Y file : \n",
    "    B1_labels = df_info.drop_duplicates(subset = [\"Phage\"], keep = \"first\")[\"KL_type_LCA\"].apply(lambda x : 1 if x == KL_type else 0).to_list()\n",
    "    graph_data[\"B1\"].y = torch.tensor(B1_labels)\n",
    "    # **************************************************************\n",
    "    # Make mask files :\n",
    "    # get the positive and negative indices lists :\n",
    "    positive_indices = [index for index,label in enumerate(B1_labels) if label==1]\n",
    "    negative_indices = []\n",
    "    for negative_index,phage in enumerate(df_info[\"Phage\"].unique().tolist()) :\n",
    "        if KL_type not in dico_prophage_kltype_associated[ID_nodes_B1_r[negative_index]] :\n",
    "            negative_indices.append(negative_index)\n",
    "    # make the train, test, val lists : \n",
    "    n_samples = len(positive_indices)\n",
    "    #train_indices, test_indices, val_indices = [],[],[]\n",
    "    # make train : \n",
    "    train_pos = random.sample(positive_indices, int(f_train*n_samples))\n",
    "    train_neg = random.sample(negative_indices, int(f_train*n_samples*ratio))\n",
    "    train_indices = train_pos + train_neg\n",
    "    train_mask = [1 if n in train_indices else 0 for n in range(0,len(B1_labels))]\n",
    "    # make test : \n",
    "    pool_positives_test = list(set(positive_indices) - set(train_pos))\n",
    "    pool_negatives_test = list(set(negative_indices) - set(train_neg))\n",
    "    test_pos = random.sample(pool_positives_test, int(f_test*n_samples))\n",
    "    test_neg = random.sample(pool_negatives_test, int(f_test*n_samples*ratio))\n",
    "    test_indices = test_pos + test_neg\n",
    "    test_mask = [1 if n in test_indices else 0 for n in range(0,len(B1_labels))]\n",
    "    # make eval\n",
    "    pool_positives_eval = list(set(positive_indices) - set(train_pos) - set(test_pos))\n",
    "    pool_negatives_eval = list(set(negative_indices) - set(train_neg) - set(test_neg))\n",
    "    eval_pos = random.sample(pool_positives_eval, int(f_eval*n_samples))\n",
    "    eval_neg = random.sample(pool_negatives_eval, int(f_eval*n_samples*ratio))\n",
    "    eval_indices = eval_pos + eval_neg\n",
    "    eval_mask = [1 if n in eval_indices else 0 for n in range(0,len(B1_labels))]\n",
    "    # Transfer data to graph :\n",
    "    graph_data[\"B1\"].train_mask = torch.tensor(train_mask)\n",
    "    graph_data[\"B1\"].test_mask = torch.tensor(test_mask)\n",
    "    graph_data[\"B1\"].eval_mask = torch.tensor(eval_mask)\n",
    "\n",
    "    return graph_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e75ee7-3932-4172-91b6-9cf5fb3a4365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Load the Dataframes :\n",
    "path_work = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "#path_work = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023\"\n",
    "\n",
    "    # Open the DF\n",
    "DF_info = pd.read_csv(f\"{path_work}/TropiGATv2.final_df.tsv\", sep = \"\\t\" ,  header = 0)\n",
    "\n",
    "# Ambiguous ones :\n",
    "# level 0 :\n",
    "DF_info_lvl_0 = DF_info[~DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "DF_info_lvl_0 = DF_info_lvl_0.drop_duplicates(subset = [\"Infected_ancestor\",\"index\",\"prophage_id\"] , keep = \"first\").reset_index(drop=True)\n",
    "dico_prophage_kltype = {row[\"Phage\"]:row[\"KL_type_LCA\"] for _,row in DF_info_lvl_0.drop_duplicates(subset = [\"Phage\"]).iterrows()}\n",
    "\n",
    "# level 1 :\n",
    "#DF_info_lvl_1 = pd.read_csv(f\"{path_work}/TropiGATv2.ambiguity.lvl_1.tsv\", sep = \",\" ,  header = 0)\n",
    "#DF_info_lvl_1 = DF_info_lvl_1.drop_duplicates(subset = [\"Infected_ancestor\",\"index\",\"prophage_id\"] , keep = \"first\").reset_index(drop=True)\n",
    "\n",
    "# level 2 :\n",
    "#DF_info_lvl_2 = pd.read_csv(f\"{path_work}/TropiGATv2.ambiguity.lvl_2.tsv\", sep = \",\" ,  header = 0)\n",
    "#DF_info_lvl_2 = DF_info_lvl_2.drop_duplicates(subset = [\"Infected_ancestor\",\"index\",\"prophage_id\"] , keep = \"first\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99b02cfc-c96c-4f26-9532-66ef0479a610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8815it [00:20, 427.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.3 s, sys: 176 ms, total: 41.4 s\n",
      "Wall time: 41.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "graph_baseline , dico_prophage_kltype_associated = TropiGAT_graph.build_graph_baseline(DF_info_lvl_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fa6beb8-a49d-48cb-850c-6cf32b383d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 225 ms, sys: 36.4 ms, total: 261 ms\n",
      "Wall time: 261 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "graph_KL64 = TropiGAT_graph.build_graph_masking(graph_baseline , \n",
    "                                 dico_prophage_kltype_associated, \n",
    "                                 DF_info_lvl_0, \n",
    "                                 \"KL64\",\n",
    "                                 2, 0.80, 0.1,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "708c4557-2862-445e-b6e8-f39f029921f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([], size=(8815, 0)), 'y': tensor([0, 0, 0,  ..., 0, 0, 0]), 'train_mask': tensor([False, False, False,  ...,  True,  True, False]), 'test_mask': tensor([False, False, False,  ..., False, False, False]), 'eval_mask': tensor([False, False, False,  ..., False, False, False])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_KL64[\"B1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e391b17-daef-44e1-b904-2fa37a29823d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 83.8 ms, sys: 32.5 ms, total: 116 ms\n",
      "Wall time: 529 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "graph_KL37 = build_graph_masking(graph_baseline , \n",
    "                                 dico_prophage_kltype_associated, \n",
    "                                 DF_info_lvl_0, \n",
    "                                 \"KL37\",\n",
    "                                 2, 0.8, 0.1,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f530bc2e-8b1e-4938-bd8f-530eea02ef6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 7929, 1: 886})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(graph_KL64[\"B1\"].y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd1579e1-1079-46b4-aebe-0348d709d443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2125"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_KL64[\"B1\"].y[graph_KL64[\"B1\"].train_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4191672f-d61b-432f-846a-d9d18696bde7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8815"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_KL64[\"B1\"].y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9458dcca-0c23-4c91-ad1c-4b1c5b01686f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geometric",
   "language": "python",
   "name": "torch_geometric"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
