{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a400685-e978-4b30-8546-fb493471d559",
   "metadata": {},
   "source": [
    "### Pre -work : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c1db14-6f88-40e4-9a1e-1f1575666320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move files around :\n",
    "\n",
    "rsync -avzhe ssh \\\n",
    "/home/concha-eloko/Téléchargements/Enterobacter_aerogenes_KCTC_2190.fasta \\\n",
    "conchae@garnatxa.srv.cpd:/home/conchae/prediction_depolymerase_tropism/post_work\n",
    "\n",
    "kleborate --kaptive_k \\\n",
    "--kaptive_k_outfile /home/conchae/prediction_depolymerase_tropism/post_work/Enterobacter_aerogenes_KCTC_2190.out \\\n",
    "-a /home/conchae/prediction_depolymerase_tropism/post_work/Enterobacter_aerogenes_KCTC_2190.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcece027-57b9-4fa5-a572-574ed792e4dd",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a6d5daf-a8f3-4c99-8ed4-51bbba154ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData, DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import to_hetero , HeteroConv , GATv2Conv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder , label_binarize , OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score , matthews_corrcoef\n",
    "\n",
    "#import TropiGAT_functions \n",
    "#from TropiGAT_functions import get_top_n_kltypes ,clean_print \n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import random\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import logging\n",
    "from multiprocessing.pool import ThreadPool\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# *****************************************************************************\n",
    "# Load the Dataframes :\n",
    "path_work = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "path_ensemble = f\"{path_work}/ficheros_28032023/ensemble_2809\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aa8fc3-af0e-4fbf-b82d-fd01981a49de",
   "metadata": {},
   "source": [
    "> Open the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85cf2073-420d-4f65-8782-e74c9c60708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_project = \"/media/concha-eloko/Linux/77_strains_phage_project\"\n",
    "path_Dpo_domain_org = \"/media/concha-eloko/Linux/depolymerase_building/clean_77_phages_depo\"\n",
    "\n",
    "dpo_embeddings = pd.read_csv(f\"{path_project}/rbp_work/Dpo_domains_77.esm2.embedding.csv\", sep = \",\" , header = None)\n",
    "dpo_embeddings = dpo_embeddings.drop([1281] , axis = 1)\n",
    "dpo_embeddings[0] = dpo_embeddings[0].apply(lambda x : x.split(\"_Dpo\")[0])\n",
    "dpo_embeddings.set_index([0], inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "edb5dca4-22ef-45c4-8262-d84071dc368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_project = \"/media/concha-eloko/Linux/PPT_clean/in_vitro\"\n",
    "\n",
    "bea_embeddings = pd.read_csv(f\"{path_project}/Bea_phages.esm2.embedding.csv\", sep = \",\" , header = None)\n",
    "bea_embeddings = bea_embeddings.drop([1281] , axis = 1)\n",
    "bea_embeddings[0] = bea_embeddings[0].apply(lambda x : x.replace(\"_\", \"__\"))\n",
    "bea_embeddings.set_index([0], inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "716095b3-007d-482d-bf99-b4a0a621d0f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_project = \"/media/concha-eloko/Linux/PPT_clean/in_vitro\"\n",
    "\n",
    "towndsend_embeddings = pd.read_csv(f\"{path_project}/Townsed_phages.esm2.embedding.csv\", sep = \",\" , header = None)\n",
    "towndsend_embeddings = towndsend_embeddings.drop([1281] , axis = 1)\n",
    "towndsend_embeddings[0] = towndsend_embeddings[0].apply(lambda x : x.replace(\"_\", \"__\"))\n",
    "towndsend_embeddings.set_index([0], inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3126380d-f807-4d2b-91de-50a1939641cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_embeddings = pd.concat([towndsend_embeddings, bea_embeddings, dpo_embeddings], axis = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166670d7-9751-4fd5-a18a-521b72d9f991",
   "metadata": {},
   "source": [
    "> Open df KL types :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f4851a2-956b-4042-bcf0-77251a0788e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phages</th>\n",
       "      <th>Protein</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1a</td>\n",
       "      <td>A1a_00002</td>\n",
       "      <td>KL151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1a</td>\n",
       "      <td>A1a_00014</td>\n",
       "      <td>KL151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1b</td>\n",
       "      <td>A1b_00048</td>\n",
       "      <td>KL157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1b</td>\n",
       "      <td>A1b_00036</td>\n",
       "      <td>KL157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1c</td>\n",
       "      <td>A1c_00046</td>\n",
       "      <td>KL1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>S13a</td>\n",
       "      <td>S13a_00036</td>\n",
       "      <td>KL102,KL149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>S13b</td>\n",
       "      <td>S13b_00058</td>\n",
       "      <td>KL63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>S13c</td>\n",
       "      <td>S13c_00055</td>\n",
       "      <td>KL102,KL149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>S13d</td>\n",
       "      <td>S13d_00057</td>\n",
       "      <td>KL14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>S13e</td>\n",
       "      <td>S13e_00021</td>\n",
       "      <td>wzi-KL136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Phages     Protein       Target\n",
       "0     A1a   A1a_00002        KL151\n",
       "1     A1a   A1a_00014        KL151\n",
       "2     A1b   A1b_00048        KL157\n",
       "3     A1b   A1b_00036        KL157\n",
       "4     A1c   A1c_00046          KL1\n",
       "..    ...         ...          ...\n",
       "66   S13a  S13a_00036  KL102,KL149\n",
       "67   S13b  S13b_00058         KL63\n",
       "68   S13c  S13c_00055  KL102,KL149\n",
       "69   S13d  S13d_00057         KL14\n",
       "70   S13e  S13e_00021    wzi-KL136\n",
       "\n",
       "[71 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bea_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aefec4b6-7f85-4427-b44c-c61797e90f3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_finetuning = \"/media/concha-eloko/Linux/PPT_clean/in_vitro/fine_tuning\"\n",
    "\n",
    "bea_df = pd.read_csv(f\"{path_finetuning}/bea_fine_tuning.df\", sep = \"\\t\", header = 0)\n",
    "bea_df[\"Protein\"] = bea_df[\"Protein\"].apply(lambda x : x.replace(\"_\", \"__\"))\n",
    "pool_bea = set([kltype.strip() for kltypes in bea_df[\"Target\"] for kltype in kltypes.split(\",\") if kltype.count(\"wzi\") == 0 if kltype.count(\"pass\") == 0])\n",
    "\n",
    "ferriol_df = pd.read_csv(f\"{path_finetuning}/ferriol_fine_tuning.df\", sep = \"\\t\", header = 0)\n",
    "ferriol_df[\"Target\"] = ferriol_df[\"Target\"].apply(lambda x : x.replace(\"K\", \"KL\"))\n",
    "pool_ferriol = set([kltype.strip() for kltypes in ferriol_df[\"Target\"] for kltype in kltypes.split(\",\") if kltype.count(\"wzi\") == 0 if kltype.count(\"pass\") == 0])\n",
    "\n",
    "towndsend_df = pd.read_csv(f\"{path_finetuning}/ferriol_fine_tuning.df\", sep = \"\\t\", header = 0)\n",
    "towndsend_df[\"Protein\"] = towndsend_df[\"Protein\"].apply(lambda x : x.replace(\"_\", \"__\"))\n",
    "pool_towndsend = set([kltype.strip() for kltypes in towndsend_df[\"Target\"] for kltype in kltypes.split(\",\") if kltype.count(\"wzi\") == 0 if kltype.count(\"pass\") == 0])\n",
    "\n",
    "dico_matrices = {\"ferriol\" : {\"matrix\" : ferriol_df, \"pool\" : pool_ferriol}, \n",
    "                 \"bea\" : {\"matrix\": bea_df, \"pool\" : pool_bea}, \n",
    "                 \"towndsend\" : {\"matrix\" : towndsend_df, \"pool\" : pool_towndsend}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dece42c-43c1-4269-ad37-842880b9c154",
   "metadata": {},
   "source": [
    "> Make the graphs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5014be80-ddea-4ff7-94bc-9ae102410eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_kltype_df(kltype) :\n",
    "    positive_lines , negative_lines = [], []\n",
    "    for author in dico_matrices :\n",
    "        if kltype in dico_matrices[author][\"pool\"] : \n",
    "            for _,row in dico_matrices[author][\"matrix\"].iterrows() :\n",
    "                if row[\"Target\"].count(\"pass\") == 0 :\n",
    "                    targets = [kltype.strip() for kltype in row[\"Target\"].split(\",\") if kltype.count(\"wzi\") == 0 if kltype.count(\"pass\") == 0]\n",
    "                    if kltype not in targets :\n",
    "                        negative_lines.append(list(row.values))\n",
    "                    else :\n",
    "                        positive_lines.append(list(row.values))\n",
    "    lines = positive_lines + negative_lines\n",
    "    df_kltype = pd.DataFrame(lines, columns = [\"phage\", \"depo\",\"KLtypes\"])\n",
    "\n",
    "    return df_kltype , len(positive_lines), len(negative_lines)\n",
    "    \n",
    "        \n",
    "def build_graph_baseline(df_info, n_positives, n_negatives) : \n",
    "    # **************************************************************\n",
    "    # initialize the graph\n",
    "    graph_data = HeteroData()\n",
    "    # Indexation process  \n",
    "    indexation_nodes_A = [0]\n",
    "    indexation_nodes_B1 = df_info[\"phage\"].unique().tolist()\n",
    "    indexation_nodes_B2 = df_info[\"depo\"].unique().tolist() \n",
    "    #ID_nodes_A = {item:index for index, item in enumerate(indexation_nodes_A)}\n",
    "    #ID_nodes_A_r = {index:item for index, item in enumerate(indexation_nodes_A)}\n",
    "    ID_nodes_B1 = {item:index for index, item in enumerate(indexation_nodes_B1)}\n",
    "    ID_nodes_B1_r = {index:item for index, item in enumerate(indexation_nodes_B1)}\n",
    "    ID_nodes_B2 = {item:index for index, item in enumerate(indexation_nodes_B2)}\n",
    "    ID_nodes_B2_r = {index:item for index, item in enumerate(indexation_nodes_B2)}\n",
    "    # **************************************************************\n",
    "    # Make the node feature file : \n",
    "    embeddings_columns = [int(i) for i in range(1, 1281)]\n",
    "    node_feature_A = torch.tensor([0], dtype=torch.float)\n",
    "    node_feature_B1 = torch.zeros((len(ID_nodes_B1), 0), dtype=torch.float)\n",
    "    node_feature_B2 = torch.tensor([df_embeddings[df_embeddings.index == depo][embeddings_columns].values[0].tolist() for depo in df_info[\"depo\"]] , dtype=torch.float)\n",
    "    # feed the graph\n",
    "    graph_data[\"A\"].x = node_feature_A\n",
    "    graph_data[\"B1\"].x = node_feature_B1\n",
    "    graph_data[\"B2\"].x = node_feature_B2\n",
    "    # **************************************************************\n",
    "    # Make edge file\n",
    "    # Node B2 (depolymerase) - Node B1 (prophage) :\n",
    "    edge_index_B2_B1 = []\n",
    "    for phage in df_info.phage.unique() :\n",
    "        all_data_phage = df_info[df_info[\"phage\"] == phage]\n",
    "        for _, row in all_data_phage.iterrows() :\n",
    "            edge_index_B2_B1.append([ID_nodes_B2[row[\"depo\"]], ID_nodes_B1[row[\"phage\"]]])\n",
    "    edge_index_B2_B1 = torch.tensor(edge_index_B2_B1 , dtype=torch.long)\n",
    "    # feed the graph\n",
    "    graph_data['B2', 'expressed', 'B1'].edge_index = edge_index_B2_B1.t().contiguous()\n",
    "    # The labels : \n",
    "    labels = [1] * n_positives + [0] * n_negatives\n",
    "    graph_data[\"B1\"].y = torch.tensor(labels)\n",
    "    return graph_data \n",
    "    \n",
    "graph_test = build_graph_baseline(df_kltype , len(positive_lines) , len(negative_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "01a18f5c-2347-4e70-a460-a340e6de7eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df, n_pos, n_neg = finetune_kltype_df(\"KL2\")\n",
    "graph_test = build_graph_baseline(test_df , n_pos , n_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5fa817-1f85-47e1-adb7-2933ed9e782d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbd3552-5531-4c85-8555-6457e0de52b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graph(KL_type) :\n",
    "    with open(f\"{path_work}/train_nn/fintuning_files/{KL_type}__node_classification.2111.log\" , \"w\") as log_outfile :\n",
    "        n_prophage = dico_prophage_count[KL_type]\n",
    "        graph_data_kltype = graph_dico[KL_type]\n",
    "        if n_prophage <= 125 : \n",
    "            model = TropiGAT_models.TropiGAT_small_module(hidden_channels = 1280, heads = 1)\n",
    "            n = \"small\"\n",
    "        else : \n",
    "            model = TropiGAT_models.TropiGAT_big_module(hidden_channels = 1280, heads = 1)\n",
    "            n = \"big\"\n",
    "        model.load_state_dict(torch.load(f\"{path_ensemble}/{KL_type}.TropiGATv2.2809.pt\"))\n",
    "        model(graph_data_kltype)\n",
    "        # \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001 , weight_decay= 0.000001)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        early_stopping = TropiGAT_models.EarlyStopping(patience=40, verbose=True, path=f\"{path_ensemble}/{KL_type}.TropiGATv2.2709.pt\", metric='MCC')\n",
    "        #\n",
    "        try : \n",
    "            for epoch in range(200):\n",
    "                train_loss = TropiGAT_models.train(model, graph_data_kltype, optimizer,criterion)\n",
    "                if epoch % 5 == 0:\n",
    "                    # Get all metrics\n",
    "                    test_loss, metrics = TropiGAT_models.evaluate(model, graph_data_kltype, criterion, graph_data_kltype[\"B1\"].test_mask)\n",
    "                    info_training_concise = f'Epoch: {epoch}\\tTrain Loss: {train_loss}\\tTest Loss: {test_loss}\\tMCC: {metrics[3]}\\tAUC: {metrics[5]}\\n'\n",
    "                    info_training = f'Epoch: {epoch}, Train Loss: {train_loss}, Test Loss: {test_loss},F1 Score: {metrics[0]}, Precision: {metrics[1]}, Recall: {metrics[2]}, MCC: {metrics[3]},Accuracy: {metrics[4]}, AUC: {metrics[5]}'\n",
    "                    log_outfile.write(info_training_concise)\n",
    "                    print(info_training)\n",
    "                    scheduler.step(test_loss)\n",
    "                torch.save(model, f\"{path_ensemble}/{KL_type}.TropiGATv2.2709.pt\")\n",
    "            # The final eval :\n",
    "            print(\"Final evaluation ...\")\n",
    "            model_final = TropiGAT_models.TropiGAT_big_module(hidden_channels = 1280, heads = 1)\n",
    "            model_final.load_state_dict(torch.load(f\"{path_ensemble}/{KL_type}.TropiGATv2.2709.pt\"))\n",
    "            eval_loss, metrics = TropiGAT_models.evaluate(model_final, graph_data_kltype, criterion,graph_data_kltype[\"B1\"].eval_mask)\n",
    "            with open(f\"{path_ensemble}/Metric_Report.2709.tsv\", \"a+\") as metric_outfile :\n",
    "                metric_outfile.write(f\"{KL_type}\\t{n_prophage}\\t{metrics[0]}\\t{metrics[1]}\\t{metrics[2]}\\t{metrics[3]}\\t{metrics[4]}\\t{metrics[5]}\\n\")\n",
    "            info_eval = f'Epoch: {epoch}, F1 Score: {metrics[0]}, Precision: {metrics[1]}, Recall: {metrics[2]}, MCC: {metrics[3]},Accuracy: {metrics[4]}, AUC: {metrics[5]}'\n",
    "            print(info_eval)\n",
    "            log_outfile.write(f\"Final evaluation ...\\n{info_eval}\")\n",
    "        except Exception as e :\n",
    "            log_outfile.write(f\"***Issue here : {e}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c76379b-9651-4bd5-ab27-aa9a609b3574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have the model weights and new data\n",
    "pretrained_model_path = \"path_to_pretrained_model.pt\"\n",
    "new_data = graph_data_for_fine_tuning  # Replace with your new data\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = TropiGAT_big_module(hidden_channels=1280, heads=1)\n",
    "model.load_state_dict(torch.load(pretrained_model_path))\n",
    "\n",
    "# Create an optimizer and scheduler\n",
    "optimizer = Adam(model.parameters(), lr=0.0001, weight_decay=0.000001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Training loop for fine-tuning\n",
    "try:\n",
    "    for epoch in range(200):\n",
    "        # Training\n",
    "        train_loss = TropiGAT_models.train(model, new_data, optimizer, criterion)\n",
    "        if epoch % 5 == 0:\n",
    "            # Validation\n",
    "            test_loss, metrics = TropiGAT_models.evaluate(model, new_data, criterion, new_data[\"B1\"].test_mask)\n",
    "            info_training_concise = f'Epoch: {epoch}\\tTrain Loss: {train_loss}\\tTest Loss: {test_loss}\\tMCC: {metrics[3]}\\tAUC: {metrics[5]}\\n'\n",
    "            info_training = f'Epoch: {epoch}, Train Loss: {train_loss}, Test Loss: {test_loss},F1 Score: {metrics[0]}, Precision: {metrics[1]}, Recall: {metrics[2]}, MCC: {metrics[3]},Accuracy: {metrics[4]}, AUC: {metrics[5]}'\n",
    "            \n",
    "            print(info_training)\n",
    "            scheduler.step(test_loss)\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    torch.save(model.state_dict(), \"fine_tuned_model.pt\")\n",
    "\n",
    "    # Final evaluation\n",
    "    model_final = TropiGAT_big_module(hidden_channels=1280, heads=1)\n",
    "    model_final.load_state_dict(torch.load(\"fine_tuned_model.pt\"))\n",
    "    eval_loss, metrics = TropiGAT_models.evaluate(model_final, new_data, criterion, new_data[\"B1\"].eval_mask)\n",
    "\n",
    "    info_eval = f'Epoch: {epoch}, F1 Score: {metrics[0]}, Precision: {metrics[1]}, Recall: {metrics[2]}, MCC: {metrics[3]},Accuracy: {metrics[4]}, AUC: {metrics[5]}'\n",
    "    print(\"Final evaluation ...\")\n",
    "    print(info_eval)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"***Issue here: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0f83f3-15db-44cc-a7e4-0f1809036c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(model_path, new_graph_data, epochs=100, learning_rate=1e-4):\n",
    "    # Load the pre-trained model\n",
    "    model = TropiGAT_models.TropiGAT_big_module(hidden_channels=1280, heads=1)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    # Set up the optimizer and scheduler\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.000001)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Loop over epochs for fine-tuning\n",
    "    for epoch in range(epochs):\n",
    "        # Train on new data\n",
    "        train_loss = TropiGAT_models.train(model, new_graph_data, optimizer, criterion)\n",
    "\n",
    "        # Evaluate and print metrics every few epochs\n",
    "        if epoch % 5 == 0:\n",
    "            eval_loss, metrics = TropiGAT_models.evaluate(model, new_graph_data, criterion, new_graph_data[\"B1\"].test_mask)\n",
    "            print(f'Epoch: {epoch}, Train Loss: {train_loss}, Eval Loss: {eval_loss}, Metrics: {metrics}')\n",
    "\n",
    "        # Adjust learning rate based on loss\n",
    "        scheduler.step(eval_loss)\n",
    "\n",
    "        # Save the fine-tuned model periodically or based on performance\n",
    "        torch.save(model, f\"{path_ensemble}/FineTuned_{epoch}.pt\")\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geometric",
   "language": "python",
   "name": "torch_geometric"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
