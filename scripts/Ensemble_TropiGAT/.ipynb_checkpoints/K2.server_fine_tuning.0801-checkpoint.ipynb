{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a400685-e978-4b30-8546-fb493471d559",
   "metadata": {},
   "source": [
    "### Pre -work : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5e53cf-d027-4f98-83cd-9b7c139789a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsync -avzhe ssh \\\n",
    "/media/concha-eloko/Linux/PPT_clean/in_vitro/in_vitro_DFs \\\n",
    "conchae@garnatxa.srv.cpd:/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcece027-57b9-4fa5-a572-574ed792e4dd",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a6d5daf-a8f3-4c99-8ed4-51bbba154ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch geometric modules\n",
    "from torch_geometric.data import HeteroData, DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import to_hetero , HeteroConv , GATv2Conv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "# Torch modules\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# SKlearn modules\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder , label_binarize , OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score , matthews_corrcoef\n",
    "\n",
    "# Ground modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import random\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import logging\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# TropiGAT modules\n",
    "import TropiGAT_graph\n",
    "import TropiGAT_models\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf961c0-38b0-425c-a913-0c0d1964335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# Load the Dataframes :\n",
    "path_work = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023\"\n",
    "DF_info = pd.read_csv(f\"{path_work}/train_nn/TropiGATv2.final_df.tsv\", sep = \"\\t\" ,  header = 0)\n",
    "DF_info_lvl_0 = DF_info[~DF_info[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "DF_info_lvl_0 = DF_info_lvl_0.drop_duplicates(subset = [\"Infected_ancestor\",\"index\",\"prophage_id\"] , keep = \"first\").reset_index(drop=True)\n",
    "\n",
    "# Log file : \n",
    "path_ensemble = f\"{path_work}/train_nn/ensemble_2809\"\n",
    "path_finetuned = f\"{path_work}/train_nn/fine_tuning/models\"\n",
    "df_prophages = DF_info_lvl_0.drop_duplicates(subset = [\"Phage\"])\n",
    "dico_prophage_count = dict(Counter(df_prophages[\"KL_type_LCA\"]))\n",
    "\n",
    "KLtypes = [kltype for kltype in dico_prophage_count if dico_prophage_count[kltype] >= 20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aa8fc3-af0e-4fbf-b82d-fd01981a49de",
   "metadata": {},
   "source": [
    "> Open the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85cf2073-420d-4f65-8782-e74c9c60708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_project = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023/in_vitro_DFs\"\n",
    "\n",
    "# ESM2 embedding dataframes :\n",
    "# Ferriol \n",
    "dpo_embeddings = pd.read_csv(f\"{path_project}/Dpo_domains_77.esm2.embedding.csv\", sep = \",\" , header = None)\n",
    "dpo_embeddings = dpo_embeddings.drop([1281] , axis = 1)\n",
    "dpo_embeddings[0] = dpo_embeddings[0].apply(lambda x : x.split(\"_Dpo\")[0])\n",
    "dpo_embeddings.set_index([0], inplace = True)\n",
    "\n",
    "# Beamud  \n",
    "bea_embeddings = pd.read_csv(f\"{path_project}/Bea_phages.esm2.embedding.csv\", sep = \",\" , header = None)\n",
    "bea_embeddings = bea_embeddings.drop([1281] , axis = 1)\n",
    "bea_embeddings[0] = bea_embeddings[0].apply(lambda x : x.replace(\"_\", \"__\"))\n",
    "bea_embeddings.set_index([0], inplace = True)\n",
    "\n",
    "# Towndsend  :\n",
    "towndsend_embeddings = pd.read_csv(f\"{path_project}/Townsed_phages.esm2.embedding.csv\", sep = \",\" , header = None)\n",
    "towndsend_embeddings = towndsend_embeddings.drop([1281] , axis = 1)\n",
    "towndsend_embeddings[0] = towndsend_embeddings[0].apply(lambda x : x.replace(\"_\", \"__\"))\n",
    "towndsend_embeddings.set_index([0], inplace = True)\n",
    "\n",
    "# ==> DF embeddings \n",
    "df_embeddings = pd.concat([towndsend_embeddings, bea_embeddings, dpo_embeddings], axis = 0)\n",
    "\n",
    "# ************************************\n",
    "# The matrices : \n",
    "# Beamud matrix :\n",
    "bea_df = pd.read_csv(f\"{path_project}/bea_fine_tuning.df\", sep = \"\\t\", header = 0)\n",
    "bea_df[\"Protein\"] = bea_df[\"Protein\"].apply(lambda x : x.replace(\"_\", \"__\"))\n",
    "pool_bea = set([kltype.strip() for kltypes in bea_df[\"Target\"] for kltype in kltypes.split(\",\") if kltype.count(\"wzi\") == 0 if kltype.count(\"pass\") == 0])\n",
    "\n",
    "# Ferriol matrix\n",
    "ferriol_df = pd.read_csv(f\"{path_project}/ferriol_fine_tuning.df\", sep = \"\\t\", header = 0)\n",
    "ferriol_df[\"Target\"] = ferriol_df[\"Target\"].apply(lambda x : x.replace(\"K\", \"KL\"))\n",
    "pool_ferriol = set([kltype.strip() for kltypes in ferriol_df[\"Target\"] for kltype in kltypes.split(\",\") if kltype.count(\"wzi\") == 0 if kltype.count(\"pass\") == 0])\n",
    "\n",
    "# Towndsend matrix :\n",
    "towndsend_df = pd.read_csv(f\"{path_project}/towndsend_fine_tuning.df\", sep = \"\\t\", header = 0)\n",
    "towndsend_df[\"Protein\"] = towndsend_df[\"Protein\"].apply(lambda x : x.replace(\"_\", \"__\"))\n",
    "pool_towndsend = set([kltype.strip() for kltypes in towndsend_df[\"Target\"] for kltype in kltypes.split(\",\") if kltype.count(\"wzi\") == 0 if kltype.count(\"pass\") == 0])\n",
    "\n",
    "# ==> dico Data\n",
    "dico_matrices = {\"ferriol\" : {\"matrix\" : ferriol_df, \"pool\" : pool_ferriol}, \n",
    "                 \"bea\" : {\"matrix\": bea_df, \"pool\" : pool_bea}, \n",
    "                 \"towndsend\" : {\"matrix\" : towndsend_df, \"pool\" : pool_towndsend}}\n",
    "\n",
    "pools_kltypes = set()\n",
    "pools_kltypes.update(pool_ferriol)\n",
    "pools_kltypes.update(pool_bea)\n",
    "pools_kltypes.update(pool_towndsend)\n",
    "pools_kltypes = list(pools_kltypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166670d7-9751-4fd5-a18a-521b72d9f991",
   "metadata": {},
   "source": [
    "> Functions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2ed7d1-f704-4484-abf3-0fae545c5895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions : \n",
    "def finetune_kltype_df(kltype) :\n",
    "    positive_lines , negative_lines = [], []\n",
    "    for author in dico_matrices :\n",
    "        if kltype in dico_matrices[author][\"pool\"] : \n",
    "            for _,row in dico_matrices[author][\"matrix\"].iterrows() :\n",
    "                if row[\"Target\"].count(\"pass\") == 0 :\n",
    "                    targets = [kltype.strip() for kltype in row[\"Target\"].split(\",\") if kltype.count(\"wzi\") == 0 if kltype.count(\"pass\") == 0]\n",
    "                    if kltype not in targets :\n",
    "                        negative_lines.append(list(row.values))\n",
    "                    else :\n",
    "                        positive_lines.append(list(row.values))\n",
    "    lines = positive_lines + negative_lines\n",
    "    n_positives = len(set([item[0] for item in positive_lines]))\n",
    "    n_negatives = len(set([item[0] for item in negative_lines]))\n",
    "    df_kltype = pd.DataFrame(lines, columns = [\"phage\", \"depo\",\"KLtypes\"])\n",
    "\n",
    "    return df_kltype , n_positives, n_negatives\n",
    "    \n",
    "def build_graph_baseline(df_info, n_positives, n_negatives) : \n",
    "    # **************************************************************\n",
    "    # initialize the graph\n",
    "    graph_data = HeteroData()\n",
    "    # Indexation process  \n",
    "    indexation_nodes_A = [0]\n",
    "    indexation_nodes_B1 = df_info[\"phage\"].unique().tolist()\n",
    "    indexation_nodes_B2 = df_info[\"depo\"].unique().tolist() \n",
    "    #ID_nodes_A = {item:index for index, item in enumerate(indexation_nodes_A)}\n",
    "    #ID_nodes_A_r = {index:item for index, item in enumerate(indexation_nodes_A)}\n",
    "    ID_nodes_B1 = {item:index for index, item in enumerate(indexation_nodes_B1)}\n",
    "    ID_nodes_B1_r = {index:item for index, item in enumerate(indexation_nodes_B1)}\n",
    "    ID_nodes_B2 = {item:index for index, item in enumerate(indexation_nodes_B2)}\n",
    "    ID_nodes_B2_r = {index:item for index, item in enumerate(indexation_nodes_B2)}\n",
    "    # **************************************************************\n",
    "    # Make the node feature file : \n",
    "    embeddings_columns = [int(i) for i in range(1, 1281)]\n",
    "    node_feature_A = torch.tensor([0], dtype=torch.float)\n",
    "    node_feature_B1 = torch.zeros((len(ID_nodes_B1), 0), dtype=torch.float)\n",
    "    node_feature_B2 = torch.tensor([df_embeddings[df_embeddings.index == depo][embeddings_columns].values[0].tolist() for depo in df_info[\"depo\"]] , dtype=torch.float)\n",
    "    # feed the graph\n",
    "    graph_data[\"A\"].x = node_feature_A\n",
    "    graph_data[\"B1\"].x = node_feature_B1\n",
    "    graph_data[\"B2\"].x = node_feature_B2\n",
    "    # **************************************************************\n",
    "    # Make edge file\n",
    "    # Node B2 (depolymerase) - Node B1 (prophage) :\n",
    "    edge_index_B2_B1 = []\n",
    "    for phage in df_info.phage.unique() :\n",
    "        all_data_phage = df_info[df_info[\"phage\"] == phage]\n",
    "        for _, row in all_data_phage.iterrows() :\n",
    "            edge_index_B2_B1.append([ID_nodes_B2[row[\"depo\"]], ID_nodes_B1[row[\"phage\"]]])\n",
    "    edge_index_B2_B1 = torch.tensor(edge_index_B2_B1 , dtype=torch.long)\n",
    "    # feed the graph\n",
    "    graph_data['B2', 'expressed', 'B1'].edge_index = edge_index_B2_B1.t().contiguous()\n",
    "    # The labels : \n",
    "    labels = [1] * n_positives + [0] * n_negatives\n",
    "    graph_data[\"B1\"].y = torch.tensor(labels)\n",
    "    # Training fraction :\n",
    "    train_mask = [1]* len(labels)\n",
    "    graph_data[\"B1\"].train_mask = torch.tensor(train_mask)\n",
    "    return graph_data \n",
    "\n",
    "def train_graph(KL_type, graph_data) :\n",
    "    with open(f\"{path_work}/train_nn/fine_tuning/log_files/{KL_type}__finetuned_node_classification.2111.log\" , \"w\") as log_outfile :\n",
    "        n_prophage = dico_prophage_count[KL_type]\n",
    "        if n_prophage <= 125 : \n",
    "            model = TropiGAT_models.TropiGAT_small_module(hidden_channels = 1280, heads = 1)\n",
    "            n = \"small\"\n",
    "        else : \n",
    "            model = TropiGAT_models.TropiGAT_big_module(hidden_channels = 1280, heads = 1)\n",
    "            n = \"big\"\n",
    "        model.load_state_dict(torch.load(f\"{path_ensemble}/{KL_type}.TropiGATv2.2809.pt\"))\n",
    "        model(graph_data)\n",
    "        # \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001 , weight_decay= 0.000001)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        #\n",
    "        try : \n",
    "            for epoch in range(50):\n",
    "                train_loss = TropiGAT_models.train(model, graph_data, optimizer,criterion)\n",
    "                if epoch % 5 == 0:\n",
    "                    # Save the model checkpoint\n",
    "                    info = f'Epoch: {epoch}\\tTrain Loss: {train_loss}\\n'\n",
    "                    log_outfile.write(info)\n",
    "                    checkpoint_path = f\"{path_finetuned}/{KL_type}.{epoch}.finetuned.TropiGATv2.2211.pt\"\n",
    "                    torch.save(model.state_dict(), checkpoint_path)\n",
    "                    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "        except Exception as e :\n",
    "            log_outfile.write(f\"***Issue here : {e}\")\n",
    "\n",
    "\n",
    "def finetune_kltype(kltype) : \n",
    "    if kltype in KLtypes :\n",
    "        df_kltype , n_positives, n_negatives = finetune_kltype_df(kltype)\n",
    "        graph_data = build_graph_baseline(df_kltype , n_positives, n_negatives)\n",
    "        train_graph(kltype , graph_data)\n",
    "    else :\n",
    "        with open(f\"{path_work}/train_nn/fine_tuning/log_files/{KL_type}__finetuned_node_classification.2111.log\" , \"w\") as log_outfile :\n",
    "            log_outfile.write(\"Not in the TropiGAT system\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with ThreadPool(10) as p:\n",
    "        p.map(finetune_kltype, pools_kltypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d1f573-1b5c-430b-a980-10c9c29872af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#BATCH --job-name=2211_finetuning__\n",
    "#SBATCH --qos=short\n",
    "#SBATCH --ntasks=1 \n",
    "#SBATCH --cpus-per-task=16 \n",
    "#SBATCH --mem=50gb \n",
    "#SBATCH --time=1-00:00:00 \n",
    "#SBATCH --output=2211_finetuning__%j.log \n",
    "\n",
    "module restore la_base\n",
    "source /storage/apps/ANACONDA/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate torch_geometric\n",
    "\n",
    "python /home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023/train_nn/fine_tuning/script_files/finetuning.2211.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94db0bc1-091e-4916-9d3d-c631f363fbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geometric",
   "language": "python",
   "name": "torch_geometric"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
