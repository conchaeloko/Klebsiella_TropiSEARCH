{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40eb639d-eb27-4bbb-a21d-f291cd64fdd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/concha-eloko/Linux/conda_envs/torch_geometric/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023/train_nn/TropiGATv2.final_df_v2.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# *****************************************************************************\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Load the Dataframes :\u001b[39;00m\n\u001b[1;32m     40\u001b[0m path_work \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 41\u001b[0m DF_info \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_work\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/train_nn/TropiGATv2.final_df_v2.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m ,  header \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     43\u001b[0m DF_info \u001b[38;5;241m=\u001b[39m DF_info\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProtein_name\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     45\u001b[0m df_prophages \u001b[38;5;241m=\u001b[39m DF_info\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhage\u001b[39m\u001b[38;5;124m\"\u001b[39m], keep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/media/concha-eloko/Linux/conda_envs/torch_geometric/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/media/concha-eloko/Linux/conda_envs/torch_geometric/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/media/concha-eloko/Linux/conda_envs/torch_geometric/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/media/concha-eloko/Linux/conda_envs/torch_geometric/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/media/concha-eloko/Linux/conda_envs/torch_geometric/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023/train_nn/TropiGATv2.final_df_v2.tsv'"
     ]
    }
   ],
   "source": [
    "# Torch geometric modules\n",
    "from torch_geometric.data import HeteroData, DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import to_hetero , HeteroConv , GATv2Conv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "# Torch modules\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# SKlearn modules\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder , label_binarize , OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score , matthews_corrcoef\n",
    "\n",
    "# Ground modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import random\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import logging\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# TropiGAT modules\n",
    "import TropiGAT_graph\n",
    "import TropiGAT_models\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# *****************************************************************************\n",
    "# Load the Dataframes :\n",
    "#path_work = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023\"\n",
    "path_work = \"/media/concha-eloko/Linux/PPT_clean\"\n",
    "\n",
    "DF_info = pd.read_csv(f\"{path_work}/train_nn/TropiGATv2.final_df_v2.tsv\", sep = \"\\t\" ,  header = 0)\n",
    "\n",
    "DF_info = DF_info.drop_duplicates(subset = [\"Protein_name\"])\n",
    "\n",
    "df_prophages = DF_info.drop_duplicates(subset = [\"Phage\"], keep = \"first\")\n",
    "dico_prophage_info = {row[\"Phage\"] : {\"prophage_strain\" : row[\"prophage_id\"] , \"ancestor\" : row[\"Infected_ancestor\"]} for _,row in df_prophages.iterrows()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1655b375-1f1e-4f7b-8231-aef01e1e1a86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7199296-d087-4bd5-b752-573a00528fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_prophages(prophage) :\n",
    "    combinations = []\n",
    "    to_exclude = set()\n",
    "    to_keep = set()\n",
    "    to_keep.add(prophage)\n",
    "    df_prophage_group = DF_info[(DF_info[\"prophage_id\"] == dico_prophage_info[prophage][\"prophage_strain\"]) & (DF_info[\"Infected_ancestor\"] == dico_prophage_info[prophage][\"ancestor\"])]\n",
    "    if len(df_prophage_group) == 1 :\n",
    "        pass\n",
    "    else :\n",
    "        depo_set = set(df_prophage_group[df_prophage_group[\"Phage\"] == prophage][\"domain_seq\"].values)\n",
    "        for prophage_tmp in df_prophage_group[\"Phage\"].unique().tolist() :\n",
    "            if prophage_tmp != prophage :\n",
    "                tmp_depo_set = set(df_prophage_group[df_prophage_group[\"Phage\"] == prophage_tmp][\"domain_seq\"].values)\n",
    "                if depo_set == tmp_depo_set :\n",
    "                    to_exclude.add(prophage_tmp)\n",
    "                else :\n",
    "                    if tmp_depo_set not in combinations :\n",
    "                        to_keep.add(prophage_tmp)\n",
    "                        combinations.append(tmp_depo_set)\n",
    "                    else :\n",
    "                        to_exclude.add(prophage_tmp)\n",
    "    return df_prophage_group , to_exclude , to_keep\n",
    "\n",
    "good_prophages = set()\n",
    "excluded_prophages = set()\n",
    "\n",
    "for prophage, info_prophage in tqdm(dico_prophage_info.items()) :\n",
    "    if prophage not in excluded_prophages and prophage not in good_prophages:\n",
    "        _, excluded_members , kept_members = get_filtered_prophages(prophage)\n",
    "        good_prophages.update(kept_members)\n",
    "        excluded_prophages.update(excluded_members)\n",
    "\n",
    "DF_info_lvl_0_filtered = DF_info[DF_info[\"Phage\"].isin(good_prophages)]\n",
    "DF_info_lvl_0_final = DF_info_lvl_0_filtered[~DF_info_lvl_0_filtered[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "\n",
    "DF_info_lvl_0 = DF_info_lvl_0_final.copy()\n",
    "\n",
    "\n",
    "\n",
    "# Log file :\n",
    "path_ensemble = f\"{path_work}/train_nn/ensemble_0702\"\n",
    "\n",
    "df_prophages = DF_info_lvl_0.drop_duplicates(subset = [\"Phage\"])\n",
    "dico_prophage_count = dict(Counter(df_prophages[\"KL_type_LCA\"]))\n",
    "\n",
    "KLtypes = [kltype for kltype in dico_prophage_count if dico_prophage_count[kltype] >= 20]\n",
    "\n",
    "# *****************************************************************************\n",
    "# Make graphs :\n",
    "graph_baseline , dico_prophage_kltype_associated = TropiGAT_graph.build_graph_baseline(DF_info_lvl_0)\n",
    "#graph_dico = {kltype : TropiGAT_graph.build_graph_masking(graph_baseline , dico_prophage_kltype_associated,DF_info_lvl_0, kltype, 5, 1, 0, 0)\n",
    "#             for kltype in DF_info_lvl_0[\"KL_type_LCA\"].unique()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# *****************************************************************************\n",
    "def train_graph(KL_type) :\n",
    "    for seed in range(1,6) :\n",
    "        with open(f\"{path_work}/train_nn/ensemble_0702_log_files/{KL_type}__{seed}__node_classification.0702.log\" , \"w\") as log_outfile :\n",
    "            n_prophage = dico_prophage_count[KL_type]\n",
    "            graph_data_kltype = TropiGAT_graph.build_graph_masking_v2(graph_baseline , dico_prophage_kltype_associated, DF_info_lvl_0, KL_type, 5, 0.7, 0.2, 0.1, seed = seed)\n",
    "            if n_prophage <= 125 : \n",
    "                model = TropiGAT_models.TropiGAT_small_module(hidden_channels = 1280, heads = 1)\n",
    "                n = \"small\"\n",
    "            else : \n",
    "                model = TropiGAT_models.TropiGAT_big_module(hidden_channels = 1280, heads = 1)\n",
    "                n = \"big\"\n",
    "            model(graph_data_kltype)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001 , weight_decay= 0.000001)\n",
    "            scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "            criterion = torch.nn.BCEWithLogitsLoss()\n",
    "            early_stopping = TropiGAT_models.EarlyStopping(patience=60, verbose=True, path=f\"{path_ensemble}/{KL_type}__{seed}.TropiGATv2.0702.pt\", metric='MCC')\n",
    "            try : \n",
    "                for epoch in range(200):\n",
    "                    train_loss = TropiGAT_models.train(model, graph_data_kltype, optimizer,criterion)\n",
    "                    if epoch % 5 == 0:\n",
    "                        # Get all metrics\n",
    "                        test_loss, metrics = TropiGAT_models.evaluate(model, graph_data_kltype,criterion, graph_data_kltype[\"B1\"].test_mask)\n",
    "                        info_training_concise = f'Epoch: {epoch}\\tTrain Loss: {train_loss}\\tTest Loss: {test_loss}\\tMCC: {metrics[3]}\\tAUC: {metrics[5]}\\tAccuracy: {metrics[4]}\\n'\n",
    "                        info_training = f'Epoch: {epoch}, Train Loss: {train_loss}, Test Loss: {test_loss},F1 Score: {metrics[0]}, Precision: {metrics[1]}, Recall: {metrics[2]}, MCC: {metrics[3]},Accuracy: {metrics[4]}, AUC: {metrics[5]}'\n",
    "                        log_outfile.write(info_training_concise)\n",
    "                        scheduler.step(test_loss)\n",
    "                        early_stopping(metrics[3], model, epoch)\n",
    "                        if early_stopping.early_stop:\n",
    "                            log_outfile.write(f\"Early stopping at epoch = {epoch}\\n\")\n",
    "                            break\n",
    "                else :\n",
    "                    torch.save(model, f\"{path_ensemble}/{KL_type}__{seed}.TropiGATv2.0702.pt\")\n",
    "                # The final eval :\n",
    "                if n == \"small\" : \n",
    "                    model_final = TropiGAT_models.TropiGAT_small_module(hidden_channels = 1280, heads = 1)\n",
    "                else :\n",
    "                    model_final = TropiGAT_models.TropiGAT_big_module(hidden_channels = 1280, heads = 1)\n",
    "                model_final.load_state_dict(torch.load(f\"{path_ensemble}/{KL_type}__{seed}.TropiGATv2.0702.pt\"))\n",
    "                eval_loss, metrics = TropiGAT_models.evaluate(model_final, graph_data_kltype, criterion,graph_data_kltype[\"B1\"].eval_mask)\n",
    "                with open(f\"{path_ensemble}/Metric_Report.0702.tsv\", \"a+\") as metric_outfile :\n",
    "                    metric_outfile.write(f\"{KL_type}__{seed}\\t{n_prophage}\\t{metrics[0]}\\t{metrics[1]}\\t{metrics[2]}\\t{metrics[3]}\\t{metrics[4]}\\t{metrics[5]}\\n\")\n",
    "                info_eval = f'Epoch: {epoch}, F1 Score: {metrics[0]}, Precision: {metrics[1]}, Recall: {metrics[2]}, MCC: {metrics[3]},Accuracy: {metrics[4]}, AUC: {metrics[5]}'\n",
    "                log_outfile.write(f\"Final evaluation ...\\n{info_eval}\")\n",
    "            except Exception as e :\n",
    "                log_outfile.write(f\"***Issue here : {e}\")\n",
    "                with open(f\"{path_ensemble}/Metric_Report.0702.tsv\", \"a+\") as metric_outfile :\n",
    "                    n_prophage = dico_prophage_count[KL_type]\n",
    "                    metric_outfile.write(f\"{KL_type}__{seed}\\t{n_prophage}\\t***Issue***\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with ThreadPool(5) as p:\n",
    "        p.map(train_graph, KLtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad0b608-5a41-4455-9fcc-526d272784ac",
   "metadata": {},
   "source": [
    "> Move the files back "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a987bfa-57b4-49a6-95d1-cce3f66e3ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsync -avzhe ssh \\\n",
    "conchae@garnatxa.srv.cpd:/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023/train_nn/ensemble_2812 \\\n",
    "/media/concha-eloko/Linux/PPT_clean/ficheros_28032023\n",
    "\n",
    "rsync -avzhe ssh \\\n",
    "conchae@garnatxa.srv.cpd:/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023/train_nn/ensemble_2812_log_files \\\n",
    "/media/concha-eloko/Linux/PPT_clean/ficheros_28032023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774aae81-b5e2-4be5-91b2-27c4660b761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def train_graph(KL_type):\n",
    "    n_splits = 3\n",
    "    n_prophage = dico_prophage_count[KL_type]\n",
    "    graph_data_kltype = graph_dico[KL_type]\n",
    "\n",
    "    # Create a directory to store the models for each cross-validation step\n",
    "    model_dir = f\"{path_ensemble}/{KL_type}/models\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # Split the data into n folds using StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_idx = 1\n",
    "    for train_indices, val_indices in skf.split(graph_data_kltype[\"B1\"].x, graph_data_kltype[\"B1\"].y):\n",
    "        print(f\"Fold {fold_idx}:\")\n",
    "        print(\"Train indices:\", train_indices)\n",
    "        print(\"Validation indices:\", val_indices)\n",
    "        \n",
    "        # Split the graph data into train and validation sets\n",
    "        graph_data_train = graph_data_kltype[train_indices]\n",
    "        graph_data_val = graph_data_kltype[val_indices]\n",
    "        \n",
    "        # Train the model for this fold\n",
    "        model = TropiGAT_models.TropiGAT_small_module(hidden_channels=1280, heads=1)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.000001)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        early_stopping = TropiGAT_models.EarlyStopping(patience=60, verbose=True, path=f\"{model_dir}/fold{fold_idx}.pt\", metric='MCC')\n",
    "        train_loss = TropiGAT_models.train(model, graph_data_train, optimizer, criterion)\n",
    "        \n",
    "        # Evaluate the model on the validation set and save the metrics to a file\n",
    "        test_loss, metrics = TropiGAT_models.evaluate(model, graph_data_val, criterion, graph_data_val[\"B1\"].eval_mask)\n",
    "        metrics_df = pd.DataFrame(metrics, index=[fold_idx])\n",
    "        if os.path.exists(f\"{model_dir}/metrics.csv\"):\n",
    "            metrics_df.to_csv(f\"{model_dir}/metrics.csv\", mode='a', header=False, index=True)\n",
    "        else:\n",
    "            metrics_df.to_csv(f\"{model_dir}/metrics.csv\", index=True)\n",
    "        \n",
    "        # Save the trained model for this fold\n",
    "        torch.save(model, f\"{model_dir}/fold{fold_idx}.pt\")\n",
    "        \n",
    "        fold_idx += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_geometric]",
   "language": "python",
   "name": "conda-env-torch_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
